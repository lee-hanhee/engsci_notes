\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}Introduction to ML, Nearest neighbors, Linear classification (LFD: 1.1-1.2)}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Types of learning}{4}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Supervised learning}{4}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}Unsupervised learning}{6}{subsubsection.1.1.2}%
\contentsline {subsubsection}{\numberline {1.1.3}Reinforcement learning}{7}{subsubsection.1.1.3}%
\contentsline {subsection}{\numberline {1.2}Classification problem setup}{7}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}k-Nearest neighbour}{7}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}Decision boundary}{9}{subsubsection.1.3.1}%
\contentsline {subsection}{\numberline {1.4}Error process for classification}{9}{subsection.1.4}%
\contentsline {subsubsection}{\numberline {1.4.1}Types of error}{9}{subsubsection.1.4.1}%
\contentsline {subsection}{\numberline {1.5}Hyperparameters}{9}{subsection.1.5}%
\contentsline {subsection}{\numberline {1.6}Over/underfitting, Outliers}{10}{subsection.1.6}%
\contentsline {subsection}{\numberline {1.7}Encoding (Ways to transform data)}{10}{subsection.1.7}%
\contentsline {subsubsection}{\numberline {1.7.1}Transform numeric data}{10}{subsubsection.1.7.1}%
\contentsline {subsubsection}{\numberline {1.7.2}Transform categorical data}{10}{subsubsection.1.7.2}%
\contentsline {subsection}{\numberline {1.8}Linear classification}{11}{subsection.1.8}%
\contentsline {subsubsection}{\numberline {1.8.1}What is the hypothesis set for linear classification problem?}{11}{subsubsection.1.8.1}%
\contentsline {subsubsection}{\numberline {1.8.2}How to describe hypothesis set through a function form?}{11}{subsubsection.1.8.2}%
\contentsline {subsubsection}{\numberline {1.8.3}Training}{11}{subsubsection.1.8.3}%
\contentsline {subsubsection}{\numberline {1.8.4}Prediction}{11}{subsubsection.1.8.4}%
\contentsline {subsubsection}{\numberline {1.8.5}How to find and draw decision boundaries?}{12}{subsubsection.1.8.5}%
\contentsline {subsection}{\numberline {1.9}Basic setup of learning problem of supervised learning}{13}{subsection.1.9}%
\contentsline {subsection}{\numberline {1.10}Basic setup of learning problem of binary linear classification}{13}{subsection.1.10}%
\contentsline {subsubsection}{\numberline {1.10.1}Training}{14}{subsubsection.1.10.1}%
\contentsline {subsubsection}{\numberline {1.10.2}How hard is it to find the best decision boundary? (i.e. minimizing the error)}{14}{subsubsection.1.10.2}%
\contentsline {section}{\numberline {2}Linear regression, Regularization (LFD: 3.2.1, 3.4.1, Appendix B)}{15}{section.2}%
\contentsline {subsection}{\numberline {2.1}Perceptron learning algorithm}{15}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}New Formulation of Binary Linear Classification}{15}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Algorithm}{15}{subsubsection.2.1.2}%
\contentsline {subsubsection}{\numberline {2.1.3}Why does PLA work? (intutive explanation)}{15}{subsubsection.2.1.3}%
\contentsline {subsubsection}{\numberline {2.1.4}Rosenblett Theorem (analytical proof why PLA works)}{16}{subsubsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.5}Remarks about PLA (uniqueness and error)}{16}{subsubsection.2.1.5}%
\contentsline {subsubsection}{\numberline {2.1.6}What would happen if we use PLA for not linearly separable datasets?}{17}{subsubsection.2.1.6}%
\contentsline {subsubsection}{\numberline {2.1.7}Multiary classification}{17}{subsubsection.2.1.7}%
\contentsline {subsection}{\numberline {2.2}Pocket algorithm}{17}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Visual intuition}{17}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Algorithm}{17}{subsubsection.2.2.2}%
\contentsline {subsection}{\numberline {2.3}Linear regression}{18}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Matrix-vector algebraic representation}{18}{subsubsection.2.3.1}%
\contentsline {subsubsection}{\numberline {2.3.2}When is the error 0? (Motivation for least squares)}{19}{subsubsection.2.3.2}%
\contentsline {subsection}{\numberline {2.4}Least squares}{19}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Gradient}{19}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Basic gradients}{20}{subsubsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.3}Problem setup}{20}{subsubsection.2.4.3}%
\contentsline {subsubsection}{\numberline {2.4.4}Solution}{21}{subsubsection.2.4.4}%
\contentsline {subsubsection}{\numberline {2.4.5}Why is it called pseudo-inverse of X?}{21}{subsubsection.2.4.5}%
\contentsline {subsubsection}{\numberline {2.4.6}Prediction}{21}{subsubsection.2.4.6}%
\contentsline {subsubsection}{\numberline {2.4.7}Geometric interpretation of least squares}{21}{subsubsection.2.4.7}%
\contentsline {subsection}{\numberline {2.5}Regularized linear regression / least squares}{22}{subsection.2.5}%
\contentsline {subsubsection}{\numberline {2.5.1}Difference between LS and RLS}{23}{subsubsection.2.5.1}%
\contentsline {subsubsection}{\numberline {2.5.2}How do we solve this problem?}{23}{subsubsection.2.5.2}%
\contentsline {subsection}{\numberline {2.6}Non-linear models}{23}{subsection.2.6}%
\contentsline {subsubsection}{\numberline {2.6.1}Circular decision boundary example}{23}{subsubsection.2.6.1}%
\contentsline {subsubsection}{\numberline {2.6.2}General form}{24}{subsubsection.2.6.2}%
\contentsline {section}{\numberline {3}Worksheet 1}{25}{section.3}%
\contentsline {subsection}{\numberline {3.1}Q1}{25}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Gradient shortcuts}{25}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}lp-Norms}{26}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Matrix multiplication}{26}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Taylor series}{26}{subsubsection.3.1.4}%
\contentsline {subsection}{\numberline {3.2}Q3}{27}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Matrix properties}{27}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Q3B}{28}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Q3C}{30}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Q3D}{30}{subsubsection.3.2.4}%
\contentsline {section}{\numberline {4}Logistic Regression (LFD: 3.3)}{31}{section.4}%
\contentsline {subsection}{\numberline {4.1}Difference between linear classification and regression}{31}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Logistic regression}{32}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{32}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Deriving the appropriate hypothesis set}{33}{subsubsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.3}Illustrating logistic regression as a neuron}{34}{subsubsection.4.2.3}%
\contentsline {subsubsection}{\numberline {4.2.4}Logistic function}{35}{subsubsection.4.2.4}%
\contentsline {subsubsection}{\numberline {4.2.5}Prediction (Reformulating the hypothesis set)}{35}{subsubsection.4.2.5}%
\contentsline {subsubsection}{\numberline {4.2.6}What is the Error Criterion?}{35}{subsubsection.4.2.6}%
\contentsline {subsection}{\numberline {4.3}Why is log-loss a reasonable choice?}{36}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}Numerical examples interpretation (i.e. distinguish between better hyperplanes)}{36}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Maximum likelihood interpretation}{37}{subsubsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.3}Cross-entropy interpretation}{37}{subsubsection.4.3.3}%
\contentsline {section}{\numberline {5}Gradient Descent (LFD: 3.3, DL: 8.3.1-8.3.3 (recommended))}{38}{section.5}%
\contentsline {subsection}{\numberline {5.1}Problem setup for gradient descent}{38}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Optimization primer}{38}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Gradient:}{38}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Gradient and optimality:}{39}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}Convex functions:}{39}{subsubsection.5.2.3}%
\contentsline {subsubsection}{\numberline {5.2.4}Chain rule}{40}{subsubsection.5.2.4}%
\contentsline {subsection}{\numberline {5.3}Simple algorithm to find min f(x) for 1D}{41}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Simple algorithm to find min f(x) for ND space}{42}{subsection.5.4}%
\contentsline {subsubsection}{\numberline {5.4.1}2D space}{43}{subsubsection.5.4.1}%
\contentsline {subsection}{\numberline {5.5}Gradient descent algorithm}{44}{subsection.5.5}%
\contentsline {subsection}{\numberline {5.6}Gradient descent performance}{44}{subsection.5.6}%
\contentsline {subsection}{\numberline {5.7}Using Gradient Descent for Logistic Regression}{44}{subsection.5.7}%
\contentsline {subsection}{\numberline {5.8}Stochastic Gradient Descent (SGD)}{45}{subsection.5.8}%
\contentsline {subsubsection}{\numberline {5.8.1}gt of SGD is an unbiased estimate of gradient of error}{45}{subsubsection.5.8.1}%
\contentsline {subsubsection}{\numberline {5.8.2}Full GD complexity vs. SGD complexity}{46}{subsubsection.5.8.2}%
\contentsline {subsubsection}{\numberline {5.8.3}Mini-batch GD is full-batch GD combined with SGD}{46}{subsubsection.5.8.3}%
\contentsline {subsection}{\numberline {5.9}PLA is an Extremer Version of Logistic Regression with SGD}{46}{subsection.5.9}%
\contentsline {subsection}{\numberline {5.10}Multiclass Logistic Regression}{47}{subsection.5.10}%
\contentsline {subsection}{\numberline {5.11}SGD update}{47}{subsection.5.11}%
\contentsline {subsection}{\numberline {5.12}Computing gradient of error}{48}{subsection.5.12}%
\contentsline {subsection}{\numberline {5.13}Softmax Logistic Regression for Binary Classification}{49}{subsection.5.13}%
\contentsline {subsection}{\numberline {5.14}Can we use GD/SGD for Linear Regression? Yes, you can}{49}{subsection.5.14}%
\contentsline {section}{\numberline {6}Multilayer Perceptron, Neural Networks (LFD: Sec 7.1, 7.2.1, 7.2.2)}{50}{section.6}%
\contentsline {section}{\numberline {7}Backpropagation, Neural Networks Implementations (LFD: 7.2, DL: 7.12, 8.4, 8.5 (recommended))}{50}{section.7}%
\contentsline {section}{\numberline {8}Unsupervised Learning, Clustering, Density Estimation (LFD: 6.3.3, 6.4)}{50}{section.8}%
\contentsline {section}{\numberline {9}EM (LFD: 6.4)}{50}{section.9}%
\contentsline {section}{\numberline {10}Markov Decision Process (AIMA: 16)}{50}{section.10}%
\contentsline {section}{\numberline {11}Reinforcement Learning (AIMA: 23.1-23.4)}{50}{section.11}%
\contentsline {section}{\numberline {12}Deep Learning Architectures CNN; Language Models; RNN; Sequence modeling and neural machine translation; Attention; LSTM (PRML: Sec 5.5.6, DL: Sec 10.1, 10.2, 10.4, 10.10, SLP: Ch. 9)}{50}{section.12}%
