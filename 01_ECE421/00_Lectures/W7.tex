LOOK IN TEXTBOOK FOR MORE DETAILS. 
\subsection{Deep Learning in Practice:}
\begin{summary}
    This is not a comprehensive list, but some of parameters that we get to choose. 
    \begin{itemize}
        \item \textbf{Architecture Choices}
        \begin{itemize}
            \item \textbf{DAG:} Number of layers, etc.
            \item \textbf{Activation functions:} (e.g., logistic, tanh, ReLU, \dots)
            \item \textbf{Error or log-loss functions}
            \item \textbf{Input preprocessing}
        \end{itemize}
    
        \item \textbf{Data Choices}
        \begin{itemize}
            \item Train, validation, test, cross-validation, \dots
        \end{itemize}
    
        \item \textbf{Algorithm Choices}
        \begin{itemize}
            \item Initialization of weights \& biases
            \item Gradient descent, SGD, minibatches, line search, momentum, Adam, \dots
            \item Stopping criterion, weight regularization, dropout, \dots
        \end{itemize}
    \end{itemize}
\end{summary}

\subsection{DL in Practice: Initialization of Weights and Biases} 
\begin{summary}
    \begin{itemize}
        \item Recall that \( \frac{\partial E}{\partial w_{lm}} \propto g'(x_m) \) (proportional)
        
        \item If \( g'(x_m) \) is close to zero, it will be very difficult to change \( w_{lm} \), therefore,
        \[
        \Rightarrow \text{Initialize } w\text{'s so } |g(x_m)| > \varepsilon \quad \text{(Minimum derivative)}
        \]
        \begin{itemize}
            \item \textbf{i.e.} Greater than the numerical precision of the computer, so you can update the weights in a meaningful way, and don't have a vanishing gradient problem. 
        \end{itemize}
    
        \item \textbf{One hidden unit:} For \( g = \tanh \), if we sample weights from a zero-mean distribution, then:
        \[
        \mathbb{E}[x_m] = \sum_{i=1}^{m-1} \mathbb{E}[w_{lm}] \mathbb{E}[g(x_i)] = \sum_{i=1}^{m-1} 0 \cdot \mathbb{E}[g(x_i)] = 0
        \]
        \begin{itemize}
            \item \textbf{Assumption:} Assuming $w$ and $g$ are independent, so we can seperate expectations for weights and post-activation b/c these are only being initialized so far.
            \item \textbf{Note:} Expected value of $w_{lm} = 0$ because we sample from a zero-mean (i.e. expectation is 0).
            \item \textbf{i.e.} Therefore, expected activity is 0 (i.e. on average $x_m =0$). This is good because for $tanh$, slope is steepest when the input is $0$ (region we want to be in).
        \end{itemize}
        
        \item \textbf{One hidden unit:} Variance calculation:
        \[
        V[x_m] = \sum_{i=1}^{m-1} V[w_{lm} g(x_i)] = \sum_{i=1}^{m-1} V[w_{lm}] \cdot V[g(x_i)]
        \]
        \begin{itemize}
            \item \textbf{Assumption:} $w$ and $g$ are independent so we can seperate their variances.
            \item \textbf{Note:} $V[g(x_i)] \leq 1$ because for $tanh$ it is bounded between $-1$ and $1$.
        \end{itemize}
        \[
        \leq \sum_{i=1}^{m-1} V[w_{lm}] = (\#\text{inputs to } x_m) \times V[w]
        \]
        \begin{itemize}
            \item \textbf{Note:} We want the variance of the input to unit $m$ to be less than or equal to 1 (i.e. $(\#\text{inputs to } x_m) \times V[w] \leq 1$).
            \item $(\#\text{inputs to } x_m) \times V[w]$: Since weights are all drawn from the same distribution. 
        \end{itemize}
    
        \item \textbf{One hidden unit:} Therefore, sample \( w \)'s with variance:
        \[
            V[w] = \frac{1}{\#\text{inputs}}
        \]
        \begin{itemize}
            \item \textbf{Note:} This is to have the variance of the input to be less than or equal to $1$, so that there is consistent variance across layers, which prevents activations from diminishing or growing uncontrollably.
        \end{itemize}
    \end{itemize}
\end{summary}

\subsubsection{Glorot and Bergio}
\begin{summary}
    \begin{itemize}
        \item Consider a fully connected layer with \( m \) inputs and \( n \) outputs.
        
        \item Sample weights \( w \)'s from a uniform distribution:
        \[
        w \sim \text{Uniform}\left(-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right)
        \]
        \begin{itemize}
            \item \textbf{Intuition:} $m$ is to keep the gradients in check, $n$ is to keep the activations in check. 
            \item \textbf{Note:} This balance prevents gradients from becoming excessively large (exploding gradients) or diminishing towards zero (vanishing gradients) as they propagate through the layers, which is crucial for effective backpropagation in deep networks.
            \item \textbf{Note:} Find a compromise that keeps both activation variance and gradient variance at a manageable level.
        \end{itemize}
    
        \item \textbf{High-level:} This initialization is a compromise between ensuring activation variance is the same across layers and all weights \( w \)'s have the same gradient variance.
        \begin{itemize}
            \item \textbf{Note:} As you go through the layers of the neural network, you want to be in the active range of the gradient, so that the weight updates can be meaningful
            \item \textbf{Note:} $\text{Variance is proportional to } \frac{6}{m+n}$ (i.e. 1 over the number of inputs and outputs, where the 6 is to take into a account the uniform distribution).
        \end{itemize}
    \end{itemize}
\end{summary}

\subsection{Avoiding overfitting (DL in Practice: Train, validation, test, stopping criterion, weight regularization, dropout, \dots)}
\begin{summary}
    \begin{itemize}
        \item \textbf{Weight Penalties (CDL 7.1):} Minimize \( E(\text{data}, \omega) + \lambda \|\omega\| \)
        \begin{itemize}
            \item Favors small weights \& neural nets that make 'smooth' predictions
            \item i.e. Keep the weights small, so that the model is less likely to overfit.
        \end{itemize}
        \item \textbf{Early stopping:} Start with tiny weights, monitor a validation set during training, and stop when the validation error starts to increase.
        \begin{itemize}
            \item Helpful because the training error will always decrease, but the validation error will start to increase when the model starts to overfit.
        \end{itemize}
        \customFigure[0.75]{00_Images/AO.png}{Avoiding overfitting}
        \item \textbf{Bagging (CDL 7.1)}: Train many (e.g., 10) neural nets, and allow them to overfit. To make a prediction for a test case, \underline{average} the predictions from the ensemble of neural nets.
        \begin{itemize}
            \item Some models will overfit in different ways, so averaging them will reduce the overfitting.
        \end{itemize}
        \customFigure[0.75]{00_Images/AO1.png}{Avoiding overfitting}
    \end{itemize}
\end{summary}

\subsection{Wasted and Stuck Units}
\begin{summary}
    The main challenge for large neural nets is not just overfitting, but that most hidden units get \underline{stuck where they aren't needed} \& are thus \underline{wasted}.
    \vspace{1em}
    
    \begin{itemize}
        \item Weight regularization \& early stopping don't help.
        \item Bagging somewhat helps, but
        \begin{itemize}
            \item The number of nets needed grows rapidly with the complexity of the data.
            \item When training each net in the ensemble, most units still get stuck.
        \end{itemize}
    \end{itemize}
\end{summary}

\begin{example}
    \customFigure[0.75]{00_Images/FA.png}{Function approximation example}
    \begin{itemize}
        \item The top plot shows a "good solution," where both sigmoid units contribute meaningfully. Sigmoid 1 captures the first rise in the function, while Sigmoid 2 adjusts to capture the second rise, leading to an accurate approximation.
        
        \item The bottom plot demonstrates a poor solution where Sigmoid 2 is "stuck." Here:
            \begin{itemize}
                \item Sigmoid 1 is positioned to capture the initial rise in the function.
                \item However, Sigmoid 2 fails to adjust effectively and remains flat, resulting in a suboptimal approximation.
            \end{itemize}
    \end{itemize}    
\end{example}

\subsubsection{Dropout (Solution to Wasted and Stuck Units)}
\begin{summary}
    \begin{itemize}
        \item \underline{Goal}: Train very large networks while avoiding \underline{overfitting and wasting most hidden units}.
        
        \begin{itemize}
            \item \textit{Recall the wasted hidden units for a simple task}
            \customFigure[0.5]{00_Images/E1.png}{Example 1}
        \end{itemize}
    
        \item \underline{Method}
        \begin{itemize}
            \item Set dropout probabilities \(\{\mu_i\}\) (i.e. probability of keeping the unit), e.g., for input units \(\mu_i = 0.8\) and for hidden units \(\mu_i = 0.5\).
            \item During training, for each forward pass, set the activity of \(\hat{x}_i\) to zero with probability \(1 - \mu_i\) (i.e. probability of dropping the unit), independent of other units.
            \item During testing, set \( w_{ij} \leftarrow w_{ij} \cdot \mu_i \) (i.e. multiply by the probability of keeping the unit) for all $i,j$.
        \end{itemize}
    \end{itemize}
\end{summary}


\subsubsection{Dropout: Cooperation and Exponential Bagging}
\begin{summary}
    \begin{itemize}
        \item Dropout simulates an exponential number of neural architectures 
        \begin{itemize}
            \item \textit{Exponential bagging}
            \item \textbf{Note:} Dropout is similar to bagging, but instead of training many nets, we sample many nets during training which all have the same weights.
        \end{itemize}
        
        \item Because the weights are shared across all networks and they are trained jointly, the units can cooperate during training to get unstuck.
    \end{itemize}
    
    \customFigure[0.5]{00_Images/DE.png}{Dropout}
    
    \textit{These are the nets that get sampled in the forward pass}
\end{summary}


\begin{example}
    Suppose each hidden unit is dropped out with \( p = 0.5 \).

    \customFigure[0.5]{00_Images/S.png}{Sigmoid with dropout}

    \begin{itemize}
        \item \( \frac{1}{4} \) of the time, only sigmoid 2 is used to predict the output, so it will be pressured to take over for sigmoid 1, which can then capture the incline on the far right.
    \end{itemize}
\end{example}