\subsection{Summary}
\begin{intuition}
    \begin{itemize}
        \item Limitations of the perceptron
        \item Decomposing of a complex problem
        \item Differentiable activation functions (sgn to logistic, tanh)
        \item Neural networks
        \item Applications to Genomics
        \item Forward propogation
        \item Back propogation
    \end{itemize}
\end{intuition}

\subsection{Motivation for neural networks}

\subsubsection{Limitation of perceptron}
\begin{intuition}
    It cannot deal with linearly inseparable data. 
    \customFigure[0.5]{00_Images/LI.png}{Linearly inseparable.}
\end{intuition}

\subsubsection{Decomposing a complex problem}
\begin{intuition}
    Take a divide and conquer approach.
    \begin{itemize}
        \item Divide the problem into $h_1$ and $h_2$. 
        \item Then you can use AND, NOT, and OR to decompose the problem into a binary encoding that repesents the 4 quadrants based on the value of $h_1$ and $h_2$. 
    \end{itemize}
    \customFigure[0.75]{00_Images/CP.png}{Decomposing the complex problem to be constructed from $f$.}
\end{intuition}

\subsubsection{Representing as a multilayer perceptron}
\begin{definition}
    \customFigure[0.75]{00_Images/MLP.png}{Multilayer Perceptron}
    \begin{itemize}
        \item \textbf{DAG:} Directed acyclic graph with weights on the edges. 
        \item \textbf{Note:} The MLP is built up with the components of AND and OR with $-1$ representing the not. 
    \end{itemize}
    \begin{itemize}
        \item \textbf{OR Gate Implementation:}
        \begin{itemize}
            \item Inputs: \( x_1 \), \( x_2 \)
            \item Weights: \( w_1 = 1 \), \( w_2 = 1 \)
            \item Bias: \( -0.5 \)
            \item Output: 
            \[
            \text{Output} = 
            \begin{cases}
            1 & \text{if } w_1 x_1 + w_2 x_2 + (-0.5) > 0 \\
            0 & \text{otherwise}
            \end{cases}
            \]
            \begin{itemize}
                \item If $x_1 = 1$ and $x_2 = 1$, $\text{Output} = 2 - 0.5 = 1.5$ (True)
                \item If $x_1 = 0$ and $x_2 = 1$, $\text{Output} = 1 - 0.5 = 0.5$ (True)
                \item If $x_1 = 1$ and $x_2 = 0$, $\text{Output} = 1 - 0.5 = 0.5$ (True)
                \item If $x_1 = 0$ and $x_2 = 0$, $\text{Output} = - 0.5 = -0.5$ (False)
            \end{itemize}
        \end{itemize}
        
        \item \textbf{AND Gate Implementation:}
        \begin{itemize}
            \item Inputs: \( x_1 \), \( x_2 \)
            \item Weights: \( w_1 = 1 \), \( w_2 = 1 \)
            \item Bias: \( -1.5 \)
            \item Output: 
            \[
            \text{Output} = 
            \begin{cases}
            1 & \text{if } w_1 x_1 + w_2 x_2 + (-1.5) > 0 \\
            0 & \text{otherwise}
            \end{cases}
            \]
            \begin{itemize}
                \item If $x_1 = 1$ and $x_2 = 1$, $\text{Output} = 2 - 1.5 = 0.5$ (True)
                \item If $x_1 = 0$ and $x_2 = 1$, $\text{Output} = 1 - 1.5 = -0.5$ (False)
                \item If $x_1 = 1$ and $x_2 = 0$, $\text{Output} = 1 - 1.5 = -0.5$ (False)
                \item If $x_1 = 0$ and $x_2 = 0$, $\text{Output} = - 1.5 = -1.5$ (False)
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Function \( f = h_1\bar{h}_2 + \bar{h}_1 h_2 \) Implementation:}
        \begin{itemize}
            \item First term: \( h_1 \bar{h}_2 \) represents the AND of \( h_1 \) and \( \bar{h}_2 \)
            \item Second term: \( \bar{h}_1 h_2 \) represents the AND of the NOT of \( h_1 \) and \( h_2 \)
            \item Final output: 
            \[
            f = (h_1 \land \neg h_2) \lor (\neg h_1 \land h_2)
            \]
        \end{itemize}
    \end{itemize}    
\end{definition}

\subsection{Differentiable activation functions}
\begin{intuition}
    \begin{itemize}
        \item For linear/logistic regression, k-means clustering, MoG have differentiable objective functions that can be used to derive a learning algorithm. 
        \item $sgn(\underline{w}^T \underline{x})$ is not differentiable. 
    \end{itemize}
\end{intuition}

\begin{definition}
    Introduce continuous activation functions that are differentiable: 
    \begin{itemize}
        \item $\text{Logistic:} \quad g(z) = \frac{1}{1 + e^{-z}}$
        \item $\text{Tanh:} \quad g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
    \end{itemize}
    \customFigure[0.5]{00_Images/AF.png}{Activation functions.}
\end{definition}

\subsection{Neural network}
\begin{example}
    \customFigure[0.5]{00_Images/NN.png}{Neural network with inputs, hidden units, and outputs that introduce the notation.}
    \begin{itemize}
        \item \textbf{Note:} The notation is given by $x_\#$ to be easier for the notation.
    \end{itemize}
\end{example}

\subsubsection{Application: Arbitrary 1D Function Approximation}
\begin{example}
    \begin{enumerate}
        \item \textbf{One hidden layer:}
        \item \textbf{Constructing a delta function}
        \begin{itemize}
            \item Constructing a delta function from the logistic function using $g(a+a/2)-g(ax-a/2)$.
            \item As $a\rightarrow \infty$, then you construct a delta function 
        \end{itemize}

        \item \textbf{Approximating any function}
        \begin{itemize}
            \item \textbf{Purpose:} There exists a set of parameters (i.e. hidden units) to arbitrary approximate the functions.
        \end{itemize}
    \end{enumerate}
    \customFigure[0.75]{00_Images/A1DF.png}{Approximating functions.}
\end{example}

\subsection{DL in Genomics (Real-life examples)}
\begin{example}
    \textbf{Detecting Protein Binding Sites in DNA}
    \customFigure[0.75]{00_Images/PB.png}{Protein binding}
    \begin{itemize}
        \item \textbf{B:} The disc is the set of representable functions, and it is initialized. The learning algorithm makes the function moves around until you reach the learned function. The true function will lie outside of the representable functions. 
    \end{itemize}
\end{example}

\begin{example}
    \customFigure[0.75]{00_Images/LNN.png}{1st layer weights}
\end{example}

\subsection{Mostly complete chart of NN}
\begin{definition}
    \customFigure[0.65]{00_Images/CNN.png}{Mostly complete chart of NN}
    \begin{itemize}
        \item \textbf{Key:} Lots of different architectures.
    \end{itemize}
\end{definition}

\subsection{Forward propogation}
\begin{definition}
    \begin{itemize}
        \item $I$ inputs, $O$ outputs, $H$ hidden units
        \item Total units $M = I + H + O$
        \item Index units so that connection $i \rightarrow j$ satisfies $i < j$
        \item Ordered activations:
        \[
        x_1, \ldots, x_I, x_{I+1}, \ldots, x_{I+H}, x_{M-O+1}, \ldots, x_M
        \]
        \begin{itemize}
            \item $x_1, \ldots, x_I$: Inputs
            \item $x_{I+1}, \ldots, x_{I+H}$: Hidden
            \item $x_{M-O+1}, \ldots, x_M$: Outputs
        \end{itemize}
        \item It is convenient to let $x_i$ be the pre-activation value for unit $i$
        \item Combining the above, we have
        \[
        \text{For } j > I, \quad x_j = \sum_{i=1}^{j-1} w_{ij} g_i(x_i)
        \]
        \begin{itemize}
            \item $g_i(x_i)$ is the activation function for $x_i$
            \item If $j$ is not connected to $i$, set $w_{ij} = 0$.
        \end{itemize}
    \end{itemize}
    \vspace{1em}
    
    \begin{itemize}
        \item If input unit $x_i$, $i \in \{1, \ldots, I\}$ is not transformed, set $g_i(x_i) = x_i$, i.e., $g_i$ is the identity.
        \begin{itemize}
            \item This means that for input units (the original features fed into the neural network), the activation function $g_i$ does not change the value of the input. 
        \end{itemize}
        \item The outputs of the neural network are $g_{M-O+1}(x_{M-O+1}), \ldots, g_M(x_M)$.
        \begin{itemize}
            \item The outputs are derived by applying the activation functions to the last set of units in the network.
            \item Here, $M$ represents the total number of units (including input, hidden, and output units)
            \item $O$ represents the number of output units. 
            \item $M-O+1$ to $M$ refers to the indices of the output units.
            \item The functions $g_{M-O+1}, \ldots, g_M$ represent the activation functions associated with each output unit. 
        \end{itemize}
    \end{itemize}    
\end{definition}

\begin{example}
    \customFigure[0.75]{00_Images/LN.png}{Layered net}
\end{example}

\subsection{Backward Propogation}
\subsubsection{Learning NNs}
\begin{example}
    \customFigure[0.75]{00_Images/LNNN.png}{Layered Neural Network}    
\end{example}

\subsubsection{Deriving Backward Prop.}
\begin{derivation}
    \textbf{Deriving $\frac{\partial E}{\partial w_{lm}}$}

    \begin{enumerate}
        \item \textbf{Forward propogation:} We know that for $j = l + 1 \dots M$, $x_j = \sum_{i=1}^{j-1} w_{ij} g_i(x_i)$ \quad \text{Eq1}

        \item $\frac{\partial E}{\partial w_{lm}} \text{ for one training case: } w_{lm} \text{ influences $E$ only via $x_m$}$
        \begin{equation*}
            \frac{\partial E}{\partial w_{lm}} = \frac{\partial E}{\partial x_m} \frac{\partial x_m}{\partial w_{lm}} = \frac{\partial E}{\partial x_m} g'_l(x_l)   
        \end{equation*}
        \begin{itemize}
            \item Last part obtained by equation 1.
        \end{itemize}

    
        \item If $x_m$ is \textit{not} an output unit, then $x_m$ influences $E$ via $x_{m+1}, \dots, x_M$, and we obtain a recursion:
    
        \[
        \frac{\partial E}{\partial x_m} = \sum_{k=m+1}^{M} \frac{\partial E}{\partial x_k} \frac{\partial x_k}{\partial x_m} = \frac{\partial E}{\partial x_m} = \sum_{k=m+1}^{M} \frac{\partial E}{\partial x_k} w_{mk} g'_m(x_m)
        \]
    
        \item  If $x_m$ \textit{is} an output unit (assuming using squared error), then
    
        \[
        \frac{\partial E}{\partial x_m} = \frac{\partial (y_m - g_m(x_m))^2}{\partial x_m} = -2(y_m - g_m(x_m)) g'_m(x_m)
        \]
    \end{enumerate}
\end{derivation}

\subsubsection{Backward Propogation Overview (Pre-activation Values):}
\begin{definition}
    Assuming DAG, and each weight feeds into one unit.
    \textbf{Forward propagation}:
    For $j = l+1 \dots M$, 
    \[
    x_j = \sum_{i=1}^{j-1} w_{ij} g_i(x_i)
    \]

    \textbf{Output unit error derivatives}:
    For $n = M \dots M-O+1$:
    \[
    \frac{\partial E}{\partial x_m} = -2(y_m - g_n(x_m)) g'_m(x_m)
    \]

    \textbf{Backpropagation}:
    For $n = M-O \dots M-O-H$:
    \[
    \frac{\partial E}{\partial x_m} = \sum_{k=m+1}^{M} \frac{\partial E}{\partial x_k} w_{mk} g'_m(x_m)
    \]

    \textbf{Weight derivatives}:
    For $l = 1 \dots M-O+1$, for $m = l+1 \dots M$
    \[
    \frac{\partial E}{\partial w_{lm}} = \frac{\partial E}{\partial x_m} g'_l(x_l)
    \]
    \textbf{Weight update rule:}
    \[
    w_{lm} = w_{lm} - \eta \frac{\partial E}{\partial w_{lm}}
    \]
    \begin{itemize}
        \item $\eta$: Learning rate.
        \item \textbf{Note:} Do a sign check, if you know that increasing the weight increases the error, therefore, you want to decrease the weight so negative sign. 
    \end{itemize}
\end{definition}

\subsubsection{Rewriting in Terms of Post-Activation Values}
\begin{derivation} FINISH THIS LATER ITS NOT CORRECT?
    We rewrite the following formulas in terms of the post-activation values:
    \[
    \hat{x}_i \triangleq g_i(x_i), \quad i = 1 \dots M
    \]
    
    \textbf{1. Forward Propagation}:
    For $j = l+1 \dots M$:
    \[
    x_j = \sum_{i=1}^{j-1} w_{ij} \hat{x}_i
    \]
    
    \textbf{2. Output Unit Error Derivatives}:
    For $n = M \dots M-O+1$:
    \[
    \frac{\partial E}{\partial x_m} = -2(y_m - \hat{x}_m) \hat{x}'_m
    \]
    
    \textbf{3. Backpropagation}:
    For $n = M-O \dots M-O-H$:
    \[
    \frac{\partial E}{\partial x_m} = \sum_{k=m+1}^{M} \frac{\partial E}{\partial x_k} w_{mk} \hat{x}'_m
    \]
    
    \textbf{4. Weight Derivatives}:
    For $l = 1 \dots M-O+1$, for $m = l+1 \dots M$:
    \[
    \frac{\partial E}{\partial w_{lm}} = \frac{\partial E}{\partial x_m} \hat{x}'_l
    \]
    
    \textbf{5. Weight Update Rule}:
    \[
    w_{lm} = w_{lm} - \eta \frac{\partial E}{\partial w_{lm}}
    \]
    
    \begin{itemize}
        \item $\eta$: Learning rate.
    \end{itemize}

\end{derivation}

\subsubsection{Relationship to perceptron learning}
\begin{intuition}
    \customFigure[0.75]{00_Images/RPL.png}{Relationship to perceptron learning.}
\end{intuition}

\subsection{SGD}
\begin{definition}
    \begin{enumerate}
        \item Initialize weights, e.g., to small random values.
        \item Iterate until stopping criteria met:
        \begin{enumerate}
            \item Select a training case.
            \item Apply forward propagation to get the values of \( x \)'s.
            \item Compute output unit error derivatives.
            \item Apply backpropagation to get \( \frac{\partial E}{\partial x} \)'s.
            \item Compute weight derivatives \( \frac{\partial E}{\partial w_{lm}} \) and update weights:
            \[
            w_{lm} \leftarrow w_{lm} - \eta \frac{\partial E}{\partial w_{lm}}
            \]
        \end{enumerate}
    \end{enumerate}
\end{definition}

\subsubsection{Local minima}
\begin{example}
    \customFigure[0.75]{00_Images/LM.png}{Local minima}
    \begin{itemize}
        \item Suppose were trying to approximate the data. Construct this using delta functions (i.e. neurons) with different amplitudes. 
        \item Each delta function will get adjusted to fit the data better to fit the local data better. 
        \item The learnt approximation is the bottom image, but there is a high local minima problem. 
    \end{itemize}
\end{example}

\subsubsection{Example}
\begin{example}
    \customFigure[0.75]{00_Images/E.png}{Example}
    \begin{itemize}
        \item 
    \end{itemize}
\end{example}

