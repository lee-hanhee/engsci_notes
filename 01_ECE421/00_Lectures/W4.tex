\textbf{Motivation:} We need a learning algorithm to find the $\underline{w}$ that minimizes $E_{in} (\underline{w})$ since we don't have a closed form solution (e.g. least squares wls)

\subsection{Problem setup for gradient descent}
\begin{intuition}
    \textbf{Given:} Differentiable function $f: \mathbb{R}^n \to \mathbb{R}$, 
    \vspace{1em}

    \textbf{Want to Solve:} $\min_{x \in \mathbb{R}^n} f(x)$ (unconstrained optimization problem)
\end{intuition}

\subsection{Optimization primer}
    \subsubsection{Gradient:}
    \begin{definition}
        For $f: \mathbb{R}^n \to \mathbb{R}$, its gradient $\nabla f: \mathbb{R}^n \to \mathbb{R}^n$ is defined as
            \[
            \nabla f(\underline{x}) =
            \begin{bmatrix}
            \frac{\partial f}{\partial x_1}(\underline{x}) \\
            \frac{\partial f}{\partial x_2}(\underline{x}) \\
            \vdots \\
            \frac{\partial f}{\partial x_n}(\underline{x})
            \end{bmatrix}
            \]
    \end{definition}

    \begin{example}
        \customFigure[0.5]{00_Images/G.png}{Gradient example.}
    \end{example}

    \subsubsection{Gradient and optimality:}
    \begin{definition}
        When $\nabla f(\underline{x}^*) = 0$, $\underline{x}^*$ can be any of 5 cases:

        \customFigure[0.75]{00_Images/GO.png}{Gradient and optimality.}
    \end{definition}

    \subsubsection{Convex functions:}
    \begin{definition}
        A function $f$ on $\mathbb{R}^m$ is \textbf{convex} if any line segment connecting two points of the graph of $f$ lies above or on the graph. 

            \begin{itemize}
                \item \textbf{Mathematically:} For any \( 0 \leq \alpha \leq 1 \) and any \( x_1 \) and \( x_2 \), $f(\alpha x_1 + (1-\alpha) x_2) \leq \alpha f(x_1) + (1-\alpha) f(x_2)$ 
                \begin{itemize}
                    \item \textbf{Equality:} If $\alpha = 0$, then $f(x_2) = f(x_2)$ and if $\alpha = 1$, then $f(x_1) = f(x_1)$
                    \item \textbf{Inequality:} For any values in between (i.e. strictly convex)
                \end{itemize}
                \item \textbf{Remark:} For convex functions, local minima are all global minima.
            \end{itemize}
    \end{definition}

    \begin{definition}
        $f$ is \textbf{concave} if $-f$ is convex. 
    \end{definition}

    \begin{definition}
        If $f(\alpha x_1 + (1-\alpha)x_2) < \alpha f(x_1) + (1-\alpha)f(x_2)$ for any \( 0 < \alpha < 1 \) and any \( x_1, x_2 \), then \( f \) is \textbf{strictly convex} 
        \begin{itemize}
            \item \textbf{Remark:} For strictly convex functions, there is only one global optimum.
        \end{itemize}
        \customFigure[0.5]{00_Images/CONVEX.png}{Convex function, but not strictly convex.}
    \end{definition}

    \begin{example}
        \customFigure[0.75]{00_Images/CNC.png}{Convex vs. Non-convex functions}
    \end{example}

    \subsubsection{Chain rule}
    \begin{definition}
        Let \( f: \mathbb{R}^n \to \mathbb{R} \) and \( h: \mathbb{R} \to \mathbb{R} \). Then
        \[
        \nabla_{\underline{x}} h(f(\underline{x})) = h'(f(\underline{x})) \nabla_{\underline{x}} f(\underline{x})
        \]
    \end{definition}

    \begin{example}
        \begin{align*}
            \nabla_{\underline{x}} \left( 1 + \log(\underline{a}^T \underline{x}) \right)^2 &= 2 \left( 1 + \log(\underline{a}^T \underline{x}) \right) \nabla_{\underline{x}} \left( 1 + \log(\underline{a}^T \underline{x}) \right) \\
            &= 2 \left( 1 + \log(\underline{a}^T \underline{x}) \right) \nabla_{\underline{x}} \log(\underline{a}^T \underline{x}) \\
            &= 2 \left( 1 + \log(\underline{a}^T \underline{x}) \right) \frac{1}{\underline{a}^T \underline{x}} \nabla_{\underline{x}}  (\underline{a}^T \underline{x}) \\
            &= \frac{2 \left( 1 + \log(\underline{a}^T \underline{x}) \right)}{\underline{a}^T \underline{x}} \underline{a} 
        \end{align*}
    \end{example}

    \begin{derivation}
        Let \( f: \mathbb{R}^n \to \mathbb{R} \) and \( h: \mathbb{R} \to \mathbb{R} \). We wish to find the gradient of the composite function \( h(f(\underline{x})) \), where \( \underline{x} = (x_1, x_2, \dots, x_n) \in \mathbb{R}^n \).

        \begin{enumerate}
            \item Consider the scalar function \( h(f(\underline{x})) \), where \( f(\underline{x}) \) is a differentiable function of \( n \) variables and \( h \) is a differentiable scalar function.
            
            \item By definition, the gradient of \( h(f(\underline{x})) \) with respect to \( \underline{x} \) is given by:
            \[
            \nabla_{\underline{x}} h(f(\underline{x})) = \left( \frac{\partial}{\partial x_1} h(f(\underline{x})), \frac{\partial}{\partial x_2} h(f(\underline{x})), \dots, \frac{\partial}{\partial x_n} h(f(\underline{x})) \right).
            \]
            
            \item For each \( i \)-th component of the gradient, we apply the standard single-variable chain rule:
            \[
            \frac{\partial}{\partial x_i} h(f(\underline{x})) = h'(f(\underline{x})) \frac{\partial}{\partial x_i} f(\underline{x}),
            \]
            where \( h'(f(\underline{x})) \) is the derivative of \( h \) with respect to its argument, and \( \frac{\partial}{\partial x_i} f(\underline{x}) \) is the partial derivative of \( f(\underline{x}) \) with respect to \( x_i \).

            \item Therefore, the gradient of \( h(f(\underline{x})) \) becomes:
            \[
            \nabla_{\underline{x}} h(f(\underline{x})) = h'(f(\underline{x})) \nabla_{\underline{x}} f(\underline{x}),
            \]
            where \( \nabla_{\underline{x}} f(\underline{x}) = \left( \frac{\partial}{\partial x_1} f(\underline{x}), \frac{\partial}{\partial x_2} f(\underline{x}), \dots, \frac{\partial}{\partial x_n} f(\underline{x}) \right) \) is the gradient of \( f(\underline{x}) \).

            \item Thus, we have shown that the gradient of \( h(f(\underline{x})) \) with respect to \( \underline{x} \) is the product of \( h'(f(\underline{x})) \), the derivative of the outer function, and \( \nabla_{\underline{x}} f(\underline{x}) \), the gradient of the inner function.
        \end{enumerate}

        \textbf{Conclusion:} The chain rule for the gradient of the composite function \( h(f(\underline{x})) \) is:
        \[
        \nabla_{\underline{x}} h(f(\underline{x})) = h'(f(\underline{x})) \nabla_{\underline{x}} f(\underline{x}).
        \]
    \end{derivation}

\subsection{Simple algorithm to find min f(x) for 1D}
\begin{intuition}
    Assuming $f(x)$ is convex for a 1D space with \( f'(x) = \frac{df}{dx}(x) \), then 
    \begin{enumerate}
    
        \item At \( x = x^* \), \( f'(x) = 0 \)
        \begin{itemize}
            \item What does it mean? \( x \) is optimal.
        \end{itemize}
    
        \item If \( x > x^* \), \( f'(x) > 0 \)
        \begin{itemize}
            \item What does it mean? \( f(x) \) increases as \( x \) increases.
        \end{itemize}
    
        \item If \( x < x^* \), \( f'(x) < 0 \)
        \begin{itemize}
            \item What does it mean? \( f(x) \) decreases as \( x \) increases.
        \end{itemize}
        
    \end{enumerate}   
    \customFigure[0.5]{00_Images/GE.png}{Gradient of a 1D function in red on the x-axis.}
\end{intuition}

\begin{definition}
    \begin{enumerate}
        \item Initialize $x = x_0$.
        \item If $f'(x) \approx 0$, then stop (i.e. found the global minimum).
        \item If $f'(x) > 0$, then $x = x - \epsilon$ (i.e. moving closer to the minimum since it will be on the RS of the minimum so subtract a constant).
        \item If $f'(x) < 0$, then $x = x + \epsilon$ (i.e. moving closer to the minimum since it will be on the LS of the minimum so add a constant).
        \item Go to step 2.
    \end{enumerate}
    \vspace{1em}

    \begin{itemize}
        \item $\epsilon$: Step size.
        \item \textbf{Intuition:} We are going in the opposite direction of the gradient (i.e. slope) to determine the step direction. This will generalize the n dimensions.
    \end{itemize}
\end{definition}

\begin{intuition}
    What should be the value of \( \varepsilon \)?
        \begin{enumerate}
            \item What happens if we use a large \( \varepsilon \)?
            \begin{itemize}
                \item Bounce around, especially close to the optimal point since it will overshoot the global minimum.
            \end{itemize}
            
            \item What happens if we use a small \( \varepsilon \)?
            \begin{itemize}
                \item It can take forever to get to the global minimum.
            \end{itemize}
        \end{enumerate}  
        \vspace{1em}

    Therefore, use time-varying step size, \( \varepsilon_t \) by starting with large step size and decrease it over time.
    \begin{itemize}
        \item \( t \): iteration number.
        \item \textbf{Intuition:} We want to get close to the optimal point, and when around the optimal point, we want to be more cautious (small step sizes).
    \end{itemize}
\end{intuition}

\begin{example}
    \( \varepsilon_t = \frac{1}{t} \)
    \begin{itemize}
        \item If the function is strongly convex (e.g., \( f''(x) > 0 \ \forall x \in \mathbb{R} \)), then we are guaranteed to converge to the optimal solution.
    \end{itemize}
\end{example}

\subsection{Simple algorithm to find min f(x) for ND space}
\begin{intuition}
    Given the current location \( x \), what should be the next step?
    \[
    x' \leftarrow x + \varepsilon u
    \]
    \begin{itemize}
        \item \( \varepsilon \): step size
        \item \( u \): direction of update, \( \|u\| = 1 \).
    \end{itemize}
    \customFigure[0.5]{00_Images/DOG.png}{Direction of gradient for x1 and x2 for the 2D case.}
\end{intuition}

    \subsubsection{2D space}
    \begin{intuition}
        \begin{enumerate}
            \item Where should we go from \( x_0 \)? Choose the direction s.t. it gives you the smallest \( f(\underline{x}') \) (in a small step \( \varepsilon \)).
            \[
            \min f(\underline{x}') = f(\underline{x} + \varepsilon  \underline{u} )
            \]

            \customFigure[0.5]{00_Images/GD.png}{When we make a contour map of x1 and x2, we can see the gradient is in blue and the negative of the gradient is in red, so we are moving towards the minimum.}
        
            \item By Taylor series:
            \[
            f(\underline{x} + \varepsilon \underline{u}) = f(\underline{x}) + \varepsilon \underline{u}^T \nabla f(\underline{x}) + O(\varepsilon^2 \|\underline{u}\|^2)
            \]
            \begin{itemize}
                \item Neglect $O(\varepsilon^2 \|\underline{u}\|^2)$.
            \end{itemize}
        
            \item So, we want to minimize \( \underline{u}^T \nabla f(\underline{x}) \):
            \begin{itemize}
                \item \( \underline{u}^T \nabla f(\underline{x}) = \|\underline{u}\| \| \nabla f(\underline{x})\| \cos \theta \), which is minimized when \( \cos \theta = -1 \Longleftrightarrow \theta = 180^\circ \)
                \item i.e. Steepest descent occurs when $\underline{u}$ is moving in the opposite direction of $\nabla f(\underline{x})$ 
            \end{itemize}
            
            \item So, \( \underline{u}^* = -\frac{\nabla f(\underline{x})}{\|\nabla f(\underline{x})\|} \) (i.e. define it in the opposite direction of the gradient)
        \end{enumerate}
        \vspace{1em}

        \begin{itemize}
            \item \( \nabla f(x) \) points in the direction where \( f(x) \) has maximum ascent.
        
            \item Observe that:
            \[
            \varepsilon u^* = -\frac{\varepsilon}{\|\nabla f(x)\|} \nabla f(x)
            \]
            \( \varepsilon_t = \frac{\varepsilon}{\|\nabla f(x)\|} \).
        \end{itemize}

    \end{intuition}

\subsection{Gradient descent algorithm}
\begin{definition}
    \begin{enumerate}
        \item Initialize $\underline{x}_0$ (typically at random).
        \item For $t = 0, 1, 2, \dots$:
        \begin{enumerate}
            \item Compute $\underline{g}_t = \nabla f(\underline{x}_t)$. (i.e. gradient)
            \item Select direction $\underline{v}_t = -\underline{g}_t$. (i.e. negative direction of gradient)
            \item Update $\underline{x}_{t+1} = \underline{x}_t + \epsilon_t \underline{v}_t$. (i.e. update rule)
            \item Stop if stopping criteria are reached.
        \end{enumerate}
        \item End for
    \end{enumerate}
    \vspace{1em}

    \begin{itemize}
        \item Condition for stopping: \( \nabla f(\underline{x}_t) \approx 0 \) is reasonable. (i.e. when we almost hit the minimum)

        \item \( \varepsilon_t \): Learning rate

        \item If we have a constant learning rate for Gradient Descent alg. (i.e., \( \varepsilon_t = \alpha \ \forall t \)), then
        \[
        \varepsilon_t \underline{v}_t = -\alpha \underline{g}_t = -\alpha \nabla f(\underline{x}) = -\alpha \|\nabla f(\underline{x})\| \frac{\nabla f(\underline{x})}{\|\nabla f(\underline{x})\|}
        \]
        \begin{itemize}
            \item \textbf{Note:} Actual step size $\alpha \|\nabla f(\underline{x})\|$ diminishes as \( \underline{x}_t \rightarrow \underline{x}^* \) (i.e. because we are getting closer to the gradient being to 0, so this product will get smaller as well)
        \end{itemize}
    \end{itemize}
\end{definition}

\subsection{Gradient descent performance}
\begin{theorem}

    \textbf{Assumptions:} (Choose any $\gamma > 0$). If
    \begin{enumerate}
        \item $f$ is sufficiently smooth (if not, can't run gradient descent)
        \item $f$ is convex (if not, may get stuck in saddle point)
        \item $f$ has at least one global optimum (if not, may not terminate, e.g., $y=x$)
        \item $\epsilon$ is sufficiently small (if not, may diverge because it will bounce around the global minimum)
    \end{enumerate}

    \textbf{Conclusion:} If run long enough, gradient descent will return a value within $\gamma$ of a global optimum. (i.e. if we have these 4 conditions met, we can always find a global optimum or a close enough solution)

\end{theorem}

\subsection{Using Gradient Descent for Logistic Regression}
\begin{definition}
    Gradient Descent would update \( \underline{w}_t \) by
    \begin{enumerate}
        \item Gradient: $\underline{g}_t = \nabla E_{\text{in}}(\underline{w}_t) = \frac{1}{N} \sum_{n=1}^{N} \nabla e_n(\underline{w}_t)$
        \begin{itemize}
            \item  \( \frac{1}{N} \sum_{n=1}^{N} \nabla e_n(\underline{w}_t) \): Average per sample error gradient.
        \end{itemize}
        \item Opposite direction of gradient: $\underline{v}_t = - \underline{g}_t$
        \item Update rule: $\underline{w}_{t+1} = \underline{w}_t + \epsilon_t \underline{v}_t = \underline{w}_{t+1} \implies \underline{w}_t - \varepsilon_t \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t)$
        \begin{itemize}
            \item  $\nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t) = \frac{1}{N} \sum_{n=1}^{N} \nabla_{\underline{w}_t} e_n(\underline{w}_t)$: Average gradient of the points.
        \end{itemize}
    \end{enumerate}
\end{definition}

\begin{intuition}
    \begin{itemize}
        \item \textbf{Good News:}
            \begin{itemize}[left=0pt]
                \item Convex function: $\frac{1}{N} \sum_{n=1}^{N} \log\left(1 + e^{-y_n \underline{w}_t^T \underline{x}_n}\right)$
                \item Therefore, Gradient Descent (GD) converges to the globally minimum point.
            \end{itemize}
        
        \item \textbf{Bad News:}
        \begin{itemize}[left=0pt]
            \item To run one iteration of Gradient Descent, we need to find \( \nabla e_n(\underline{w}_t) \) for all \( n \in \{1, 2, \dots, N\} \) points.
            \item This is costly, especially when \( N \) is large (big data), therefore, let's look at SGD.
        \end{itemize}
    \end{itemize}
\end{intuition}

\begin{derivation}
    \textbf{Logistic Regression In-Sample Error:}
    \[
    E_{\text{in}}(\underline{w}) = \frac{1}{N} \sum_{n=1}^{N} e_n(\underline{w}) = \frac{1}{N} \sum_{n=1}^{N} \log\left(1 + e^{-y_n \underline{w}^T \underline{x}_n}\right)
    \]

    \begin{align*}
        \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t) &= \frac{1}{N} \sum_{n=1}^{N} \nabla_{\underline{w}_t} e_n(\underline{w}_t) \\
        &= \frac{1}{N} \sum_{n=1}^{N} \nabla_{\underline{w}_t} \log\left(1 + e^{-y_n \underline{w}_t^T \underline{x}_n}\right) \\
        &= \frac{1}{N} \sum_{n=1}^{N} \frac{1}{1 + e^{-y_n \underline{w}_t^T \underline{x}_n}} \nabla_{\underline{w}_t} \left( 1 + e^{-y_n \underline{w}_t^T \underline{x}_n} \right) \\ 
        &= \frac{1}{N} \sum_{n=1}^{N} \frac{1}{1 + e^{-y_n \underline{w}_t^T \underline{x}_n}} e^{-y_n \underline{w}_t^T \underline{x}_n} (-y_n \underline{x}_n) \\
        &= \frac{1}{N} \sum_{n=1}^{N} \frac{-y_n \underline{x}_n}{1 + e^{y_n \underline{w}_t^T \underline{x}_n}} 
    \end{align*}
    \begin{itemize}
        \item \textbf{Note:} The last line is achieved by dividing $e^{-y_n \underline{w}_t^T \underline{x}_n}$ in the numerator and denominator. 
    \end{itemize}

\end{derivation}

\subsection{Stochastic Gradient Descent (SGD)}
\begin{definition}
    In each iteration:
    \begin{enumerate}[left=0pt]
        \item Choose uniformly at random a datapoint \( (\underline{x}_n, y_n) \).
        \item \( \underline{g}_t = \nabla_{\underline{w}_t} e_n(\underline{w}_t) \) \quad \text{(use the gradient of the \(n\)-th sample to update \( \underline{x}_t \))}.
        \item \( \underline{v}_t = -\underline{g}_t \).
        \item \( \underline{x}_{t+1} = \underline{x}_t + \varepsilon_t \underline{v}_t \).
    \end{enumerate}
\end{definition}

\begin{intuition}
    \textbf{But does it work?}
    \begin{itemize}[left=0pt]
        \item It has been proven that for convex functions and under mild conditions, SGD will find the solution.
        \item Don't worry about the proof. Don't worry about the conditions.
    \end{itemize}
\end{intuition}

    \subsubsection{gt of SGD is an unbiased estimate of gradient of error}
    \begin{intuition}
        \begin{itemize}
            \item \textbf{Let's study the expected update direction derived by SGD in each step:}

            \begin{align*}
                \mathbb{E}[\underline{g}_t] &= \mathbb{P}[n=1] \nabla_{\underline{w}_t} e_1(\underline{w}_t) + \dots + \mathbb{P}[n=N] \nabla_{\underline{w}_t} e_N(\underline{w}_t) \\
                &= \frac{1}{N} \sum_{n=1}^{N} \nabla_{\underline{w}_t} e_n(\underline{w}_t) = \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t) \quad \text{\textit{(True gradient)}}
            \end{align*}
    
            \item \textbf{By SGD, \( \underline{g}_t \) is an "unbiased estimator" of the true gradient \( \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t) \):}
            \begin{itemize}[left=0pt]
                \item It is a noisy version of \( \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t) \), but it is unbiased because:
                \[
                \mathbb{E}[\underline{g}_t] = \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t)
                \]
            \end{itemize}
        \end{itemize}
    \end{intuition}

    \subsubsection{Full GD complexity vs. SGD complexity}
    \begin{intuition}
        \begin{itemize}
            \item \textbf{Full GD ("batch GD"):} Per iteration complexity is \( O(Nd) \).
            \begin{itemize}[left=0pt]
                \item When \( N \) is large, it is costly, where $d$ is the number of features.
            \end{itemize}
    
            \item \textbf{SGD:} Per iteration complexity is \( O(d) \).
            \begin{itemize}[left=0pt]
                \item \textbf{Key:} Complexity of \( N \) iterations of SGD \( = \) complexity of 1 iteration of GD.
                \begin{itemize}
                    \item But with SGD, in each iteration, we update based on some noisy estimate of the gradient.
                    \item Numerically, SGD outperforms full GD. This is because usually there are high redundancies in a large dataset.
                    \begin{itemize}
                        \item So, you don't have to use all points to find the right direction.
                    \end{itemize}
                \end{itemize} 
            \end{itemize}
        \end{itemize}
    \end{intuition}

    \subsubsection{Mini-batch GD is full-batch GD combined with SGD}
    \begin{definition}
        \textbf{Mini-batch GD:} In each iteration:
        \begin{enumerate}[left=0pt]
            \item Choose \( M \) sample \( r_1, r_2, \dots, r_M \in \{1, \dots, N\} \) uniformly at random.
            \item \( \underline{g}_t = \frac{1}{M} \sum_{i=1}^{M} \nabla_{\underline{w}_t} e_{r_i}(\underline{w}_t) \).
            \item \( \underline{v}_t = -\underline{g}_t \).
            \item \( \underline{w}_{t+1} = \underline{w}_t + \varepsilon_t \underline{v}_t \).
        \end{enumerate}
    \end{definition}

    \begin{intuition}
        \textbf{Benefits:}
        \begin{itemize}[left=0pt]
            \item \( \underline{g}_t \) is a more reliable estimator of the true gradient.
            \item Per iteration complexity: \( O(Md) \).
            \item In practice, we use multi-core CPUs/GPUs. So, we can find the gradient for \( M \) datapoints in parallel and get a better estimator of the update direction.
        \end{itemize}
    \end{intuition}

\subsection{PLA is an Extremer Version of Logistic Regression with SGD}
\begin{definition}
    \begin{itemize}
        \item \textbf{Logistic regression with SGD updates:}
        \[
        \nabla_{\underline{w}_t} E_{\text{in}}(\underline{w}_t) = \frac{-y_n \underline{x}_n}{1 + e^{y_n \underline{w}_t^T \underline{x}_n}}
        \]
        and
        \[
        \underline{w}_{t+1} = \underline{w}_t + \varepsilon_t \frac{y_n \underline{x}_n}{1 + e^{y_n \underline{w}_t^T \underline{x}_n}}.
        \]
    
        \item \textbf{When we randomly pick some \( \underline{x}_n \) which is misclassified:}
        \begin{itemize}[left=0pt]
            \item \( y_n \underline{w}_t^T \underline{x}_n < 0 \quad \Rightarrow \quad 0 < e^{y_n \underline{w}_t^T \underline{x}_n} < 1 \).
            \item The SGD update would be:
            \[
            \underline{w}_{t+1} = \underline{w}_t + \frac{\varepsilon_t}{1 + e^{y_n \underline{w}_t^T \underline{x}_n}} (y_n \underline{x}_n)
            \]
            \item In extreme case, \( e^{y_n \underline{w}_t^T \underline{x}_n} \approx 0 \), and the update rule would be:
            \[
            \underline{w}_{t+1} \approx \underline{w}_t + \varepsilon_t (y_n \underline{x}_n) \quad \textit{(same as PLA if \( \varepsilon_t = 1 \))}.
            \]
        \end{itemize}
    

        \item \textbf{When we randomly pick some \( \underline{x}_n \) which is correctly classified:}
        \begin{itemize}[left=0pt]
            \item \( y_n \underline{w}_t^T \underline{x}_n > 0 \quad \Rightarrow \quad 1 < e^{y_n \underline{w}_t^T \underline{x}_n} \).
            \item The SGD update would be:
            \[
            \underline{w}_{t+1} = \underline{w}_t + \frac{\varepsilon_t}{1 + e^{y_n \underline{w}_t^T \underline{x}_n}} (y_n \underline{x}_n)
            \]
            \item In extreme case, \( \frac{1}{1 + e^{y_n \underline{w}_t^T \underline{x}_n}} \approx 0 \), and the update rule would be:
            \[
            \underline{w}_{t+1} \approx \underline{w}_t \quad \textit{(same as PLA as it does not update \( \underline{w} \) for correctly classified points)}.
            \]
        \end{itemize}
    \end{itemize}
\end{definition}

\subsection{Multiclass Logistic Regression}
\begin{definition}
    \begin{itemize}
        \item \textbf{Hypothesis Set:} Let \( \Omega = \{ \underline{w}_{(1)}, \underline{w}_{(2)}, \dots, \underline{w}_{(c)} \} \) be the weight vectors for \( c \) classes.
        

        \item \textbf{Hypothesize that:}
        \begin{align*}
            \mathbb{P}[y_n &= i | \underline{x}_n] = \frac{e^{\underline{w}_{(i)}^T \underline{x}_n}}{\sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n}}, \quad \text{for} \ i \in \{1, \dots, c\} \\    
            &= \hat{P}_{\Omega}(i | \underline{x}_n) \quad \textit{(Softmax function)} 
        \end{align*}
        

        \item \textbf{Error Criterion:}
        \[
        e_n(\Omega) = -\log \hat{P}_{\Omega}(y_n | \underline{x}_n) = -\underline{w}_{(y_n)}^T \underline{x}_n + \log \sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n}
        \]
        
    \end{itemize}
\end{definition}

\begin{example}
    \customFigure[0.75]{00_Images/EX.png}{Example of how the 3rd weight line is the farthest away for so it has the largest probability}
\end{example}

\subsection{SGD update}
\begin{definition}
    \begin{itemize}
        \item \textbf{SGD update rule:} To minimize \( E_{\text{in}}(\Omega) \), we need to use GD. Hence, we must find the gradient.
        \[
            \nabla_{\Omega} e_n(\Omega) =
            \begin{bmatrix}
                \frac{\partial e_n}{\partial \underline{w}_{(1)}}(\Omega) \\
                \frac{\partial e_n}{\partial \underline{w}_{(1)}}(\Omega) \\
                \vdots \\
                \frac{\partial e_n}{\partial \underline{w}_{(c)}}(\Omega) \\
                \vdots \\
                \frac{\partial e_n}{\partial \underline{w}_{(c)}}(\Omega)
            \end{bmatrix}
            =
            \begin{bmatrix}
                \nabla_{\underline{w}_{(1)}} e_n(\Omega) \\
                \vdots \\
                \nabla_{\underline{w}_{(c)}} e_n(\Omega)
            \end{bmatrix}
        \]
        Therefore, the update rule is
        \[
        \Omega_{t+1} = \Omega_t - \varepsilon_t \nabla_{\Omega_t} e_n(\Omega)
        \quad \Rightarrow \quad
        \begin{bmatrix}
            \underline{w}_{t+1}(1) \\
            \vdots \\
            \underline{w}_{t+1}(c)
        \end{bmatrix}
        =
        \begin{bmatrix}
            \underline{w}_t(1) \\
            \vdots \\
            \underline{w}_t(c)
        \end{bmatrix}
        - \varepsilon_t
        \begin{bmatrix}
            \nabla_{\underline{w}_t(1)} e_n(\Omega) \\
            \vdots \\
            \nabla_{\underline{w}_t(c)} e_n(\Omega)
        \end{bmatrix}
        \]
        
        \item \textbf{SGD update:}
        
        In each iteration \( t \):
        \begin{itemize}[left=0pt]
            \item Set \( \Omega_t = \{\underline{w}_t(1), \dots, \underline{w}_t(c)\} \).
            \item Sample a datapoint \( n \sim \text{Uniform}(\{1, \dots, N\}) \).
            \item For \( i = 1, 2, \dots, c \):
            \begin{enumerate}
                \item $\underline{g}_t(i) = \nabla_{\underline{w}_t(i)} e_n(\Omega)$
                \item $\underline{v}_t(i) = -\underline{g}_t(i)$
                \item $\underline{w}_{t+1}(i) = \underline{w}_t(i) + \varepsilon_t \underline{v}_t(i)$
            \end{enumerate}
        \end{itemize}
    \end{itemize}
\end{definition}

\subsection{Computing gradient of error}
\begin{definition}
    \textbf{Computing \( \nabla_{\underline{w}_{(i)}} e_n(\Omega_t) \):}

    \begin{align*}
        e_n(\Omega_t) &= -\log \hat{\mathbb{P}}_{\Omega_t}(y_n | \underline{x}_n) \\
        &= -\log \frac{e^{\underline{w}_{(y_n)}^T \underline{x}_n}}{\sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n}} \\
        &= -\underline{w}_{(y_n)}^T \underline{x}_n + \log \left( \sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n} \right) 
    \end{align*}
    \vspace{1em}

    \textbf{For \( i = y_n \):}
    \[
    \nabla_{\underline{w}_{t(i)}} e_n(\Omega_t) = \nabla_{\underline{w}_{t(y_n)}} \left( -\underline{w}_{(y_n)}^T \underline{x}_n + \log \left( \sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n} \right) \right)
    \]
    \[
    = -\underline{x}_n + \frac{e^{\underline{w}_{(y_n)}^T \underline{x}_n}}{\sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n}} \underline{x}_n
    \]
    \vspace{1em}

    \textbf{For \( i \neq y_n \):}
    \[
    \nabla_{\underline{w}_{t(i)}} e_n(\Omega_t) = \nabla_{\underline{w}_{t(i)}} \left( -\underline{w}_{(y_n)}^T \underline{x}_n + \log \left( \sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n} \right) \right)
    \]
    \[
    = \frac{e^{\underline{w}_{(i)}^T \underline{x}_n}}{\sum_{j=1}^{c} e^{\underline{w}_{(j)}^T \underline{x}_n}} \underline{x}_n
    \]
\end{definition}

\subsection{Softmax Logistic Regression for Binary Classification}
\begin{intuition}
    \textbf{What is the relation between Softmax logistic regression for \( c = 2 \) and binary logistic regression?}

    \[
    \hat{P}_{\Omega}(1 | \underline{x}_n) = \frac{e^{\underline{w}_{(1)}^T \underline{x}_n}}{e^{\underline{w}_{(1)}^T \underline{x}_n} + e^{\underline{w}_{(2)}^T \underline{x}_n}} = \frac{e^{(\underline{w}_{(1)} - \underline{w}_{(2)})^T \underline{x}_n}}{1 + e^{(\underline{w}_{(1)} - \underline{w}_{(2)})^T \underline{x}_n}}
    \]
    \begin{itemize}
        \item The last line is acheived by dividing by $e^{\underline{w}_{(2)}^T \underline{x}_n}$
    \end{itemize}

    \[
    \hat{P}_{\Omega}(2 | \underline{x}_n) = \frac{e^{\underline{w}_{(2)}^T \underline{x}_n}}{e^{\underline{w}_{(1)}^T \underline{x}_n} + e^{\underline{w}_{(2)}^T \underline{x}_n}} = 1 - \hat{P}_{\Omega}(1 | \underline{x}_n)
    \]

    It is logistic regression with \( \underline{w} = \underline{w}_{(1)} - \underline{w}_{(2)} \).
\end{intuition}

\subsection{Can we use GD/SGD for Linear Regression? Yes, you can}
\begin{definition}
    \begin{itemize}
        \item \textbf{In-sample error:} $E_{\text{in}}(\underline{w}) = \frac{1}{N} \sum_{n=1}^{N} e_n(\underline{w}) = \frac{1}{N} \sum_{n=1}^{N} (\underline{w}^T \underline{x}_n - y_n)^2$
    
        \item \textbf{\( \underline{w}_{ls} \), the optimal solution:}
        \begin{itemize}
            \item $ \underline{w}_{ls} = (X^T X)^{-1} X^T y $
            \item \textbf{Complexity:} \( O(Nd^2 + d^3) \)
        \end{itemize}
    
        \item \textbf{\( E_{\text{in}}(\underline{w}) \) is convex. So, with GD/SGD, as \( t \to \infty \), \( \underline{w}_t \) converges to the optimal solution, i.e., \( \underline{w}_{\text{LS}} \).}
    
        \[
        \nabla_{\underline{w}} e_n(\underline{w}) = 2 (\underline{w}^T \underline{x}_n - y_n) \underline{x}_n
        \]
    
        \item \textbf{Time complexity of SGD, Full-GD, Mini-batch GD:}
        \begin{itemize}
            \item SGD: \( O(d) \)
            \item Full GD: \( O(Nd) \)
            \item Mini GD: \( O(Md) \)
        \end{itemize}
    \end{itemize}
\end{definition}

\begin{intuition}
    \textbf{Benefits of GD over Least-Squares Method:}
    \begin{enumerate}
        \item \textbf{Better Complexity.}
        
        \item \textbf{Most often, we are not interested in finding the exact optimal solution.}
        \begin{itemize}
            \item We only care about test error, not \( E_{\text{in}} \) (train error).
            \item If we have noisy data, the optimal solution leads to overfitting.
            \begin{itemize}
                \item In practice, we run GD for a few iterations.
                \item Then, we do validation and stop when the validation error starts deteriorating.
            \end{itemize}
        \end{itemize}
    \end{enumerate}
    \customFigure[0.5]{00_Images/B.png}{Error vs. iteration.}
\end{intuition}

\subsection{Non-convex functions}
\begin{intuition}
    \begin{itemize}
        \item Often have multiple extrema.
        \item Also may have saddle points.
    \end{itemize}
    \customFigure[0.5]{00_Images/NC.png}{Non-convex functions}
\end{intuition}

    \subsubsection{Saddle points}
    \begin{definition}
        A saddle point is minimum in some direction, and maximum in another direction. 
        \customFigure[0.75]{00_Images/SP.png}{Saddle point}
    \end{definition}

    \begin{intuition}
        \begin{itemize}
            \item \textbf{Gradient Descent (GD)} would have very slow progress near saddle points.
            \begin{itemize}
                \item The preset number of iterations will run out.
            \end{itemize}
            
            \item In high-dimensional space, it's highly likely to have saddle points.
            
            \item Most often, we are dealing with high-dimensional spaces.
            \begin{itemize}
                \item How to use GD in these high-dimensional spaces? SGD with momentum
            \end{itemize}
        \end{itemize}
    \end{intuition}

\subsection{SGD with Momentum (heavy ball momentum)}
    
    \begin{intuition} \textbf{Motivation:}
        \textbf{Basic SGD:} 
        \begin{enumerate}
            \item $\underline{g}_t = \nabla e_n(\underline{w}_t)$
            \item $\underline{v}_t = -\underline{g}_t$
            \item $\underline{w}_{t+1} = \underline{w}_t + \epsilon_t \underline{v}_t$
        \end{enumerate}

        \textbf{New idea:} Add a \textit{momentum} (a push) so that SGD would not stop if \( \nabla e_n(w_t) \approx 0 \).

        When you let this heavy ball go, would it stop at the flat region? No, because of inertia / momentum.

        \customFigure[0.5]{00_Images/BALL.png}{Ball rolling down}
    \end{intuition}

    \begin{definition}
        \begin{enumerate}
            \item $\underline{g}_t = \nabla e_n(\underline{w}_t)$
            \item $\underline{v}_t = -\epsilon_t \underline{g}_t + \mu \underline{v}_{t-1}$
            \begin{itemize}
                \item $0 < \mu < 1 \quad \text{e.g.,} \, \mu = 0.9$ (i.e. accumulation of your previous movements)
            \end{itemize}
            \item $\underline{w}_{t+1} = \underline{w}_t + \underline{v}_t$
        \end{enumerate}
    \end{definition}

    \begin{example}
        An example of how it accumulates the previous movements:
        \[
        \underline{v}_{t-1} = -\epsilon_{t-1} \underline{g}_{t-1} + \mu \underline{v}_{t-2}
        \]

        \[
        \underline{v}_{t-1} = -\epsilon_{t-1} \underline{g}_{t-1} + \mu \left( -\epsilon_{t-2} \underline{g}_{t-2} + \mu \underline{v}_{t-3} \right)
        \]
    \end{example}

    \begin{example}
        \customFigure[0.75]{00_Images/SGDM.png}{SGD with momentum where the red and blue lines are used to calculate the green lines which is the actual direction towards the minimum.}
    \end{example}

    \subsubsection{How does momentum help?}
    \begin{intuition}
        \begin{enumerate}
            \item Momentum helps SGD escape flat regions, saddle points, and shallow local optimum. 
                \customFigure[0.5]{00_Images/1P.png}{Example of momentum escaping points}
                \begin{itemize}
                    \item Even though the gradient is $0$, the term $v_t$ is non-zero, making it move out of this gradient 0 zone. 
                \end{itemize}
                \customFigure[0.75]{00_Images/EX1.png}{Example of SGD vs. SGD + Momentum}
            \item Momentum can lead to faster convergence, even for convex functions. Consider the stretched ellipsoid: \(f(x_1, x_2) = 0.1x_1^2 + 2x_2^2.\)
            \customFigure[0.5]{00_Images/EP.png}{Example of the ellipsoid}

            \begin{itemize}
                \item Its minimum is at (0,0).
                \item This function is very flat in the direction of \(x_1\).
                \item The gradient in the \(x_2\) direction is much higher than in \(x_1\).
                \item With a small learning rate, SGD won’t diverge in the \(x_2\) direction but is very slow in the \(x_1\) direction.
                \item With a large learning rate, SGD progresses more rapidly in \(x_1\), but diverges in \(x_2\).
            \end{itemize}
            \customFigure[0.75]{00_Images/EP1.png}{Showing how a slow learning rate, causes it to move slow in the x1 direction, but a large learning rate causes it to diverge in the x2 direction.}
        \end{enumerate}
    \end{intuition}

\subsubsection{Nestrov Momentum}
\begin{intuition}
    \textbf{Motivation:} 

    \begin{itemize}
        \item Heavy-ball momentum method works well in practice.
        \item But we don’t have any theoretical proof for it.
        \item Nesterov, in 1983, modified the momentum and could prove nice theoretical guarantees.
    \end{itemize}
\end{intuition}
\begin{definition}
    You update your location with your velocity first, and then take the gradient.
    \begin{enumerate}
        \item $\underline{v}_t = -\epsilon_t \nabla e_n\left(\underline{w}_t + \mu \underline{v}_{t-1}\right) + \mu \underline{v}_{t-1}$
        \item $\underline{w}_{t+1} = \underline{w}_t + \underline{v}_t$
    \end{enumerate}
    
    \begin{itemize}
        \item Probably better convergence for convex functions:
        \begin{itemize}
            \item Full GD: \( |f(\underline{w}_t) - f(\underline{w}^*)| = O\left(\frac{1}{t}\right) \)
            \item With Nesterov: \( |f(\underline{w}_t) - f(\underline{w}^*)| = O\left(\frac{1}{t^2}\right) \)
        \end{itemize}
    \end{itemize}    
\end{definition}