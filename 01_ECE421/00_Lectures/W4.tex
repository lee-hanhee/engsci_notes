\textbf{Motivation:} We need a learning algorithm to find the $\underline{w}$ that minimizes $E_{in} (\underline{w})$ since we don't have a closed form solution

\subsection{Problem setup for gradient descent}
\begin{intuition}
    \textbf{Given:} Differentiable function $f: \mathbb{R}^n \to \mathbb{R}$, 
    \vspace{1em}

    \textbf{Want to Solve:} $\min_{x \in \mathbb{R}^n} f(x)$ (unconstrained optimization problem)
\end{intuition}

\subsection{Optimization primer}
    \subsubsection{Gradient:}
    \begin{definition}
        For $f: \mathbb{R}^n \to \mathbb{R}$, its gradient $\nabla f: \mathbb{R}^n \to \mathbb{R}^n$ is defined as
            \[
            \nabla f(\underline{x}) =
            \begin{bmatrix}
            \frac{\partial f}{\partial x_1}(\underline{x}) \\
            \frac{\partial f}{\partial x_2}(\underline{x}) \\
            \vdots \\
            \frac{\partial f}{\partial x_n}(\underline{x})
            \end{bmatrix}
            \]
    \end{definition}

    \begin{example}
        \customFigure[0.5]{00_Images/G.png}{Gradient example.}
    \end{example}

    \subsubsection{Gradient and optimality:}
    \begin{definition}
        When $\nabla f(\underline{x}^*) = 0$, $\underline{x}^*$ can be any of 5 cases:

        \customFigure[0.75]{00_Images/GO.png}{Gradient and optimality.}
    \end{definition}

    \subsubsection{Convex functions:}
    \begin{definition}
        A function $f$ on $\mathbb{R}^m$ is \textbf{convex} if any line segment connecting two points of the graph of $f$ lies above or on the graph. 

            \begin{itemize}
                \item \textbf{Mathematically:} For any \( 0 \leq \alpha \leq 1 \) and any \( x_1 \) and \( x_2 \), $f(\alpha x_1 + (1-\alpha) x_2) \leq \alpha f(x_1) + (1-\alpha) f(x_2)$ 
                \begin{itemize}
                    \item \textbf{Equality:} If $\alpha = 0$, then $f(x_2) = f(x_2)$ and if $\alpha = 1$, then $f(x_1) = f(x_1)$
                    \item \textbf{Inequality:} For any values in between (i.e. strictly convex)
                \end{itemize}
                \item \textbf{Remark:} For convex functions, local minima are all global minima.
            \end{itemize}
    \end{definition}

    \begin{definition}
        $f$ is \textbf{concave} if $-f$ is convex. 
    \end{definition}

    \begin{definition}
        If $f(\alpha x_1 + (1-\alpha)x_2) < \alpha f(x_1) + (1-\alpha)f(x_2)$ for any \( 0 < \alpha < 1 \) and any \( x_1, x_2 \), then \( f \) is \textbf{strictly convex} 
        \begin{itemize}
            \item \textbf{Remark:} For strictly convex functions, there is only one global optimum.
        \end{itemize}
        \customFigure[0.5]{00_Images/CONVEX.png}{Convex function, but not strictly convex.}
    \end{definition}

    \begin{example}
        \customFigure[0.75]{00_Images/CNC.png}{Convex vs. Non-convex functions}
    \end{example}

\subsection{Simple algorithm to find min f(x) for 1D}
\begin{intuition}
    Assuming $f(x)$ is convex for a 1D space with \( f'(x) = \frac{df}{dx}(x) \), then 
    \begin{enumerate}
    
        \item At \( x = x^* \), \( f'(x) = 0 \)
        \begin{itemize}
            \item What does it mean? \( x \) is optimal.
        \end{itemize}
    
        \item If \( x > x^* \), \( f'(x) > 0 \)
        \begin{itemize}
            \item What does it mean? \( f(x) \) increases as \( x \) increases.
        \end{itemize}
    
        \item If \( x < x^* \), \( f'(x) < 0 \)
        \begin{itemize}
            \item What does it mean? \( f(x) \) decreases as \( x \) increases.
        \end{itemize}
        
    \end{enumerate}   
    \customFigure[0.5]{00_Images/GE.png}{Gradient of a 1D function in red on the x-axis.}
\end{intuition}

\begin{definition}
    \begin{enumerate}
        \item Initialize $x = x_0$.
        \item If $f'(x) \approx 0$, then stop (i.e. found the global minimum).
        \item If $f'(x) > 0$, then $x = x - \epsilon$ (i.e. moving closer to the minimum since it will be on the RS of the minimum so subtract a constant).
        \item If $f'(x) < 0$, then $x = x + \epsilon$ (i.e. moving closer to the minimum since it will be on the LS of the minimum so add a constant).
        \item Go to step 2.
    \end{enumerate}
    \vspace{1em}

    \begin{itemize}
        \item $\epsilon$: Step size.
        \item \textbf{Intuition:} We are going in the opposite direction of the gradient (i.e. slope) to determine the step direction. This will generalize the n dimensions.
    \end{itemize}
\end{definition}

\begin{intuition}
    What should be the value of \( \varepsilon \)?
        \begin{enumerate}
            \item What happens if we use a large \( \varepsilon \)?
            \begin{itemize}
                \item Bounce around, especially close to the optimal point since it will overshoot the global minimum.
            \end{itemize}
            
            \item What happens if we use a small \( \varepsilon \)?
            \begin{itemize}
                \item It can take forever to get to the global minimum.
            \end{itemize}
        \end{enumerate}  
        \vspace{1em}

    Therefore, use time-varying step size, \( \varepsilon_t \) by starting with large step size and decrease it over time.
    \begin{itemize}
        \item \( t \): iteration number.
        \item \textbf{Intuition:} We want to get close to the optimal point, and when around the optimal point, we want to be more cautious (small step sizes).
    \end{itemize}
\end{intuition}

\begin{example}
    \( \varepsilon_t = \frac{1}{t} \)
    \begin{itemize}
        \item If the function is strongly convex (e.g., \( f''(x) > 0 \ \forall x \in \mathbb{R} \)), then we are guaranteed to converge to the optimal solution.
    \end{itemize}
\end{example}

\subsection{Simple algorithm to find min f(x) for ND space}
\begin{intuition}
    Given the current location \( x \), what should be the next step?
    \[
    x' \leftarrow x + \varepsilon u
    \]
    \begin{itemize}
        \item \( \varepsilon \): step size
        \item \( u \): direction of update, \( \|u\| = 1 \).
    \end{itemize}
    \customFigure[0.5]{00_Images/DOG.png}{Direction of gradient for x1 and x2 for the 2D case.}
\end{intuition}

    \subsubsection{2D space}
    \begin{intuition}
        \begin{enumerate}
            \item Where should we go from \( x_0 \)? Choose the direction s.t. it gives you the smallest \( f(\underline{x}') \) (in a small step \( \varepsilon \)).
            \[
            \min f(\underline{x}') = f(\underline{x} + \varepsilon  \underline{u} )
            \]

            \customFigure[0.5]{00_Images/GD.png}{When we make a contour map of x1 and x2, we can see the gradient is in blue and the negative of the gradient is in red, so we are moving towards the minimum.}
        
            \item By Taylor series:
            \[
            f(\underline{x} + \varepsilon \underline{u}) = f(\underline{x}) + \varepsilon \underline{u}^T \nabla f(\underline{x}) + O(\varepsilon^2 \|\underline{u}\|^2)
            \]
            \begin{itemize}
                \item Neglect $O(\varepsilon^2 \|\underline{u}\|^2)$.
            \end{itemize}
        
            \item So, we want to minimize \( \underline{u}^T \nabla f(\underline{x}) \):
            \begin{itemize}
                \item \( \underline{u}^T \nabla f(\underline{x}) = \|\underline{u}\| \| \nabla f(\underline{x})\| \cos \theta \), which is minimized when \( \cos \theta = -1 \Longleftrightarrow \theta = 180^\circ \)
                \item i.e. Steepest descent occurs when $\underline{u}$ is moving in the opposite direction of $\nabla f(\underline{x})$ 
            \end{itemize}
            
            \item So, \( \underline{u}^* = -\frac{\nabla f(\underline{x})}{\|\nabla f(\underline{x})\|} \) (i.e. define it in the opposite direction of the gradient)
        \end{enumerate}
        \vspace{1em}

        \begin{itemize}
            \item \( \nabla f(x) \) points in the direction where \( f(x) \) has maximum ascent.
        
            \item Observe that:
            \[
            \varepsilon u^* = -\frac{\varepsilon}{\|\nabla f(x)\|} \nabla f(x)
            \]
            \( \varepsilon_t = \frac{\varepsilon}{\|\nabla f(x)\|} \).
        \end{itemize}

    \end{intuition}

\subsection{Gradient descent algorithm}
\begin{definition}
    \begin{enumerate}
        \item Initialize $\underline{x}_0$ (typically at random).
        \item For $t = 0, 1, 2, \dots$:
        \begin{enumerate}
            \item Compute $\underline{g}_t = \nabla f(\underline{x}_t)$. (i.e. gradient)
            \item Select direction $\underline{v}_t = -\underline{g}_t$. (i.e. negative direction of gradient)
            \item Update $\underline{x}_{t+1} = \underline{x}_t + \epsilon_t \underline{v}_t$. (i.e. update rule)
            \item Stop if stopping criteria are reached.
        \end{enumerate}
        \item End for
    \end{enumerate}
    \vspace{1em}

    \begin{itemize}
        \item Condition for stopping: \( \nabla f(\underline{x}_t) \approx 0 \) is reasonable. (i.e. when we almost hit the minimum)

        \item \( \varepsilon_t \): Learning rate

        \item If we have a constant learning rate for Gradient Descent alg. (i.e., \( \varepsilon_t = \alpha \ \forall t \)), then
        \[
        \varepsilon_t \underline{v}_t = -\alpha \underline{g}_t = -\alpha \nabla f(\underline{x}) = -\alpha \|\nabla f(\underline{x})\| \frac{\nabla f(\underline{x})}{\|\nabla f(\underline{x})\|}
        \]
        \begin{itemize}
            \item \textbf{Note:} Actual step size $\alpha \|\nabla f(\underline{x})\|$ diminishes as \( \underline{x}_t \rightarrow \underline{x}^* \) (i.e. because we are getting closer to the gradient being to 0, so this product will get smaller as well)
        \end{itemize}
    \end{itemize}
\end{definition}

\subsection{Gradient descent performance}
\begin{theorem}

    \textbf{Assumptions:} (Choose any $\gamma > 0$). If
    \begin{enumerate}
        \item $f$ is sufficiently smooth (if not, can't run gradient descent)
        \item $f$ is convex (if not, may get stuck in saddle point)
        \item $f$ has at least one global optimum (if not, may not terminate, e.g., $y=x$)
        \item $\epsilon$ is sufficiently small (if not, may diverge because it will bounce around the global minimum)
    \end{enumerate}

    \textbf{Conclusion:} If run long enough, gradient descent will return a value within $\gamma$ of a global optimum. (i.e. if we have these 4 conditions met, we can always find a global optimum or a close enough solution)

\end{theorem}

\subsection{Using Gradient Descent for Logistic Regression}

\subsection{Stochastic Gradient Descent (SGD)}

    \subsubsection{gt of SGD is an unbiased estimate of gradient of error}

    \subsubsection{Full GD complexity vs. SGD complexity}

    \subsubsection{Mini-batch GD is full-batch GD combined with SGD}

\subsection{PLA is an Extremer Version of Logistic Regression with SGD}

\subsection{Multiclass Logistic Regression}

\subsection{SGD update}

\subsection{Computing gradient of error}

\subsection{Softmax Logistic Regression for Binary Classification}

\subsection{Can we use GD/SGD for Linear Regression? Yes, you can}
