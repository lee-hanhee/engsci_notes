\section{Introduction}
    \subsection{What is a ML task?}
        \subsubsection{Different types of learning}
            \begin{definition}
                \begin{itemize}
                    \item \textbf{Supervised Learning:} Trains models using labeled data to learn a mapping from inputs to outputs.
                    \begin{itemize}
                        \item \textbf{Data:} Labeled
                        \item \textbf{Goal:} Predict correct label
                    \end{itemize}
                    \item \textbf{Reinforcement Learning:} Involves an agent interacting with an environment to maximize cumulative rewards based on its actions.
                    \begin{itemize}
                        \item \textbf{Data:} Reward signal
                        \item \textbf{Goal:} Maximize reward signal
                    \end{itemize}
                    \item \textbf{Unsupervised Learning:} Discovers patterns or structures in unlabeled data, often used for clustering or dimensionality reduction.
                    \begin{itemize}
                        \item \textbf{Data:} Unlabeled data
                        \item \textbf{Goal:} Varies (typically looking for interesting patterns in data)
                    \end{itemize}
                \end{itemize}            
            \end{definition}
        \subsubsection{Typical ML pipeline}
            \begin{process}
                \begin{enumerate}
                    \item Input representation: what each dimension of x contains
                    \item Model hypothesis class: y = g(wx + b)
                    \item Training algorithm to find w and b
                    \item Test model
                \end{enumerate}
            \end{process}

\section{Unsupervised learning}
    \subsection{What is unsupervised learning?}
        \subsubsection{Applications of unsupervised learning}
            \begin{definition}
                \begin{itemize}
                    \item \textbf{Clustering:} Involves grouping similar data points together based on their features. The goal is to partition the dataset into clusters where data points within the same cluster are more similar to each other than to those in other clusters.
                    \item \textbf{Dimensionality Reduction:} Reduce the number of features in a dataset while preserving its essential structure. This is useful for simplifying data, reducing noise, and improving computational efficiency. 
                    \item \textbf{Data Visualization:} Graphical representation of data to help understand and interpret the underlying patterns and relationships. 
                \end{itemize}
            \end{definition}

    \subsection{Clustering with the k-means algorithm}
        \subsubsection{Notation}
            \href{http://shabal.in/visuals/kmeans/6.html}{K-means Visualization}


    \subsection{Distortion in the k-means algorithm}

    \subsection{Principle component analysis}



    \begin{example}
        \begin{enumerate}
            \item Suppose logistic regression outputted a \(\underline{w}\) such that 
            \[
            \hat{P}_{\underline{w}}(+1 | \underline{x}_n) \approx 0.999 \Rightarrow \hat{P}_{\underline{w}}(-1 | \underline{x}_n) \approx 0.001
            \]

            The "machine" is very confident that \(\hat{y}_n = +1\) (i.e. in the correct prediction).
        
            \item If \( y_n = +1 \) (i.e. the true label was \(+1\)):
            \[
            e_n(\underline{w}) = -\log \hat{P}_{\underline{w}}(y_n | \underline{x}_n) = -\log(0.999) \approx 10^{-4}
            \]
            
            \item What if \( y_n = -1 \)? What do you expect to see for \( e_n(\underline{w}) \)?
            The "machine" is confident in the incorrect prediction

            \item Therefore,
            \[
            e_n(\underline{w}) = -\log \hat{P}_{\underline{w}}(y_n | \underline{x}_n) = -\log(0.001) \approx 3
            \]
        \end{enumerate}        
    \end{example}
