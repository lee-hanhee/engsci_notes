\subsection{Unsupervised Learning}
\begin{intuition}

    \textbf{Supervised:} Input \( X \), predict output \( Y \).
    \begin{itemize}
        \item Given labeled data: \( (X, Y) \).
        \item The goal is to learn a mapping function from \( X \) to \( Y \) that minimizes prediction error.
    \end{itemize}

    \textbf{Unsupervised:} Learn about \( X \).
    \begin{itemize}
        \item No labeled output \( Y \), instead learn which values of \( X \) are good.
        \begin{itemize}
            \item One version of good corresponds to likelihood (probability): 
            \begin{itemize}
                \item $p(X) \text{ is high for training cases, and low elsewhere.}$: In areas where training data is sparse, \( p(X) \) is low, indicating uncertainty or outliers.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{intuition}

\subsection{Clustering}
\begin{intuition}
    INSERT FIGURE 9.1 FROM TEXTBOOK
\end{intuition}

\subsection{K-means Clustering}
\subsubsection{K-means Error Function}

\subsubsection{Algorithm}

\subsubsection{Error Minimization Example}

\subsubsection{Application}

\subsubsection{What's wrong with k-means}

\subsection{Density Estimation}
\subsection{Mixture of Gaussians}

\subsubsection{Example: 2D Gaussian}

\subsubsection{Scalar Example}

\subsubsection{Example: Mixture of 3 Gaussians}

\subsubsection{Likelihood of Data}

\subsubsection{Relating k-means and MoG}

\subsection{Hard-Assignment Learning of MoG}
\subsection{EM: Soft Assignment}