\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to ML, Nearest neighbors, Linear classification (LFD: 1.1-1.2)}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Types of learning}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Supervised learning}{4}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear classification}}{4}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Linear regression}}{5}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linear transformation of data}}{5}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Unsupervised learning}{6}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces K-Means Clustering}}{6}{figure.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density Estimation-Gaussian Mixture Models}}{6}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Reinforcement learning}{7}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Classification problem setup}{7}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}k-Nearest neighbour}{7}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 1-nearest neighbour and linear classifier example.}}{8}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 1-nearest neighbour and linear classifier complex example.}}{8}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Nearest neighbour comparison.}}{9}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Decision boundary}{9}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Error process for classification}{9}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Types of error}{9}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Hyperparameters}{9}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Over/underfitting, Outliers}{10}{subsection.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Over and under fitting of k-nearest neighbours.}}{10}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Encoding (Ways to transform data)}{10}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.1}Transform numeric data}{10}{subsubsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.2}Transform categorical data}{10}{subsubsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Linear classification}{11}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.1}What is the hypothesis set for linear classification problem?}{11}{subsubsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2}How to describe hypothesis set through a function form?}{11}{subsubsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.3}Training}{11}{subsubsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.4}Prediction}{11}{subsubsection.1.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.5}How to find and draw decision boundaries?}{12}{subsubsection.1.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Linear classification example.}}{13}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Basic setup of learning problem of supervised learning}{13}{subsection.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Summary of the process.}}{13}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Basic setup of learning problem of binary linear classification}{13}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.1}Training}{14}{subsubsection.1.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.2}How hard is it to find the best decision boundary? (i.e. minimizing the error)}{14}{subsubsection.1.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of making a decision boundary.}}{14}{figure.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Linearly separable vs. Not linearly seperable}}{14}{figure.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear regression, Regularization (LFD: 3.2.1, 3.4.1, Appendix B)}{15}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Perceptron learning algorithm}{15}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}New Formulation of Binary Linear Classification}{15}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Algorithm}{15}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why does PLA work? (intutive explanation)}{15}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Table for classification}}{16}{figure.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Rosenblett Theorem (analytical proof why PLA works)}{16}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Remarks about PLA (uniqueness and error)}{16}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces PLA not unique.}}{17}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}What would happen if we use PLA for not linearly separable datasets?}{17}{subsubsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.7}Multiary classification}{17}{subsubsection.2.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pocket algorithm}{17}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Visual intuition}{17}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The best weight is the weight with the lowest error at this iteration.}}{17}{figure.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Algorithm}{17}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Linear regression}{18}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Matrix-vector algebraic representation}{18}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}When is the error 0? (Motivation for least squares)}{19}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Least squares}{19}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Gradient}{19}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The dimensionality is 0, where the derivative being negative indicates that the steepest increase is to the left. The derivative being positive indicates that the steepest increase is to the right.}}{20}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Basic gradients}{20}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Problem setup}{20}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Solution}{21}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Why is it called pseudo-inverse of X?}{21}{subsubsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.6}Prediction}{21}{subsubsection.2.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7}Geometric interpretation of least squares}{21}{subsubsection.2.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Column span example.}}{22}{figure.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularized linear regression / least squares}{22}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Example of a sine for regularization.}}{23}{figure.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Difference between LS and RLS}{23}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}How do we solve this problem?}{23}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Non-linear models}{23}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Circular decision boundary example}{23}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Circular boundary.}}{24}{figure.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}General form}{24}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces There are 3 points at $x=0,1,2$ (i.e. features), which are the inputs with the output $y=(6,0,0)$.}}{24}{figure.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Worksheet 1}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Q1}{25}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Gradient shortcuts}{25}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}lp-Norms}{26}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Matrix multiplication}{26}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Taylor series}{26}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q3}{27}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Matrix properties}{27}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Q3B}{28}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Q3C}{30}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Q3D}{30}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Visualization.}}{31}{figure.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Logistic Regression (LFD: 3.3)}{31}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Difference between linear classification and regression}{31}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Linear classification represented as a neuron.}}{32}{figure.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Linear regression represented as a neuron.}}{32}{figure.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Logistic regression}{32}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{32}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Given an input, the decision boundary classifies as either $+1$ or $-1$.}}{32}{figure.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Probability based on distance to the decision boundary.}}{33}{figure.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Deriving the appropriate hypothesis set}{33}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Two points on either side of the decision boundary and intuition on d.}}{33}{figure.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Derivation of d visually.}}{34}{figure.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Logistic function}}{34}{figure.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Illustrating logistic regression as a neuron}{34}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Logistic regression.}}{35}{figure.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Logistic function}{35}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Prediction (Reformulating the hypothesis set)}{35}{subsubsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6}What is the Error Criterion?}{35}{subsubsection.4.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Log-loss error visualization}}{35}{figure.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Why is log-loss a reasonable choice?}{36}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Numerical examples interpretation (i.e. distinguish between better hyperplanes)}{36}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces From this we can see that L2 is more preferred because it gives more room between the boundary and the points.}}{36}{figure.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Maximum likelihood interpretation}{37}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Cross-entropy interpretation}{37}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Gradient Descent (LFD: 3.3, DL: 8.3.1-8.3.3 (recommended))}{38}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Problem setup for gradient descent}{38}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Optimization primer}{38}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Gradient:}{38}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Gradient example.}}{39}{figure.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Gradient and optimality:}{39}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Gradient and optimality.}}{39}{figure.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Convex functions:}{39}{subsubsection.5.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Convex function, but not strictly convex.}}{40}{figure.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Convex vs. Non-convex functions}}{40}{figure.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Chain rule}{40}{subsubsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Simple algorithm to find min f(x) for 1D}{41}{subsection.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Gradient of a 1D function in red on the x-axis.}}{42}{figure.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Simple algorithm to find min f(x) for ND space}{42}{subsection.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Direction of gradient for x1 and x2 for the 2D case.}}{43}{figure.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}2D space}{43}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces When we make a contour map of x1 and x2, we can see the gradient is in blue and the negative of the gradient is in red, so we are moving towards the minimum.}}{43}{figure.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Gradient descent algorithm}{44}{subsection.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Gradient descent performance}{44}{subsection.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Using Gradient Descent for Logistic Regression}{44}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Stochastic Gradient Descent (SGD)}{45}{subsection.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.8.1}gt of SGD is an unbiased estimate of gradient of error}{45}{subsubsection.5.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.8.2}Full GD complexity vs. SGD complexity}{46}{subsubsection.5.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.8.3}Mini-batch GD is full-batch GD combined with SGD}{46}{subsubsection.5.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}PLA is an Extremer Version of Logistic Regression with SGD}{46}{subsection.5.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.10}Multiclass Logistic Regression}{47}{subsection.5.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Example of how the 3rd weight line is the farthest away for so it has the largest probability}}{47}{figure.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.11}SGD update}{47}{subsection.5.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.12}Computing gradient of error}{48}{subsection.5.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.13}Softmax Logistic Regression for Binary Classification}{49}{subsection.5.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.14}Can we use GD/SGD for Linear Regression? Yes, you can}{49}{subsection.5.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Error vs. iteration.}}{49}{figure.41}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Multilayer Perceptron, Neural Networks (LFD: Sec 7.1, 7.2.1, 7.2.2)}{50}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Backpropagation, Neural Networks Implementations (LFD: 7.2, DL: 7.12, 8.4, 8.5 (recommended))}{50}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Unsupervised Learning, Clustering, Density Estimation (LFD: 6.3.3, 6.4)}{50}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}EM (LFD: 6.4)}{50}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Markov Decision Process (AIMA: 16)}{50}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Reinforcement Learning (AIMA: 23.1-23.4)}{50}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Deep Learning Architectures CNN; Language Models; RNN; Sequence modeling and neural machine translation; Attention; LSTM (PRML: Sec 5.5.6, DL: Sec 10.1, 10.2, 10.4, 10.10, SLP: Ch. 9)}{50}{section.12}\protected@file@percent }
\gdef \@abspage@last{50}
