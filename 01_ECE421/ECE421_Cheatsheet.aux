\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to ML, Nearest neighbors, Linear classification (LFD: 1.1-1.2)}{5}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Types of learning}{5}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Supervised learning}{5}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear classification}}{6}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Linear regression}}{6}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linear transformation of data}}{7}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Unsupervised learning}{7}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces K-Means Clustering}}{8}{figure.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density Estimation-Gaussian Mixture Models}}{8}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Reinforcement learning}{8}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Classification problem setup}{9}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}k-Nearest neighbour}{9}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 1-nearest neighbour and linear classifier example.}}{9}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 1-nearest neighbour and linear classifier complex example.}}{10}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Nearest neighbour comparison.}}{10}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Decision boundary}{10}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Error process for classification}{10}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Types of error}{11}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Hyperparameters}{11}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Over/underfitting, Outliers}{11}{subsection.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Over and under fitting of k-nearest neighbours.}}{11}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Encoding (Ways to transform data)}{12}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.1}Transform numeric data}{12}{subsubsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.2}Transform categorical data}{12}{subsubsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Linear classification}{12}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.1}What is the hypothesis set for linear classification problem?}{12}{subsubsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2}How to describe hypothesis set through a function form?}{12}{subsubsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.3}Training}{13}{subsubsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.4}Prediction}{13}{subsubsection.1.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.5}How to find and draw decision boundaries?}{13}{subsubsection.1.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Linear classification example.}}{14}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Basic setup of learning problem of supervised learning}{14}{subsection.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Summary of the process.}}{15}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Basic setup of learning problem of binary linear classification}{15}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.1}Training}{15}{subsubsection.1.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.2}How hard is it to find the best decision boundary? (i.e. minimizing the error)}{16}{subsubsection.1.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of making a decision boundary.}}{16}{figure.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Linearly separable vs. Not linearly seperable}}{16}{figure.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear regression, Regularization (LFD: 3.2.1, 3.4.1, Appendix B)}{17}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Perceptron learning algorithm}{17}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}New Formulation of Binary Linear Classification}{17}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Algorithm}{17}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why does PLA work? (intutive explanation)}{17}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Table for classification}}{17}{figure.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Rosenblett Theorem (analytical proof why PLA works)}{18}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Remarks about PLA (uniqueness and error)}{18}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces PLA not unique.}}{18}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}What would happen if we use PLA for not linearly separable datasets?}{19}{subsubsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.7}Multiary classification}{19}{subsubsection.2.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pocket algorithm}{19}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Visual intuition}{19}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The best weight is the weight with the lowest error at this iteration.}}{19}{figure.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Algorithm}{19}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Linear regression}{19}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Matrix-vector algebraic representation}{20}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}When is the error 0? (Motivation for least squares)}{20}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Least squares}{21}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Gradient}{21}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The dimensionality is 0, where the derivative being negative indicates that the steepest increase is to the left. The derivative being positive indicates that the steepest increase is to the right.}}{21}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Basic gradients}{21}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Problem setup}{22}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Solution}{22}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Why is it called pseudo-inverse of X?}{23}{subsubsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.6}Prediction}{23}{subsubsection.2.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7}Geometric interpretation of least squares}{23}{subsubsection.2.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Column span example.}}{24}{figure.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularized linear regression / least squares}{24}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Example of a sine for regularization.}}{25}{figure.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Difference between LS and RLS}{25}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}How do we solve this problem?}{25}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Non-linear models}{25}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Circular decision boundary example}{25}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Circular boundary.}}{26}{figure.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}General form}{26}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces There are 3 points at $x=0,1,2$ (i.e. features), which are the inputs with the output $y=(6,0,0)$.}}{26}{figure.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Worksheet 1}{28}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Q1}{28}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Gradient shortcuts}{28}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}lp-Norms}{28}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Matrix multiplication}{28}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Taylor series}{29}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q3}{30}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Matrix properties}{30}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Q3B}{30}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Q3C}{32}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Q3D}{32}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Visualization.}}{33}{figure.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Logistic Regression (LFD: 3.3)}{34}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Difference between linear classification and regression}{34}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Linear classification represented as a neuron.}}{34}{figure.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Linear regression represented as a neuron.}}{34}{figure.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Logistic regression}{35}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{35}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Given an input, the decision boundary classifies as either $+1$ or $-1$.}}{35}{figure.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Probability based on distance to the decision boundary.}}{35}{figure.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Deriving the appropriate hypothesis set}{35}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Two points on either side of the decision boundary and intuition on d.}}{35}{figure.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Derivation of d visually.}}{36}{figure.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Logistic function}}{36}{figure.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Illustrating logistic regression as a neuron}{36}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Logistic regression.}}{37}{figure.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Logistic function}{37}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Prediction (Reformulating the hypothesis set)}{37}{subsubsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6}What is the Error Criterion?}{37}{subsubsection.4.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Log-loss error visualization}}{38}{figure.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Why is log-loss a reasonable choice?}{38}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Numerical examples interpretation (i.e. distinguish between better hyperplanes)}{38}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces From this we can see that L2 is more preferred because it gives more room between the boundary and the points.}}{38}{figure.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Maximum likelihood interpretation}{39}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Cross-entropy interpretation}{40}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Worksheet 2}{41}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Gradient Descent (LFD: 3.3, DL: 8.3.1-8.3.3 (recommended))}{42}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Problem setup for gradient descent}{42}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Optimization primer}{42}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Gradient:}{42}{subsubsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Gradient example.}}{42}{figure.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Gradient and optimality:}{43}{subsubsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Gradient and optimality.}}{43}{figure.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Convex functions:}{43}{subsubsection.6.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Convex function, but not strictly convex.}}{43}{figure.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Convex vs. Non-convex functions}}{44}{figure.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Chain rule}{44}{subsubsection.6.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Simple algorithm to find min f(x) for 1D}{45}{subsection.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Gradient of a 1D function in red on the x-axis.}}{45}{figure.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Simple algorithm to find min f(x) for ND space}{46}{subsection.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Direction of gradient for x1 and x2 for the 2D case.}}{46}{figure.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}2D space}{46}{subsubsection.6.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces When we make a contour map of x1 and x2, we can see the gradient is in blue and the negative of the gradient is in red, so we are moving towards the minimum.}}{47}{figure.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Gradient descent algorithm}{47}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Gradient descent performance}{48}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Using Gradient Descent for Logistic Regression}{48}{subsection.6.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Stochastic Gradient Descent (SGD)}{49}{subsection.6.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.1}gt of SGD is an unbiased estimate of gradient of error}{49}{subsubsection.6.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.2}Full GD complexity vs. SGD complexity}{49}{subsubsection.6.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.8.3}Mini-batch GD is full-batch GD combined with SGD}{50}{subsubsection.6.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.9}PLA is an Extremer Version of Logistic Regression with SGD}{50}{subsection.6.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.10}Multiclass Logistic Regression}{51}{subsection.6.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Example of how the 3rd weight line is the farthest away for so it has the largest probability}}{51}{figure.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.11}SGD update}{51}{subsection.6.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.12}Computing gradient of error}{52}{subsection.6.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.13}Softmax Logistic Regression for Binary Classification}{52}{subsection.6.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.14}Can we use GD/SGD for Linear Regression? Yes, you can}{53}{subsection.6.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Error vs. iteration.}}{53}{figure.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.15}Non-convex functions}{53}{subsection.6.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Non-convex functions}}{54}{figure.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.15.1}Saddle points}{54}{subsubsection.6.15.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces Saddle point}}{54}{figure.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.16}SGD with Momentum (heavy ball momentum)}{54}{subsection.6.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {44}{\ignorespaces Ball rolling down}}{55}{figure.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {45}{\ignorespaces SGD with momentum where the red and blue lines are used to calculate the green lines which is the actual direction towards the minimum.}}{55}{figure.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.16.1}How does momentum help?}{56}{subsubsection.6.16.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Example of momentum escaping points}}{56}{figure.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces Example of SGD vs. SGD + Momentum}}{56}{figure.47}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces Example of the ellipsoid}}{56}{figure.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces Showing how a slow learning rate, causes it to move slow in the x1 direction, but a large learning rate causes it to diverge in the x2 direction.}}{57}{figure.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.16.2}Nestrov Momentum}{57}{subsubsection.6.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Worksheet 3}{58}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Steps to Calculate the Characteristic Polynomial of a Matrix}{58}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Steps to Calculate the Eigenvectors of a Matrix}{58}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Gaussian Elimination with Matrix Visualization}{58}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Eigenvalue Decomposition of a Matrix}{59}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Singular Value Decomposition (SVD)}{60}{subsection.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Unsupervised Learning, Clustering, K-means clustering, Density Estimation, Mixture of Gaussians, Hard-assignment learning of MoG, EM: Soft assignment (PRML: 9.1-2)}{62}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Summary}{62}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Supervised vs. Unsupervised Learning}{62}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}K-means Clustering}{62}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}K-means Error Function}{62}{subsubsection.8.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.2}Algorithm}{62}{subsubsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.3}Illustration}{63}{subsubsection.8.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces K-means Algorithm Illustration}}{64}{figure.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.4}Error Minimization Example}{65}{subsubsection.8.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {51}{\ignorespaces Error minimization}}{65}{figure.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.5}What's wrong with k-means}{65}{subsubsection.8.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {52}{\ignorespaces What's wrong with kmeans.}}{66}{figure.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Mixture of Gaussians}{66}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.1}Normal Distribution}{66}{subsubsection.8.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.2}Visualization}{66}{subsubsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {53}{\ignorespaces Visualization of the mean with the variances.}}{67}{figure.53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {54}{\ignorespaces Mixing proportions.}}{67}{figure.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.3}Example: 2D Gaussian}{67}{subsubsection.8.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.4}Scalar Example}{68}{subsubsection.8.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {55}{\ignorespaces Scalar example}}{69}{figure.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.5}Example: Mixture of 3 Gaussians}{69}{subsubsection.8.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {56}{\ignorespaces Mixture of 3 Gaussians}}{70}{figure.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.6}Likelihood of Data}{70}{subsubsection.8.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.7}Relating k-means and MoG}{71}{subsubsection.8.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.8}Learning a MoG}{71}{subsubsection.8.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.9}Hard-Assignment Learning of MoG}{72}{subsubsection.8.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.10}EM: Soft Assignment}{72}{subsubsection.8.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.11}Transform}{73}{subsubsection.8.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.12}Example of EM}{73}{subsubsection.8.4.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {57}{\ignorespaces Example of EM.}}{73}{figure.57}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Backpropagation, Neural Networks Implementations (LFD: 7.2, DL: 7.12, 8.4, 8.5 (recommended))}{75}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Summary}{75}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Motivation for neural networks}{75}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Limitation of perceptron}{75}{subsubsection.9.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {58}{\ignorespaces Linearly inseparable.}}{75}{figure.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Decomposing a complex problem}{75}{subsubsection.9.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {59}{\ignorespaces Decomposing the complex problem to be constructed from $f$.}}{76}{figure.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.3}Representing as a multilayer perceptron}{76}{subsubsection.9.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {60}{\ignorespaces Multilayer Perceptron}}{76}{figure.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Differentiable activation functions}{77}{subsection.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {61}{\ignorespaces Activation functions.}}{77}{figure.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Neural network}{77}{subsection.9.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {62}{\ignorespaces Neural network with inputs, hidden units, and outputs that introduce the notation.}}{77}{figure.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}Application: Arbitrary 1D Function Approximation}{78}{subsubsection.9.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {63}{\ignorespaces Approximating functions.}}{78}{figure.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}DL in Genomics (Real-life examples)}{78}{subsection.9.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {64}{\ignorespaces Protein binding}}{79}{figure.64}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {65}{\ignorespaces 1st layer weights}}{79}{figure.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Mostly complete chart of NN}{80}{subsection.9.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {66}{\ignorespaces Mostly complete chart of NN}}{80}{figure.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Forward propogation}{81}{subsection.9.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Unsupervised Learning, Clustering, Density Estimation (LFD: 6.3.3, 6.4)}{82}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}EM (LFD: 6.4)}{83}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Markov Decision Process (AIMA: 16)}{84}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Reinforcement Learning (AIMA: 23.1-23.4)}{85}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Deep Learning Architectures CNN; Language Models; RNN; Sequence modeling and neural machine translation; Attention; LSTM (PRML: Sec 5.5.6, DL: Sec 10.1, 10.2, 10.4, 10.10, SLP: Ch. 9)}{86}{section.14}\protected@file@percent }
\gdef \@abspage@last{86}
