\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to ML, Nearest neighbors, Linear classification (LFD: 1.1-1.2)}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Types of learning}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Supervised learning}{4}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear classification}}{4}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Linear regression}}{5}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linear transformation of data}}{5}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Unsupervised learning}{5}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces K-Means Clustering}}{6}{figure.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density Estimation-Gaussian Mixture Models}}{6}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Reinforcement learning}{6}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Classification problem setup}{7}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}k-Nearest neighbour}{7}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 1-nearest neighbour and linear classifier example.}}{8}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 1-nearest neighbour and linear classifier complex example.}}{8}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Nearest neighbour comparison.}}{8}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Decision boundary}{9}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Error process for classification}{9}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Types of error}{9}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Hyperparameters}{9}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Over/underfitting, Outliers}{9}{subsection.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Over and under fitting of k-nearest neighbours.}}{10}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Encoding (Ways to transform data)}{10}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.1}Transform numeric data}{10}{subsubsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.2}Transform categorical data}{10}{subsubsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Linear classification}{10}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.1}What is the hypothesis set for linear classification problem?}{11}{subsubsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2}How to describe hypothesis set through a function form?}{11}{subsubsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.3}Training}{11}{subsubsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.4}Prediction}{11}{subsubsection.1.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.5}How to find and draw decision boundaries?}{11}{subsubsection.1.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Linear classification example.}}{12}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Basic setup of learning problem of supervised learning}{13}{subsection.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Summary of the process.}}{13}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Basic setup of learning problem of binary linear classification}{13}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.1}Training}{14}{subsubsection.1.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.2}How hard is it to find the best decision boundary? (i.e. minimizing the error)}{14}{subsubsection.1.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of making a decision boundary.}}{14}{figure.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Linearly separable vs. Not linearly seperable}}{14}{figure.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear regression, Regularization (LFD: 3.2.1, 3.4.1, Appendix B)}{14}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Perceptron learning algorithm}{14}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}New Formulation of Binary Linear Classification}{14}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Algorithm}{15}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why does PLA work? (intutive explanation)}{15}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Table for classification}}{15}{figure.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Rosenblett Theorem (analytical proof why PLA works)}{16}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Remarks about PLA (uniqueness and error)}{16}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces PLA not unique.}}{16}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}What would happen if we use PLA for not linearly separable datasets?}{16}{subsubsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.7}Multiary classification}{16}{subsubsection.2.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pocket algorithm}{16}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Visual intuition}{17}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The best weight is the weight with the lowest error at this iteration.}}{17}{figure.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Algorithm}{17}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Linear regression}{17}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Matrix-vector algebraic representation}{18}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}When is the error 0? (Motivation for least squares)}{18}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Least squares}{18}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Gradient}{19}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The dimensionality is 0, where the derivative being negative indicates that the steepest increase is to the left. The derivative being positive indicates that the steepest increase is to the right.}}{19}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Basic gradients}{19}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Problem setup}{20}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Solution}{20}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Why is it called pseudo-inverse of X?}{20}{subsubsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.6}Prediction}{21}{subsubsection.2.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7}Geometric interpretation of least squares}{21}{subsubsection.2.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Column span example.}}{21}{figure.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularized linear regression / least squares}{22}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Example of a sine for regularization.}}{22}{figure.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Difference between LS and RLS}{22}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}How do we solve this problem?}{23}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Non-linear models}{23}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Circular decision boundary example}{23}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Circular boundary.}}{23}{figure.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}General form}{24}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces There are 3 points at $x=0,1,2$ (i.e. features), which are the inputs with the output $y=(6,0,0)$.}}{24}{figure.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Worksheet 1}{25}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Q1}{25}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Gradient shortcuts}{25}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}lp-Norms}{25}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Matrix multiplication}{26}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Taylor series}{26}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q3}{27}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Matrix properties}{27}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Q3B}{27}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Q3C}{29}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Q3D}{29}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Visualization.}}{30}{figure.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Logistic Regression (LFD: 3.3)}{30}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Difference between linear classification and regression}{30}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Linear classification represented as a neuron.}}{31}{figure.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Linear regression represented as a neuron.}}{31}{figure.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Logistic regression}{31}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Motivation}{31}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Given an input, the decision boundary classifies as either $+1$ or $-1$.}}{31}{figure.25}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Probability based on distance to the decision boundary.}}{32}{figure.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Deriving the appropriate hypothesis set}{32}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Two points on either side of the decision boundary and intuition on d.}}{32}{figure.27}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Derivation of d visually.}}{33}{figure.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Logistic function}}{33}{figure.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Illustrating logistic regression as a neuron}{33}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Logistic regression.}}{34}{figure.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Logistic function}{34}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Prediction (Reformulating the hypothesis set)}{34}{subsubsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.6}What is the Error Criterion?}{34}{subsubsection.4.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Log-loss error visualization}}{34}{figure.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Why is log-loss a reasonable choice?}{35}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Numerical examples interpretation (i.e. distinguish between better hyperplanes)}{35}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces From this we can see that L2 is more preferred because it gives more room between the boundary and the points.}}{35}{figure.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Maximum likelihood interpretation}{36}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Cross-entropy interpretation}{36}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Gradient Descent (LFD: 3.3, DL: 8.3.1-8.3.3 (recommended))}{37}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Problem setup for gradient descent}{37}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Optimization primer}{37}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Gradient:}{37}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Gradient example.}}{38}{figure.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Gradient and optimality:}{38}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Gradient and optimality.}}{38}{figure.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Convex functions:}{38}{subsubsection.5.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Convex function, but not strictly convex.}}{39}{figure.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Convex vs. Non-convex functions}}{39}{figure.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Simple algoirthm to find min f(x) for 1D}{39}{subsection.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Gradient of a 1D function in red on the x-axis.}}{40}{figure.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}N-dimensional space}{40}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}2D space}{41}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Gradient descent algorithm}{41}{subsubsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Gradient descent performance}{41}{subsubsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Multilayer Perceptron, Neural Networks (LFD: Sec 7.1, 7.2.1, 7.2.2)}{42}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Backpropagation, Neural Networks Implementations (LFD: 7.2, DL: 7.12, 8.4, 8.5 (recommended))}{42}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Unsupervised Learning, Clustering, Density Estimation (LFD: 6.3.3, 6.4)}{42}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}EM (LFD: 6.4)}{42}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Markov Decision Process (AIMA: 16)}{42}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Reinforcement Learning (AIMA: 23.1-23.4)}{42}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Deep Learning Architectures CNN; Language Models; RNN; Sequence modeling and neural machine translation; Attention; LSTM (PRML: Sec 5.5.6, DL: Sec 10.1, 10.2, 10.4, 10.10, SLP: Ch. 9)}{42}{section.12}\protected@file@percent }
\gdef \@abspage@last{42}
