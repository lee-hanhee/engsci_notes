\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to ML, Nearest neighbors, Linear classification (LFD: 1.1-1.2)}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Types of learning}{3}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Supervised learning}{3}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear classification}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Linear regression}}{4}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-linear transformation of data}}{4}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Unsupervised learning}{4}{subsubsection.1.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces K-Means Clustering}}{5}{figure.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density Estimation-Gaussian Mixture Models}}{5}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Reinforcement learning}{5}{subsubsection.1.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Classification problem setup}{6}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}k-Nearest neighbour}{6}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 1-nearest neighbour and linear classifier example.}}{7}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 1-nearest neighbour and linear classifier complex example.}}{7}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Nearest neighbour comparison.}}{7}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Decision boundary}{8}{subsubsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Error process for classification}{8}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Types of error}{8}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Hyperparameters}{8}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Over/underfitting, Outliers}{8}{subsection.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Over and under fitting of k-nearest neighbours.}}{9}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Encoding (Ways to transform data)}{9}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.1}Transform numeric data}{9}{subsubsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.7.2}Transform categorical data}{9}{subsubsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Linear classification}{9}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.1}What is the hypothesis set for linear classification problem?}{10}{subsubsection.1.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.2}How to describe hypothesis set through a function form?}{10}{subsubsection.1.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.3}Training}{10}{subsubsection.1.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.4}Prediction}{10}{subsubsection.1.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.8.5}How to find and draw decision boundaries?}{10}{subsubsection.1.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Linear classification example.}}{11}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Basic setup of learning problem of supervised learning}{12}{subsection.1.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Summary of the process.}}{12}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Basic setup of learning problem of binary linear classification}{12}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.1}Training}{13}{subsubsection.1.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.10.2}How hard is it to find the best decision boundary? (i.e. minimizing the error)}{13}{subsubsection.1.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of making a decision boundary.}}{13}{figure.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Linearly separable vs. Not linearly seperable}}{13}{figure.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Linear regression, Regularization (LFD: 3.2.1, 3.4.1, Appendix B)}{13}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Perceptron learning algorithm}{13}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}New Formulation of Binary Linear Classification}{13}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Algorithm}{14}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Why does PLA work? (intutive explanation)}{14}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Table for classification}}{14}{figure.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Rosenblett Theorem (analytical proof why PLA works)}{15}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Remarks about PLA (uniqueness and error)}{15}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces PLA not unique.}}{15}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}What would happen if we use PLA for not linearly separable datasets?}{15}{subsubsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.7}Multiary classification}{15}{subsubsection.2.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pocket algorithm}{15}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Visual intuition}{16}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The best weight is the weight with the lowest error at this iteration.}}{16}{figure.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Algorithm}{16}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Linear regression}{16}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Matrix-vector algebraic representation}{17}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}When is the error 0? (Motivation for least squares)}{17}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Least squares}{17}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Gradient}{18}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The dimensionality is 0, where the derivative being negative indicates that the steepest increase is to the left. The derivative being positive indicates that the steepest increase is to the right.}}{18}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Basic gradients}{18}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Problem setup}{19}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Solution}{19}{subsubsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Why is it called pseudo-inverse of X?}{19}{subsubsection.2.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.6}Prediction}{20}{subsubsection.2.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.7}Geometric interpretation of least squares}{20}{subsubsection.2.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Column span example.}}{20}{figure.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Regularized linear regression / least squares}{21}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Example of a sine for regularization.}}{21}{figure.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Difference between LS and RLS}{21}{subsubsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}How do we solve this problem?}{22}{subsubsection.2.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Non-linear models}{22}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Circular decision boundary example}{22}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Circular boundary.}}{22}{figure.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}General form}{23}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces There are 3 points at $x=0,1,2$ (i.e. features), which are the inputs with the output $y=(6,0,0)$.}}{23}{figure.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Logistic Regression (LFD: 3.3)}{24}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Descent (LFD: 3.3, DL: 8.3.1-8.3.3 (recommended))}{24}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Multilayer Perceptron, Neural Networks (LFD: Sec 7.1, 7.2.1, 7.2.2)}{24}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Backpropagation, Neural Networks Implementations (LFD: 7.2, DL: 7.12, 8.4, 8.5 (recommended))}{24}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Unsupervised Learning, Clustering, Density Estimation (LFD: 6.3.3, 6.4)}{24}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}EM (LFD: 6.4)}{24}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Markov Decision Process (AIMA: 16)}{24}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Reinforcement Learning (AIMA: 23.1-23.4)}{24}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Deep Learning Architectures CNN; Language Models; RNN; Sequence modeling and neural machine translation; Attention; LSTM (PRML: Sec 5.5.6, DL: Sec 10.1, 10.2, 10.4, 10.10, SLP: Ch. 9)}{24}{section.11}\protected@file@percent }
\gdef \@abspage@last{24}
