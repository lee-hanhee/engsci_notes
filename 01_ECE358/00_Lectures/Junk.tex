%% PRELIMINARIES
\subsection{Algorithm}
\begin{definition}
    An algorithm is any well-defined computational procedure tht takes some value, or set of values, as input and produces some value, or set of values, as output in a finite amount of time. 
\end{definition}

\subsection{Pseudocode}
\begin{definition}
    A description of the steps in an algorithm using a mix of conventions of programming languages (like assignment operator, conditional operator, loop) with informal, usually self-explanatory, notation of actions and conditions.
\end{definition}

%% ASYMPTOTITCS
\begin{example}
    What asymptotic notation should you use for insertion sort's worst-case, best-case, and general running time? 
    \vspace{1em}

    \begin{itemize}
        \item \textbf{Worst-case:} $O(n^2)\text{, } \Omega(n^2)\text{, and } \Theta(n^2)$ can be used, but $\Theta(n^2)$ is the most precise. 
        \item \textbf{Best-case:} $O(n)\text{, } \Omega(n)\text{, and } \Theta(n)$ can be used, but $\Theta(n)$ is the most precise. 
        \item \textbf{Average-case:} $O(n^2)$ because in all cases, its running time grows no faster than $n^2$. Or $\Omega(n)$ because in all cases, its running time is at least as fast as $n$.
    \end{itemize}
\end{example}

\subsection{Deterministic vs. Randomized}
\begin{definition}
    \begin{itemize}
        \item \textbf{Deterministic algorithms}: These algorithms produce the same output for a given input every time they are run, as their execution follows a predictable sequence of steps.
        \item \textbf{Randomized algorithms}: These algorithms incorporate random choices in their logic, which may lead to different outputs or different execution paths for the same input across multiple runs.
    \end{itemize}
\end{definition}

\subsection{Types of analysis}
\begin{definition}
    \begin{itemize}
        \item \textbf{Best-case}: The algorithm runs in the shortest time due to optimal input.
        \item \textbf{Worst-case}: The algorithm takes the longest time due to the most challenging input.
        \item \textbf{Average-case}: The running time averaged over all inputs, assuming equal likelihood.
        \item \textbf{Expected-case}: The running time based on a probability distribution of inputs.
        \item \textbf{Amortized}: The average time per operation over a sequence of operations.
    \end{itemize}
\end{definition}

\subsubsection{Proofs}

    \begin{derivation} Transitivity
        \begin{enumerate}
            \item \textbf{Show $f(n) = \Theta(g(n))$}
            
            From the first condition, we know that there exist constants $C_1$, $C_2$, and $n_1$ such that for all $n \geq n_1$:
            \[
            0 \leq C_1 \cdot g(n) \leq f(n) \leq C_2 \cdot g(n)
            \]
            
            \item \textbf{Show $g(n) = \Theta(h(n))$}
            
            Next, from the second condition, we know there exist constants $C_3$, $C_4$, and $n_2$ such that for all $n \geq n_2$:
            \[
            0 \leq C_3 \cdot h(n) \leq g(n) \leq C_4 \cdot h(n)
            \]
            
            \item \textbf{Combine the Results}
            
            Since $g(n) = \Theta(h(n))$, we can combine this result with the earlier bounds on $f(n)$. We now have:
            \[
            0 \leq C_1 \cdot C_3 \cdot h(n) \leq C_1 \cdot g(n) \leq f(n) \leq C_2 \cdot g(n) \leq C_2 \cdot C_4 \cdot h(n)
            \]
            Thus, $f(n)$ is bounded above and below by $h(n)$ for sufficiently large $n$. This is obtained by subbing in $C_3 h(n) \leq g(n)$ into the lower bound for $f(n)$ (i.e. $C_1 g(n)$). The same is done for the upper bound. 
            
            \item \textbf{Define New Constants}
            
            Let us define new constants $C'_1 = C_1 \cdot C_3$ and $C'_2 = C_2 \cdot C_4$. We now have:
            \[
            0 \leq C'_1 \cdot h(n) \leq f(n) \leq C'_2 \cdot h(n)
            \]
            This holds for all $n \geq \max(n_1, n_2)$, or for simplicity, we can state $n \geq n_1 + n_2$.
            
            \item \textbf{Conclusion:} Therefore, we have shown that:
                \[
                f(n) = \Theta(h(n))
                \]
        \end{enumerate}
            
    \end{derivation}

    \begin{derivation}
        \( f(n) = O(g(n)) \iff g(n) = \Omega(f(n)) \)
        \begin{enumerate}
            
            \item \textbf{Proof:} We start with the assumption that \( f(n) = O(g(n)) \), which implies the following inequality for some constants \( C > 0 \) and \( n_0 \geq 0 \):
            \[
            0 \leq f(n) \leq C \cdot g(n) \quad \forall n \geq n_0
            \]
            
            \item \textbf{Apply the inequality.}
            Rewriting the inequality, we get the following bound for \( f(n) \):
            \[
            0 \leq \frac{1}{C} \cdot f(n) \leq g(n) \quad \forall n \geq n_0
            \]
            
            \item \textbf{Conclude \( g(n) = \Omega(f(n)) \).}
        \end{enumerate}        
    \end{derivation}

    \begin{derivation} \( f(n) = \Theta(g(n)) \iff g(n) = \Theta(f(n)) \)
        \begin{enumerate}
            
            \item \textbf{Express \( f(n) = \Theta(g(n)) \) in terms of \( O \) and \( \Omega \).}
            \[
            f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \quad \text{and} \quad f(n) = \Omega(g(n))
            \]
        
            \item \textbf{Apply the equivalence of Big-O and Big-Omega.}
            From the equivalence proven earlier, we know that:
            \[
            f(n) = O(g(n)) \iff g(n) = \Omega(f(n)) \quad \text{and} \quad f(n) = \Omega(g(n)) \iff g(n) = O(f(n))
            \]
            
            \item \textbf{Step 3: Transpose the statements.}
            By applying the equivalences, we can substitute them into the definition of \( f(n) = \Theta(g(n)) \):
            \[
            f(n) = \Theta(g(n)) \iff g(n) = \Omega(f(n)) \quad \text{and} \quad g(n) = O(f(n))
            \]
            
            \item \textbf{Step 4: Conclusion.}
            Therefore, since we have established both \( g(n) = O(f(n)) \) and \( g(n) = \Omega(f(n)) \), we can conclude that:
            \[
            g(n) = \Theta(f(n))
            \]
        \end{enumerate}
    \end{derivation}

%% RECCURRENCES
A recurrence $T(n)$ is \textbf{algorithmic} if, for every sufficiently large \textbf{threshold} constant $n_0 >0$, the following two properties hold:
\begin{enumerate}
    \item $\forall$ $n<n_0$, $T(n) = \Theta(1)$ (i.e. $\exists \; c_1, \; c_2 \in \mathbb{R}$ s.t. $0<c_1\leq T(n) \leq c_2 \text{ for } n<n_0$)
    \item $\forall$ $n \geq n_0$, every path of recursion terminates in a defined base case within a finite number of recursive invocations (prevents infinite recursive loop or failure to compute a solution).
\end{enumerate}

\begin{intuition}
    Whenever a recurrence is stated without an explicit base case, we assume that the recurrence is algorithmic.
    \begin{itemize}
        \item \textbf{Implication:} This means we can pick any sufficiently large threshold constant $n_0$.
    \end{itemize}
\end{intuition}

\item \textbf{Bounds:} Rather than trying to prove $\Theta$-bound directly, first prove an $O$-bound, and then prove an $\Omega$-bound, then use Theorem 3.1.
            \item \textbf{Making a good guess:}
            \begin{itemize}
                \item See if the recurrence is similar to one you've seen before, then guessing a similar solution.
                \item Determine loose upper and lower bounds on the recurrence and then reduce your range of uncertainty.
            \end{itemize}
            \item \textbf{Trick:} Subtract a lower-order term when the math fails to work out in the induction proof.
            
%% GRAPHS and TREES
\item \textbf{Degree of a vertex:} Number of neighbors that a vertex has.
\item 
\item \textbf{Subgraph:} $G' = (V', E')$ is a subgraph of a graph $G$ iff:
\begin{itemize}
    \item $V' \subseteq V$
    \item $E' \subseteq E$
    \item If $e = (v, u) \in E'$, then $v \in V'$ and $u \in V'$
\end{itemize}
\item \textbf{Adjacent vertices:} $N(v) = \{u : (v, u) \in E\}$
\item \textbf{Incident edges:} $I(v) = \{(u, v) : (u, v) \in E\}$

\subsubsection{Types of walks, trails, and paths}
\begin{terminology}
    \begin{itemize}
        \item \textbf{Walk:} A sequence of vertices in which each vertex is adjacent to the next. Both vertices and edges can be repeated.
        \begin{itemize}
            \item e.g. 1-2-3-5-2-1-5-3
        \end{itemize}
        
        \item \textbf{Trail:} A walk with no repeated edges.
        \begin{itemize}
            \item e.g. 1-2-3-5-2-6
        \end{itemize}
        
        \item \textbf{Path:} A trail with no repeated vertices, except the start and end vertices in the case of a cycle.
        \begin{itemize}
            \item e.g. 1-2-3-5-4
        \end{itemize}
        
        \item \textbf{Simple Path:} A path with no repetition of vertices.
        \begin{itemize}
            \item e.g. 1-2-3-5
        \end{itemize}
        
        \item \textbf{Circuit:} A trail that begins and ends at the same vertex.
        \begin{itemize}
            \item e.g. 5-2-1-5-3-4-5
        \end{itemize}
        
        \item \textbf{Cycle:} A path that begins and ends at the same vertex, with no other repeated vertices.
        \begin{itemize}
            \item e.g. 5-3-4-5
        \end{itemize}
        
    \end{itemize}
\end{terminology}

\textbf{Is there an edge between $v_i$ and $v_j$?}
\begin{itemize}
    \item \textbf{AM:} $O(1)$
    \item \textbf{AL:} $O(d)$ 
    \begin{itemize}
        \item $d$: Maximum degree in the graph.
    \end{itemize}
\end{itemize}
\vspace{1em}

\textbf{Find all vertices adjacent to $v_i$:}
\begin{itemize}
    \item \textbf{AM:} $O(V)$ 
    \item \textbf{AL:} $O(d)$ 
\end{itemize}

\subsubsection{Morphisms}
\begin{definition}
    \begin{itemize}
        \item A \textbf{homomorphism} between two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ is a mapping
        \[
        f: V_1 \to V_2 \quad \text{s.t.} \quad (u, v) \in E_1 \Rightarrow (f(u), f(v)) \in E_2 \quad \text{for all} \quad u, v \in V_1.
        \]
        \item An \textbf{isomorphism} between two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ is a bijection
        \[
        f: V_1 \to V_2 \quad \text{s.t.} \quad (u, v) \in E_1 \iff (f(u), f(v)) \in E_2 \quad \text{for all} \quad u, v \in V_1.
        \]
        We say that $G_1$ is isomorphic to $G_2$ or $G_1 \cong G_2$.
    \end{itemize}
    
    \textbf{Explanation in English:}
    \begin{itemize}
        \item \textbf{Homomorphism:}
        \begin{itemize}
            \item A function that maps vertices of one graph to another while preserving edges.
        \end{itemize}

        \item \textbf{Isomorphism:}
        \begin{itemize}
            \item A bijective function that maps one graph onto another, preserving edges both ways. The two graphs are identical in structure.
        \end{itemize}
    \end{itemize}
\end{definition}

\subsubsection{Spanning tree}
\begin{definition}
    $G'$ is a spanning tree of $G$, if it is a tree and $V'=V$.
\end{definition}

\subsection{Forest}
\begin{definition}
    An undirected graph is acyclic but possibly disconnected.
\end{definition}

\customFigure[1]{00_Images/Tree_Forest_Difference.png}{(a) A free tree, (b) A forest, (c) a graph that contains a cycle and is therefore neither a tree nor a forest.}

\subsubsection{Binary trees}
\begin{definition}
    A \textbf{binary tree} $T$ is a structure defined on a finite set of nodes that either
    \begin{itemize}
        \item contains no nodes, or 
        \item is composed of three disjoint sets of nodes: a \textbf{root} node, a \textbf{left subtree}, and a \textbf{right subtree}.
    \end{itemize}
\end{definition}

\subsubsection{Terminology}
\begin{terminology}
    \begin{itemize}
        \item \textbf{Empty Tree:} A binary tree with no nodes.
        \item \textbf{Left and Right Child:} The roots of the non-empty left and right subtrees of the root.
        \item \textbf{Full Binary Tree:} Every node is either a leaf or has exactly two children.
        \item \textbf{Position Matters:} The distinction between left and right children is crucial in a binary tree, unlike in general ordered trees.
    \end{itemize}
\end{terminology}

\subsection{Rooted and ordered trees}
    \subsubsection{Rooted trees}
    \begin{definition}
        A \textbf{rooted tree} is a free tree in which one of the vertices is distinguished from the others.
        \begin{itemize}
            \item \textbf{Root:} Distinguished vertex of the tree.
            \item \textbf{Node:} Vertex of a rooted tree.
        \end{itemize}
    \end{definition}
    
    \subsubsection{Ordered trees}
    \begin{definition}
        An \textbf{ordered tree} is a rooted tree in which the children of each node are ordered.
    \end{definition}

    \customFigure[1]{00_Images/Rooted_Ordered_Trees.png}{Rooted and ordered trees. If the tree is ordered, the relative left-to-right order of the children of a node matters, but if rooted, then they are the same tree.}

    \subsubsection{Positional trees}
    \begin{definition}
        The children of a node are labeled with distinct positive integers.
    \end{definition}


%% BST

\subsubsection{Successor}
\begin{definition}
    \begin{lstlisting}[language=Python, caption={Successor}]
        Successor(x)
            (a) Left-most node of its right subtree.
            (b) Run "in-order" on the fly.
    \end{lstlisting}

    \begin{itemize}
        \item $O(n) \rightarrow O(1) \text{ for balanced trees}.$
    \end{itemize}
\end{definition}