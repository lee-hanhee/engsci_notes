\subsection{Greedy Algorithms Intro and Class Scheduling}

\begin{intuition}
    \begin{itemize}
        \item "Greed works, greed is right, greed is good." 
        \item Greedy algorithms make locally optimal choices at each stage with the goal of finding a global optimum. This approach can approximate NP-complete problems.
    \end{itemize}
\end{intuition}

\subsubsection{Characteristics}
\begin{definition}
        Greedy algorithms are good for max/min problems. 
        \begin{itemize}
            \item \textbf{Optimal substructure:} Same as DP
            \item \textbf{Greedy principle:} At every step do greedy choice that looks at the best option at the time begin (i.e. aims to identify and select the most favorable choice at each of the given stagesaims to identify and select the most favorable choice at each of the given stages). 
        \end{itemize}
\end{definition}

\begin{warning}
    You will need to do a proof of correctness for DP and GA.
\end{warning}

\subsubsection{Class Scheduling Example}
\begin{example}
    Given one room and many classes to schedule, the goal is to maximize the number of classes scheduled in one day. The greedy aprooach is that
    \begin{enumerate}
        \item Sort by finish time (i.e. start with the class that starts more early)
        \item Schedule classes in this order based on sorted times.
        \item The time complexity for sorting is $O(n\log(n))$.
    \end{enumerate}
    \customFigure[0.75]{00_Images/CSE.png}{Class scheduling example.}

\end{example}

\subsubsection{Proof of Correctness of Class Scheduling (On exams)}
\begin{derivation}

    Prove $g_1,\ldots,g_m$ is optimal:
    \begin{itemize}
        \item ATaC: Given that greedy choices $g_1, g_2, \dots, g_m$ are not optimal, but some other optimal choices $o_1, o_2, \dots, o_m$ exist $(m > n)$. 
        \item By the way the greedy algorithm works, $f(g_i) \leq f(o_i)$ (f is the finish time) because it replaces based on the finish time. 
        \customFigure[0.75]{00_Images/POC.png}{It is showing that you can replace $o$ by $g$. In the diagram, it uses $n$ instead of $m$, but same meaning (i.e. $n=m$).}
        \begin{itemize}
            \item You can replace $o_1$ with $g_1$ by construction because $g_1$ finishes first. 
            \item You can replace $o_2$ with $g_2$ by construction
            \item Up to $g_m$
            \item $o_{m+1} \ldots o_k$ cannot exist because the greedy algorithm didn't pick any. So we proved that the optimal solution is $G$
        \end{itemize}
        \item $o_{n+1},\dots, o_m$ cannot exist as greedy algorithm would take these into account, therefore, this implies a contradiction since the greedy algorithm would take these into account.
        \item Therefore, $O=G$ and $G=\text{optimal}$.
    \end{itemize}
\end{derivation}

\begin{warning}
    Usually use proof by contradiction. 
\end{warning}

\subsection{Knapsack Problem (Show Difference B/W DP and GA)}

\subsubsection{Problem setup}
\begin{intuition}
A thief breaks into a store and the thief's bag can carry up to $\omega$ weight. The store has $n$ items, each with weight $w_i$ and value $v_i$ of item $i$.
\end{intuition}

\subsubsection{Types of Knapsack Problems}
\begin{definition}
    \begin{itemize}
        \item \textbf{Fractional:} Can take part of an item (Greedy solution).
        \item \textbf{0-1:} Can take or not take an item (Dynamic programming).
    \end{itemize}
\end{definition}

\subsubsection{Fractional Knapsack}
\begin{definition}
For fractional knapsack:
\begin{itemize}
    \item Sort $\frac{v_i}{w_i}$ and keep taking items in decreasing order.
    \begin{itemize}
        \item \textbf{Intuition:} Take the most valuable per weight items until you reach the max weight that you can hold.
    \end{itemize}
\end{itemize}
\end{definition}

\subsubsection{0-1 Knapsack}
\begin{definition}
    For 0-1 knapsack, fix some order $1, \dots, n$ of the items. The recursive formula for 0-1 knapsack is:
    \[
    C[i, \omega] = \begin{cases} 
        0 & \text{if } i = 0 \text{ and } \omega = 0 \\
        C[i-1, \omega] & \text{if } \omega_i > \omega \\
        \max \{C[i-1, \omega - \omega_i] +v_i, \; C[i-1, \omega]\} & \text{if } \omega_i \leq \omega
    \end{cases}
    \]
    \begin{itemize}
        \item $C[i, \omega]$ denotes the optimal (most valuable) selection in $1\ldots i$ with $\omega$ as the leftover weight.
        \item First case is an artificial base case.
        \item Second case is when the weight exceeds limit, so the max is the previous value that was valid. 
        \item Third case is the max of not picking the item and picking the item and adding the value of it and subtracting the weight of the result of adding item for the next recursion.
    \end{itemize}
\end{definition}

\begin{definition}
    $O(n\cdot \omega)$
\end{definition}

\subsection{Intro to Huffman Encoding}
\begin{intuition}
    \customFigure[0.75]{00_Images/I.png}{45\% contains a, 13\% contains b, and so on. Converting this to binary using $d(c_1)$, which is fixed.}
    \begin{itemize}
        \item e.g. bad in binary is $001 \; 000 \; 011$.
    \end{itemize}
    \customFigure[0.75]{00_Images/CHARS.png}{$d(c_2)$ variable size encoding to account for assigning fewer bits to more frequent characters}
    \begin{itemize}
        \item e.g. bad $101 \; 0 \; 111$. Since is $a$ is 45\%, therefore, we only assign one bit to it. 
        \item \textbf{Note:} This encoding is not unique. 
    \end{itemize}
\end{intuition}

\subsubsection{Problem Statement}
\begin{definition}
Huffman coding minimizes the size of the encoded file $B(T)$ by assigning fewer bits to more frequent characters.
\begin{equation*}
    B(T) = \text{size of text encoding} = \sum_c f(c)d(c) 
\end{equation*}
\begin{itemize}
    \item $\sum_c$: Sum over all characters in text file of binary code of the character times the frequency of the character.
\end{itemize}
\end{definition}

\subsubsection{Steps in Huffman Coding}
\begin{process}
    \begin{enumerate}
        \item Write two letters $x$,$y$ with the lowest frequency in a tree-like fashion.
        \item These two letters is replaced by the two letters with a new pseudo key $z$ that sums their frequencies $f(z)=f(x)+f(y)$
        \item Repeat till you get to a root. 
        \item Label the resulting tree with $0s$ and $1s$ going left and right respectively.
        \customFigure[0.75]{00_Images/TREE.png}{Tree}
    \end{enumerate}
    \begin{itemize}
        \item e.g. $100 \; 0 \; 110$
        \item the tree allows to read the word off the word for bat. 
        \item Note: This puts the highest frequency characters up high with the lowest binary bits. 
        \item No matter then encoding, you can create a tree. 
    \end{itemize}

\end{process}

\subsubsection{Time complexity}
\begin{definition}
    Time $O(n \log n)$
\end{definition}

\begin{intuition}
    If you can prove the greedy choice is the correct choice, and then remaining optimal solution, then you can prove that the rest of the greedy choices will be also be good. you are breaking down the problem more and more. In this example, the first two greedy chocies is choosing the lowest frequenceis, and then you have a smaller problem, which can be done inductively. These are the two theorems that we will be proving. 
\end{intuition}

\subsubsection{Optimal substructure}
\begin{theorem}
Every optimal tree can have the two lowest frequency keys being siblings, differing by one bit only in the highest tree depth. 

\end{theorem}

\begin{derivation}
    Assume the two lowest frequency keys are $x$ and $y$ and $T$ is optimal. 
    \customFigure[0.75]{00_Images/E.png}{T optimal, T' (swapped b and x from T optimal), and T'' (swapped y and c from T')}
    \begin{itemize}
        \item \textbf{Note:} This shows that the tree remains optimal. This is just like the class scheduling. (i.e. $T''$ is still optimal after rotations). T' is not optimal, but T'' is still optimal. 
        \item \textbf{Key:} No matter what tree you give him, you can transform into the greedy without changing optimality. 
        \item \textbf{Optimal to greedy:} T optimal is the tree that they give and x and y are the two lowest frequency, but they are not at the highest depth, so we can perform swaps to get the greedy option, which is T''.
        \item \textbf{Note 2:} Optimal means minimizing $B(T)$.
    \end{itemize}
    \vspace{1em}

    Prove that $B(T') \leq B(T)$
    \begin{enumerate}
        \item Let $B(T)$ be the sum of the frequencies and their bit-lengths:
        \[
        B(T) = \sum_c f(c) \cdot d(c)
        \]
        We want to show that $B(T') \leq B(T_{\text{opt}})$, where $T_{\text{opt}}$ is the optimal tree and $T'$ is a different tree (similar proof for $B(T'') \leq B(T')$)
        \vspace{1em}
    
        \item By the greedy principle:
        \[
        B(T_{\text{opt}}) - B(T') = \sum_c f(c)(d(c_{opt}) - \sum_c f(c) d(c_{gr}) = ((f(b) - f(x))) (d_{T_{opt}} (b) - d_{T_{opt}} (x)) \geq 0
        \]
        Thus, $B(T_{\text{opt}}) \geq B(T')$.
    \end{enumerate}
\end{derivation}

\begin{derivation}
    \customFigure[0.75]{00_Images/D1.png}{Derivation.}
\end{derivation}