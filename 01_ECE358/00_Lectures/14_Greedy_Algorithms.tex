\subsection{Greedy Algorithms Intro and Class Scheduling}

\begin{intuition}
    \begin{itemize}
        \item "Greed works, greed is right, greed is good." 
        \item Greedy algorithms make locally optimal choices at each stage with the goal of finding a global optimum. This approach can approximate NP-complete problems.
    \end{itemize}
\end{intuition}

\subsubsection{Characteristics}
\begin{definition}
        Greedy algorithms are good for max/min problems. 
        \begin{itemize}
            \item \textbf{Optimal substructure:} Same as DP
            \item \textbf{Greedy principle:} At every step do greedy choice that looks at the best option at the time begin (i.e. aims to identify and select the most favorable choice at each of the given stagesaims to identify and select the most favorable choice at each of the given stages). 
        \end{itemize}
\end{definition}

\begin{warning}
    You will need to do a proof of correctness for DP and GA.
\end{warning}

\subsubsection{Class Scheduling Example}
\begin{example}
    Given one room and many classes to schedule, the goal is to maximize the number of classes scheduled in one day. The greedy aprooach is that
    \begin{enumerate}
        \item Sort by finish time (i.e. start with the class that starts more early)
        \item Schedule classes in order based on sorted times.
        \item The time complexity for sorting is $O(n\log(n))$.
    \end{enumerate}
    \customFigure[0.75]{00_Images/CSE.png}{Class scheduling example.}

\end{example}

\subsubsection{Proof of Correctness of Class Scheduling (On exams)}
\begin{derivation}

    Prove $g_1,\ldots,g_m$ is optimal:
    \begin{itemize}
        \item ATaC: Given that greedy choices $g_1, g_2, \dots, g_m$ are not optimal, but some other optimal choices $o_1, o_2, \dots, o_m$ exist $(m > n)$. 
        \item By the way the greedy algorithm works, $f(g_i) \leq f(o_i)$ (f is the finish time) because it replaces based on the finish time. 
        \customFigure[0.75]{00_Images/POC.png}{WHAT IS THIS EXAMPLE SHOWING.}
        \begin{itemize}
            \item You can replace $o_1$ with $g_1$ by construction because $g_1$ finishes first. 
            \item You can replace $o_2$ with $g_2$ by constrcution 
            \item Up to $g_m$
            \item $o_{m+1} \ldots o_k$ cannot exist because the greedy algorithm didn't pick any. So we proved that the optimal solution is $G$
        \end{itemize}
        \item $o_{n+1},\dots, o_m$ cannot exist as greedy algorithm would take these into account, therefore, this implies a contradiction since the greedy algorithm would take these into account.
        \item Therefore, $O=G$ and $G=\text{optimal}$.
    \end{itemize}
\end{derivation}

\begin{warning}
    Usually use proof by contradiction. 
\end{warning}

\subsection{Knapsack Problem (Show Difference B/W DP and GA)}

\subsubsection{Problem setup}
\begin{intuition}
A thief breaks into a store and the thief's bag can carry up to $\omega$ weight. The store has $n$ items, each with weight $w_i$ and value $v_i$ of item $i$.
\end{intuition}

\subsubsection{Types of Knapsack Problems}
\begin{definition}
    \begin{itemize}
        \item \textbf{Fractional:} Can take part of an item (Greedy solution).
        \item \textbf{0-1:} Can take or not take an item (Dynamic programming).
    \end{itemize}
\end{definition}

\subsubsection{Fractional Knapsack}
\begin{definition}
For fractional knapsack:
\begin{itemize}
    \item Sort $\frac{v_i}{w_i}$ and keep taking items in decreasing order.
    \begin{itemize}
        \item \textbf{Intuition:} Take the most valuable per weight items until you reach the max weight that you can hold.
    \end{itemize}
\end{itemize}
\end{definition}

\subsubsection{0-1 Knapsack}
\begin{definition}
    For 0-1 knapsack, fix some order $1, \dots, n$ of the items. The recursive formula for 0-1 knapsack is:
    \[
    C[i, \omega] = \begin{cases} 
        0 & \text{if } i = 0 \text{ and } \omega = 0 \\
        C[i-1, \omega] & \text{if } \omega_i > \omega \\
        \max \{C[i-1, \omega - \omega_i] +v_i, \; C[i-1, \omega]\} & \text{if } \omega_i \leq \omega
    \end{cases}
    \]
    \begin{itemize}
        \item $C[i, \omega]$ denotes the optimal (most valuable) selection in $1\ldots i$ with $\omega$ as the leftover weight.
        \item First case is an artificial base case.
    \end{itemize}
\end{definition}

\begin{definition}
    $O(n\cdot \omega)$
\end{definition}

\subsection{Intro to Huffman Encoding}
\begin{intuition}
    \customFigure[0.75]{00_Images/I.png}{45\% contains a, 13\% contains b, and so on. Converting this to binary using $d(c_1)$, which is fixed.}
    \begin{itemize}
        \item e.g. bad in binary is $001 \; 000 \; 011$.
    \end{itemize}
    \customFigure[0.75]{00_Images/CHARS.png}{$d(c_2)$ variable size encoding to account for assigning fewer bits to more frequent characters}
    \begin{itemize}
        \item e.g. bad $101 \; 0 \; 111$. Since is $a$ is 45\%, therefore, we only assign one bit to it. 
    \end{itemize}
\end{intuition}

\subsubsection{Problem Statement}
\begin{definition}
Huffman coding minimizes the size of the encoded file $B(T)$ by assigning fewer bits to more frequent characters.
\begin{equation*}
    B(T) = \text{size of text encoding} = \sum_c f(c)d(c) 
\end{equation*}
\begin{itemize}
    \item $\sum_c$: Sum over all characters in text file of binary code of the character times the frequency of the character.
\end{itemize}
\end{definition}

\subsubsection{Steps in Huffman Coding}
\begin{itemize}
    \item Write two letters with the lowest frequency in a tree-like fashion.
    \item Replace the two letters with a new character that sums their frequencies.
    \item Label the resulting tree.
    \customFigure[0.75]{00_Images/LG.png}{Tree}
\end{itemize}

\subsubsection{Algorithm}
\begin{process}
    \begin{itemize}
        \item Unite the two lowest frequency keys $x$ and $y$, new pseudo key $z$ w/ $f(z) = f(x) + f(y)$ 
        \item Repeat till you get a root. 
        \item Label resulting tree w/ $0s$ and $1s$ going left and right. 
    \end{itemize}

    \begin{itemize}
        \item e.g. $100 \; 0 \; 110$
        \item the tree allows to read the word off the word for bat. 
        \item Note: This puts the highest frequency characters up high with the lowest binary bits. 
    \end{itemize}

\end{process}

\subsubsection{Time complexity}
\begin{definition}
    Time $O(n \log n)$
\end{definition}

\begin{intuition}
    If you can prove the greedy choice is the correct choice, and then remaining optimal solution, then you can prove that the rest of the greedy choices will be also be good. you are breaking down the problem more and more. In this example, the first two greedy chocies is choosing the lowest frequenceis, and then you have a smaller problem, which can be done inductively. These are the two theorems that we will be proving. 
\end{intuition}

Now prove the optimality: 
\subsubsection{Greedy principle}
\begin{theorem}
Every optimal tree can have the two lowest frequency keys being siblings, differing by one bit only in the highest tree depth. 

\end{theorem}


No matter then encoding, you can create a tree. 
IS THIS A PROOF?
\begin{derivation}
    Assume the two lowest frequency keys are $x$ and $y$ and $T$ remains optimal. 
    \customFigure[0.75]{00_Images/E.png}{T optimal, T' (swapped b and x from T optimal), and T'' (swapped y and c from T')}
    \begin{itemize}
        \item \textbf{Note:} This shows that the tree remains optimal. This is just like the class scheduling. 
        \item T' is not optimal, but T'' is still optimal. 
        \item No matter what tree you give him, you can transsform into the greedy without changing optimality. 
        \item You can do the swaps to put x and y, but they are not at the greatest depth, you can perform swaps to put them at the lowest frequnecies. 
        \item T otpimal is the tree that they give and x and y are the two lowest frequency, but they are not at the highest depth, so we can perform swaps to get the greedy option, which is T''.
        \item Optimal --> greedy. This is just like the class scheduling. Given a class scheduling, we can find an optimal tree. 
        \item class schedulign was shown with g1,g2,,...,gn and o1,o2,on. you can keep on swaps to create the optimal tree that you were given and transform it into the greeedy option. 
        \item optimal means minimizing $B(T)$.
    \end{itemize}
    \vspace{1em}

    Prove that $B(T') \leq B(T)$

    \begin{align*}
        B(T) - B(T') = \sum f(c) d_T (c) - \sum f(c) d_{T'} (c) = ((f(b) - f(x))) (d_T (b) -
    \end{align*}
    \begin{itemize}
        \item $f(b)$
    \end{itemize}

    \begin{enumerate}
        \item Let $B(T)$ be the sum of the frequencies and their bit-lengths:
        \[
        B(T) = \sum_c f(c) \cdot d(c)
        \]
        We want to show that $B(T') \leq B(T_{\text{opt}})$, where $T_{\text{opt}}$ is the optimal tree and $T'$ is a different tree (similar proof for $B(T'') \leq B(T')$)
        \vspace{1em}
    
        \item By the greedy principle:
        \[
        B(T_{\text{opt}}) - B(T') = \sum_c f(c)(d(c_{opt}) - \sum_c f(c) d(c_{gr}) \geq 0
        \]
        Thus, $B(T_{\text{opt}}) \geq B(T')$.
    \end{enumerate}
\end{derivation}

\subsubsection{Optimal Substructure}
WHAT IS THE WORKFLOW, HOW DID WE GET HERE. IS THIS A THEOREM?
\begin{theorem}
    \begin{enumerate}
        \item Let \( x \) $\leftarrow z \rightarrow$ \( y \) be part of \( T_{\text{opt}} \) for character set \( C \). Then
        \[
        T_{\text{opt}} = T_{\text{opt}} - \{x, y\} + \{z\}
        \]
        is optimal prefix code for new set \( C' = C - \{x, y\} + \{z\} \), where \( f(z') = f(x) + f(y) \).
    
        \item We have
        \[
        f(x) \cdot d_{T_{\text{opt}}}(x) + f(y) \cdot d_{T_{\text{opt}}}(y) = f(z) \cdot d_{T_{\text{opt}}}(z) + [f(x) + f(y)]
        \]
        because 
        \[
        d_{T_{\text{opt}}}(x) = d_{T_{\text{opt}}}(y) = d_{T_{\text{opt}}}(z) + 1.
        \]
    
        \item Every time we "move" from \( T_{\text{opt}} \) to \( T'_{\text{opt}} \) by merging 2 keys, we have
        \[
        \mathcal{B}(T_{\text{opt}}) = \mathcal{B}(T'_{\text{opt}}) + (f(x) + f(y)).
        \]
    
        \item ATaC \( T'_{\text{opt}} \) is not optimal for \( C' \), but \( T'' \) is. 
    
        \[
        \mathcal{B}(T'') + (f(x) f(y)) < \mathcal{B}(T'_{\text{opt}}) + (f(x) f(y)),
        \]
        which is more optimal than \( T_{\text{opt}} \), a contradiction.
    \end{enumerate}

\end{theorem}

\begin{derivation}
    Let in $T_{opt}$ for character set $C$ then 
    \begin{equation*}
        T' = T - \{x,y\} + \{z\}
    \end{equation*}
    is optimal prefix code for $C = C - \{x,y\} + {z}$, where $f(z) = f(x) + f(y)$
    \begin{itemize}
        \item ef is combined together to ahve a frequencty of $f(ef) = 14$. This essentially combines them together. 
        \item In english, $B(T) = B(T') + [f(x) + f(y)]$
        \item show tha t 
    \end{itemize}
\end{derivation}