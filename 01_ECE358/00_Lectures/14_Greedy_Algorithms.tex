\subsection{Greedy Algorithms Intro and Class Scheduling}

\begin{intuition}
    \begin{itemize}
        \item "Greed works, greed is right, greed is good." 
        \item Greedy algorithms make locally optimal choices at each stage with the goal of finding a global optimum. This approach can approximate NP-complete problems.
    \end{itemize}
\end{intuition}

\subsubsection{Characteristics}
\begin{intuition}
        \begin{itemize}
            \item Optimal substructure
            \item Greedy principle: a greedy choice now leads to a global optimum
        \end{itemize}
\end{intuition}

\subsubsection{Class Scheduling Example}
\begin{example}
    Given one room and many classes, the goal is to maximize the number of classes scheduled in one day. 
    \begin{itemize}
        \item Sort by finish time.
        \item Schedule classes in order based on sorted times.
    \end{itemize}
    \customFigure[0.75]{00_Images/CSE.png}{Class scheduling example.}
    The time complexity for sorting is $O(n\log(n))$.
\end{example}

\subsubsection{Proof of Correctness of Class Scheduling (On exams)}
\begin{derivation}
    \begin{itemize}
        \item ATaC: Given that greedy choices $g_1, g_2, \dots, g_n$ are not optimal, but some other optimal choices $o_1, o_2, \dots, o_m$ exist $(m > n)$. 
        \item By the way the greedy algorithm works, $f(g_i) \leq f(o_i)$ (f is the finish time) because it replaces based on the finish time. 
        \customFigure[0.75]{00_Images/POC.png}{WHAT IS THIS EXAMPLE SHOWING.}
        \item $o_{n+1},\dots, o_m$ cannot exist as greedy algorithm would take these into account, therefore, this implies a contradiction since the greedy algorithm would take these into account.
    \end{itemize}
\end{derivation}

\subsection{Knapsack Problem}

\subsubsection{Problem setup}
\begin{intuition}
A thief breaks into a store and the thief's bag can carry up to $\omega$ weight. The store has $n$ items, each with weight $w_i$ and value $v_i$ of item $i$.
\end{intuition}

\subsubsection{Types of Knapsack Problems}
\begin{definition}
    \begin{itemize}
        \item \textbf{Fractional:} Can take part of an item (Greedy solution).
        \item \textbf{0-1:} Can take or not take an item (Dynamic programming).
    \end{itemize}
\end{definition}

\subsubsection{Fractional Knapsack}
\begin{definition}
For fractional knapsack:
\begin{itemize}
    \item Sort $\frac{v_i}{w_i}$ and keep taking items in that order.
\end{itemize}
\end{definition}

\subsubsection{0-1 Knapsack}
\begin{definition}
    For 0-1 knapsack, fix some order $1, \dots, n$ of the items. The recursive formula for 0-1 knapsack is:
    \[
    C[i, \omega] = \begin{cases} 
        0 & \text{if } i = 0, \omega = 0 \\
        C[i-1, \omega] & \text{if } w_i > \omega \\
        \max \{C[i-1, \omega - \omega_i] + v_i, \; C[i-1, \omega]\} & \text{if } \omega \geq \omega_i 
    \end{cases}
    \]
    where $C[i, \omega]$ denotes the maximum value achievable with $i$ items and a leftover weight of $\omega$.
\end{definition}

\subsection{Intro to Huffman Encoding}
\begin{intuition}
    \customFigure[0.75]{00_Images/I.png}{WHAT IS THIS SHOWING ME }
    \begin{itemize}
        \item 
    \end{itemize}
\end{intuition}

\subsubsection{Problem Statement}
\begin{intuition}
Huffman coding minimizes the size of the encoded file by assigning fewer bits to more frequent characters.
\begin{equation*}
    \sum_c f(c)d(c) = \text{total number of bits needed.}
\end{equation*}
\end{intuition}

\begin{example}
    \customFigure[0.75]{00_Images/CHARS.png}{WHAT IS THIS SHOWING ME?}
    \begin{itemize}
        \item 
    \end{itemize}
\end{example}

\subsubsection{Steps in Huffman Coding}
\begin{itemize}
    \item Write two letters with the lowest frequency in a tree-like fashion.
    \item Replace the two letters with a new character that sums their frequencies.
    \item Label the resulting tree.
    \customFigure[0.75]{00_Images/LG.png}{Tree}
\end{itemize}

\subsubsection{Proof of Correctness}
\begin{theorem}
Every optimal tree can have the two lowest frequency keys being siblings, that differ only on one bit, with the greatest depth.

\end{theorem}

IS THIS A PROOF?
\begin{derivation}
    Assume two lowest frequency keys $x$ and $y$ and $T''$ remains optimal. 
    \customFigure[0.75]{00_Images/E.png}{T optimal, T', and T''}
    \vspace{1em}

    \begin{enumerate}
        \item Let $B(T)$ be the sum of the frequencies and their bit-lengths:
        \[
        B(T) = \sum_c f(c) \cdot d(c)
        \]
        We want to show that $B(T') \leq B(T_{\text{opt}})$, where $T_{\text{opt}}$ is the optimal tree and $T'$ is a different tree (similar proof for $B(T'') \leq B(T')$)
        \vspace{1em}
    
        \item By the greedy principle:
        \[
        B(T_{\text{opt}}) - B(T') = \sum_c f(c)(d(c_{opt}) - \sum_c f(c) d(c_{gr}) \geq 0
        \]
        Thus, $B(T_{\text{opt}}) \geq B(T')$.
    \end{enumerate}
\end{derivation}

\subsubsection{Optimal Substructure}
WHAT IS THE WORKFLOW, HOW DID WE GET HERE. IS THIS A THEOREM?
\begin{theorem}
    \begin{enumerate}
        \item Let \( x \) $\leftarrow z \rightarrow$ \( y \) be part of \( T_{\text{opt}} \) for character set \( C \). Then
        \[
        T_{\text{opt}} = T_{\text{opt}} - \{x, y\} + \{z\}
        \]
        is optimal prefix code for new set \( C' = C - \{x, y\} + \{z\} \), where \( f(z') = f(x) + f(y) \).
    
        \item We have
        \[
        f(x) \cdot d_{T_{\text{opt}}}(x) + f(y) \cdot d_{T_{\text{opt}}}(y) = f(z) \cdot d_{T_{\text{opt}}}(z) + [f(x) + f(y)]
        \]
        because 
        \[
        d_{T_{\text{opt}}}(x) = d_{T_{\text{opt}}}(y) = d_{T_{\text{opt}}}(z) + 1.
        \]
    
        \item Every time we "move" from \( T_{\text{opt}} \) to \( T'_{\text{opt}} \) by merging 2 keys, we have
        \[
        \mathcal{B}(T_{\text{opt}}) = \mathcal{B}(T'_{\text{opt}}) + (f(x) + f(y)).
        \]
    
        \item ATaC \( T'_{\text{opt}} \) is not optimal for \( C' \), but \( T'' \) is. 
    
        \[
        \mathcal{B}(T'') + (f(x) f(y)) < \mathcal{B}(T'_{\text{opt}}) + (f(x) f(y)),
        \]
        which is more optimal than \( T_{\text{opt}} \), a contradiction.
    \end{enumerate}

\end{theorem}