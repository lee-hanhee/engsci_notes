Asymptotic efficiency focuses on understanding how the running time of an algorithm grows with the input size, particularly for large inputs.
\subsection{Big-O (Upper Bound)}
    \begin{definition} 
        $ f(n) = O(g(n)) \text{ iff } \exists \text{ positive constants } c \text{ and } n_0 \text{ s.t. } 0 \leq f(n) \leq c g(n) \; \forall \; n \geq n_0 $
        \begin{itemize}
            \item \textbf{Asymptotic upper bound:} Grows \textbf{no faster} than a certain rate, based on the highest-order term.
        \end{itemize}
    \end{definition}

    \begin{warning}
        Every function $f(n)$ in the set $O(g(n))$ must be \textbf{asymptotically nonnegative} (i.e. $f(n)$ must be positive whenever $n$ is sufficiently large).
    \end{warning}

    \begin{example}
        To show that \( 13n + 7 \in O(n) \), we need to find constants \( c > 0 \) and \( n_0 > 0 \) such that:

        \[
        13n + 7 \leq c \cdot n \quad \text{for all } n \geq n_0.
        \]

        \begin{enumerate}
            \item Start with the inequality:
            
            \[
            13n + 7 \leq c \cdot n.
            \]
            
            \item Rearrange the inequality to isolate \( c \):
            
            \[
            13n + 7 \leq c \cdot n \implies 13 + \frac{7}{n} \leq c.
            \]
            
            \item For this inequality to hold for all \( n \geq n_0 \), choose \( n_0 \) such that \( \frac{7}{n_0} \) is sufficiently small.
            
            \item Set \( n_0 = 7 \):
            
            \[
            \frac{7}{n_0} = \frac{7}{7} = 1.
            \]
            
            Now the inequality becomes:
            
            \[
            13 + \frac{7}{7} = 13 + 1 = 14 \leq c.
            \]
            
            \item Choose \( c = 14 \). This choice satisfies:
            
            \[
            13 + \frac{7}{n} \leq 14 \quad \text{for all } n \geq 7.
            \]
            
            \item Therefore, with \( c = 14 \) and \( n_0 = 7 \), we have:
            
            \[
            13n + 7 \leq 14n \quad \text{for all } n \geq 7.
            \]
        \end{enumerate}
    \end{example}

    \begin{example}
        To prove that \( \frac{1}{2}n^2 - 3n \in O(n^2) \), we need to show:

        \[
        \frac{1}{2}n^2 - 3n \leq C \cdot n^2 \quad \text{for some constants } C > 0 \text{ and } n_0 > 0 \text{, for all } n \geq n_0.
        \]

        \begin{enumerate}
            \item Start with the inequality we want to prove:
            
            \[
            \frac{1}{2}n^2 - 3n \leq C \cdot n^2.
            \]
            
            \item Rearrange the inequality to isolate \( C \):
            
            \[
            \frac{1}{2}n^2 - 3n \leq C \cdot n^2 \implies \frac{1}{2} - \frac{3}{n} \leq C.
            \]
            
            \item For the inequality to hold for all \( n \geq n_0 \), we need to make \( \frac{3}{n} \) sufficiently small:
            
            \item Set \( n_0 = 1 \). For \( n \geq 1 \):
            
            \[
            \frac{3}{n} \leq 3 \quad \Rightarrow \quad \frac{1}{2} - \frac{3}{n} \leq \frac{1}{2} - 3.
            \]
            
            However, this inequality might lead to negative values for smaller \( n \), so we adjust \( C \) accordingly.
            
            \item Choose \( C = \frac{7}{2} \). With this choice:
            
            \[
            \frac{1}{2} - \frac{3}{n} \leq C \quad \text{for all } n \geq 1.
            \]
            
            Setting \( C = 3.5 \) ensures that:
            
            \[
            \frac{1}{2} - \frac{3}{n} \leq 3.5 \quad \text{holds for all } n \geq 1.
            \]
            
            \item Therefore, with \( C = 3.5 \) and \( n_0 = 1 \), we have:
            
            \[
            \frac{1}{2}n^2 - 3n \leq 3.5n^2 \quad \text{for all } n \geq 1.
            \]
        \end{enumerate}

        This confirms that \( \frac{1}{2}n^2 - 3n \in O(n^2) \).
    \end{example}

    \begin{warning}
        The choice of $C=3.5$ was just one option. Any constant C that satisfies the inequality works. It doesn't have to be the tightest bound for big-O.
    \end{warning}

    \begin{example}
        \[
        n! = 1 \cdot 2 \cdot 3 \cdot 4 \cdot \ldots \cdot n \leq n \cdot n \cdot n \cdot \ldots \cdot n = O(n^n) \quad \text{(with \( n \) terms)}
        \]

        Similarly:

        \[
        \log n! = O(n \log n) = O(\log n^n)
        \]
    \end{example}

    \begin{example}
        \textbf{Statement:} Prove that $2^{n+1} = O(2^n)$.
        
        \textbf{Step 1:} Begin by noting that $2^{n+1}$ can be rewritten as:
        \[
        2^{n+1} = 2 \cdot 2^n
        \]
        We want to show that $2^{n+1} \leq C \cdot 2^n$ for some constant $C$ and for all $n \geq n_0$.
        
        \textbf{Step 2:} To satisfy the definition of Big-O, we need:
        \[
        0 \leq 2 \cdot 2^n \leq C \cdot 2^n \quad \forall n \geq 0
        \]
        This inequality holds if $C \geq 2$, as the factor of 2 on the left-hand side is less than or equal to $C$.
        
        \textbf{Step 3:} Let us choose $C = 3$ and $n_0 = 1$. Now check the inequality for all $n \geq n_0$:
        \[
        0 \leq 2 \cdot 2^n \leq C \cdot 2^n \quad \forall n \geq n_0
        \]
        This is true because $2 \cdot 2^n \leq 3 \cdot 2^n$ holds for all $n \geq 1$.
        
        \textbf{Conclusion:} Therefore, we have shown that:
        \[
        2^{n+1} = O(2^n)
        \]
    \end{example}
        

    \begin{example}
        \textbf{Statement:} Prove that $2^{n+1} = \Omega(2^n)$.
        
        \textbf{Step 1:} Recall the definition of $\Omega$: we need to show that $2^{n+1} \geq C \cdot 2^n$ for some constant $C > 0$ and for all $n \geq n_0$.
        
        \textbf{Step 2:} Again, we can rewrite $2^{n+1}$ as:
        \[
        2^{n+1} = 2 \cdot 2^n
        \]
        We want to show:
        \[
        0 \leq C \cdot 2^n \leq 2 \cdot 2^n \quad \forall n \geq 0
        \]
        This inequality holds if $C \leq 2$.
        
        \textbf{Step 3:} Let us choose $C = 1$ and $n_0 = 1$. Now verify the inequality:
        \[
        0 \leq C \cdot 2^n \leq 2 \cdot 2^n \quad \forall n \geq 1
        \]
        This is true because $1 \cdot 2^n \leq 2 \cdot 2^n$ holds for all $n \geq 1$.
        
        \textbf{Conclusion:} Therefore, we have shown that:
        \[
        2^{n+1} = \Omega(2^n)
        \]
    \end{example}

\subsection{Big-Omega (Lower Bound)}
    \begin{definition}
        $ f(n) = \Omega(g(n)) \text{ iff } \exists \text{ positive constants } c \text{ and } n_0 \text{ such that } 0 \leq c g(n) \leq f(n) \; \forall \; n \geq n_0 $
        \begin{itemize}
            \item \textbf{Asymptotic lower bound:} Grows \textbf{at least as fast} as a certain rate, based on the highest-order term.
        \end{itemize}
    \end{definition}

    \begin{example}
        \begin{enumerate}
            \item The sum can be approximated by considering that the sequence is bounded above by:
        
            \[
            1 + 2 + 3 + \ldots + n \geq \left\lceil \frac{n}{2} \right\rceil + \left( \left\lceil \frac{n}{2} \right\rceil + 1 \right) + \ldots + n.
            \]
        
            \item Grouping terms to form pairs around the middle:
        
            \[
                \left\lceil \frac{n}{2} \right\rceil + \left( \left\lceil \frac{n}{2} \right\rceil + 1 \right) + \ldots + n \geq \left\lceil \frac{n}{2} \right\rceil \times \left\lceil \frac{n}{2} \right\rceil = \left\lceil \frac{n^2}{4} \right\rceil
            \]
        
            \item To express this in terms of \( \Omega(n^2) \), note that:
        
            \[
            \left\lceil \frac{n^2}{4} \right\rceil \geq \frac{n^2}{4}
            \]
        
            \item This implies:
        
            \[
            \left( \frac{1}{n} \right)n^2 = \Omega(n^2)
            \]
        
            \item Finding \( n_1 \): We choose \( n_1 \) such that the inequality holds for all \( n \geq n_1 \). Let's find \( n_1 \) by ensuring:
        
            \[
            \left\lceil \frac{n^2}{4} \right\rceil \geq c \cdot n^2
            \]
        
            Choosing \( c = \frac{1}{4} \), we need \( \left\lceil \frac{n^2}{4} \right\rceil \geq \frac{n^2}{4} \). This inequality naturally holds for \( n \geq 1 \). Thus, we can choose \( n_1 = 1 \), which gives:
        
            \[
            \left\lceil \frac{n^2}{4} \right\rceil = \Omega(n^2)
            \]
        
            This satisfies the inequality for all \( n_1 \geq 1 \).
        \end{enumerate}
        
        Thus, the sum \( 1 + 2 + 3 + \ldots + n \) is \( \Omega(n^2) \), indicating a quadratic lower bound.        
    \end{example}

\subsection{Big-Theta (Tight Bound)}
    \begin{definition}
        $ f(n) = \Theta(g(n)) \text{ iff } \exists \text{ positive constants } c_1, \; c_2, \; n_0 \text{ s.t. } 0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \; \forall \; n \geq n_0 $
        \begin{itemize}
            \item \textbf{Asymptotically tight bounds:} Grows \textbf{precisely} at a certain rate, based on the highest-order term.
        \end{itemize}
    \end{definition}

    \begin{example}
        Consider the sum \( 1 + 2 + 3 + \ldots + n \), prove that it is $\Theta(n^2)$
        \begin{enumerate}
            \item From the previous example with Big-Omega, we know that $\Omega(n^2)$
            \item The sum can be written as:

            \[
            1 + 2 + 3 + \ldots + n = \frac{n(n+1)}{2}
            \]
        
            \item Expanding the expression:
        
            \[
            = \frac{1}{2}n^2 + \frac{1}{2}n
            \]
        
            \item Therefore, this sum is:
        
            \[
            = O(n^2)
            \]
            \item By Theorem 3.1, $\Theta(n^2)$.
        \end{enumerate}
    \end{example}

    \begin{example}
        Let \( f(n) = \sum_{i=1}^n i^k \). We want to show that \( f(n) = \Omega(n^{k+1}) \) and \( f(n) = O(n^{k+1}) \), indicating that \( f(n) \) is asymptotically bounded both above and below by \( n^{k+1} \).

        \begin{enumerate}

            \item Prove that \( f(n) = O(n^{k+1}) \):
            
            \begin{enumerate}
                \item Start with the definition of \( f(n) \):
                
                \[
                f(n) = \sum_{i=1}^n i^k.
                \]
                
                \item Observe that each \( i^k \) is less than or equal to \( n^k \) for \( i \leq n \). Therefore, we can bound the sum from above:
                
                \[
                \sum_{i=1}^n i^k \leq \sum_{i=1}^n n^k = n^k + n^k + \ldots + n^k = n \cdot n^k = n^{k+1}.
                \]
                
                \item This implies:
                
                \[
                f(n) = O(n^{k+1}),
                \]
                
                where the constant \( C \) can be chosen as 1. Thus, there exists a constant \( C > 0 \) such that \( f(n) \leq C \cdot n^{k+1} \) for sufficiently large \( n \).
                
            \end{enumerate}

            \item Prove that \( f(n) = \Omega(n^{k+1}) \):
            
            \begin{enumerate}
                \item To establish a lower bound, consider using a double sequence method:
                
                \[
                2f(n) = \sum_{i=1}^n i^k + \sum_{i=1}^n (n-i+1)^k.
                \]
                
                \item Notice that the first sum is ascending (i.e., \( 1^k + 2^k + \ldots + n^k \)) and the second sum is descending (i.e., \( n^k + (n-1)^k + \ldots + 1^k \)):
                
                \[
                2f(n) = \sum_{i=1}^n i^k + \sum_{i=1}^n (n-i+1)^k.
                \]
                
                \item Each pair \( i^k + (n-i+1)^k \) is greater than or equal to \( \left(\frac{n}{2}\right)^k \):
                
                \[
                2f(n) \geq \sum_{i=1}^n \left(\frac{n}{2}\right)^k = \left(\frac{n}{2}\right)^k + \ldots + \left(\frac{n}{2}\right)^k = n \cdot \left(\frac{n}{2}\right)^k = \frac{n^{k+1}}{2^k}.
                \]
                
                \item This simplifies to:
                
                \[
                2f(n) \geq \frac{n^{k+1}}{2^k} \Rightarrow f(n) \geq \frac{n^{k+1}}{2^{k+1}}.
                \]
                
                \item Therefore:
                
                \[
                f(n) = \Omega(n^{k+1}),
                \]
                
                showing that \( f(n) \) has a quadratic lower bound.
                
            \end{enumerate}

        \end{enumerate}

        \textbf{Conclusion:} Since \( f(n) \) is bounded above and below by \( n^{k+1} \), we conclude:

        \[
        f(n) = \Theta(n^{k+1}).
        \]
    \end{example}

    \begin{example}
        Show that $(n + a)^b = \Theta(n^b)$
        \begin{enumerate}
            \item \textbf{Key Observation:}
                \[
                n + a \leq n + |a| \leq 2n \quad \text{if} \ n \geq |a|
                \]
                \[
                n + a \geq n - |a| \geq \frac{1}{2} n \quad \text{if} \ n \geq 2|a|
                \]
            
            \item $(n + a)^b = O(n^b):$
            \[
            0 \leq (n + a)^b \leq C \cdot n^b
            \]
            Let $n_0 = |a| \Rightarrow 0 \leq (n + a)^b \leq (2n)^b = 2^b n^b$.
            Therefore, let $C \geq 2^b$ and the above holds (i.e. using the key observation):
            \[
            \therefore (n + a)^b = O(n^b)
            \]

            \item $(n + a)^b = \Omega(n^b):$
            \[
            0 \leq C \cdot n^b \leq (n + a)^b
            \]
            Let $n_0 = 2|a| \Rightarrow (n + a)^b \geq \left(\frac{1}{2} n \right)^b = \left(\frac{1}{2} \right)^b n^b$.
            Therefore, let $0 < C \leq \left(\frac{1}{2}\right)^b$ and the above holds:
            \[
            \therefore (n + a)^b = \Omega(n^b)
            \]

            \item Conclusion:
            \[
            \therefore (n + a)^b = O(n^b) \quad \text{and} \quad (n + a)^b = \Omega(n^b) \Rightarrow (n + a)^b = \Theta(n^b)
            \]
            \end{enumerate}
    \end{example}

\customFigure[1]{00_Images/BigO_Omega_Theta_Notation.png}{Graphical examples of the Big-O, Big-Omega, and Big-Theta.}

\begin{intuition}
    In all asymptotic notations, you are trying to describe a function after $n_0$ (i.e. ignore all flunctuation before).
\end{intuition}

\subsection{Theorem 3.1}
    \begin{theorem}
        $f(n) = \Theta(g(n))$ iff $f(n) = O(g(n))$ and $f(n)=\Omega(g(n))$.
    \end{theorem}

\subsection{Small-O (Strictly Slower)}
\begin{definition}
    $f(n) = o(g(n)) \text{ iff } \forall c > 0, \exists n_0 > 0 \text{ s.t. } 0 \leq f(n) < c g(n) \text{ for all } n \geq n_0.$
\end{definition}

\subsection{Small-Omega (Strictly Faster)}
\begin{definition}
    $f(n) = \omega(g(n)) \text{ iff } \forall c > 0, \exists n_0 > 0 \text{ s.t. } 0 \leq c g(n) < f(n) \text{ for all } n \geq n_0.$
\end{definition}

\subsection{Asymptotic notation and running times}
    \begin{warning}
        Make sure that the asymptotic notation you use is as precise as possible without overstating which running time (i.e. worst-case, best-case, or any average/expected-case) it applies to.
    \end{warning}

\subsection{Abuses of asymptotic notation}
    \begin{intuition}
        \begin{itemize}
            \item \textbf{Equality:}
            \begin{itemize}
                \item When asymptotic notation stands alone on the RS of an equation (or inequality), then $= \text{ means } \in$.
                \item When asymptotic notation is in a formula, it is an anonymous function (AF) that we do not care to name.
                \item When asymptotic notation appears on the LS of an equation: No matter how the AF is chosen on the LS, there is a way to choose the AF on the RS to make the equation valid.
            \end{itemize}
            \item \textbf{Variable tending toward $\infty$ must be inferred from context:}
            \begin{itemize}
                \item e.g. $O(g(n))$, then we are interested in the growth of $g(n)$ as $n$ grows.
                \item e.g. $f(n) = O(1)$, then $f(n)$ is bounded from above by a constant as $n$ goes to $\infty$.
                \item e.g. $T(n) = O(1) \text{ for } n<3$ is that there exists a positive constant $c$ such that $T(n) \leq c \text{ for } n<3$.
            \end{itemize}
        \end{itemize}
    \end{intuition}

\subsection{Comparing function properties}
    \begin{definition}
        
        \textbf{Transitivity:}
        \begin{itemize}
            \item $f(n) = \Theta(g(n)) \text{ and } g(n)=\Theta(h(n)) \text{ imply } f(n) = \Theta(h(n))$
            \item $f(n) = O(g(n)) \text{ and } g(n)=O(h(n)) \text{ imply } f(n) = O(h(n))$
            \item $f(n) = \Omega(g(n)) \text{ and } g(n)=\Omega(h(n)) \text{ imply } f(n) = \Omega(h(n))$
        \end{itemize}
        \vspace{1em}

        \textbf{Symmetry:}
        \begin{itemize}
            \item $f(n) = \Theta(g(n)) \text{ iff } g(n) = \Theta(f(n))$.
        \end{itemize}
        \vspace{1em}

        \textbf{Transpose symmetry:}
        \begin{itemize}
            \item $f(n) = O(g(n)) \text{ iff } g(n) = \Omega(f(n))$
        \end{itemize}
        \vspace{1em}

        \textbf{Different functions:}
        \begin{itemize}
            \item \( n^a = O(n^b) \), iff \( a \leq b \).
            \item \( \log_a(n) = O(\log_b(n)) \), $\forall$ \( a, b \).
            \item \( c^n = O(d^n) \), iff \( c \leq d \).
            \item If \( f(n) = O(f'(n)) \) and \( g(n) = O(g'(n)) \), then:
            \begin{enumerate}
                \item \( f(n) \cdot g(n) = O(f'(n) \cdot g'(n)) \).
                \item \( f(n) + g(n) = O(\max\{f'(n), g'(n)\}) \).
                \begin{itemize}
                    \item ' is not a derivative, just another function. 
                \end{itemize}
            \end{enumerate}
        \end{itemize}
        
    \end{definition}

    \begin{intuition}
        \begin{itemize}
            \item \( 1 \ll \log^*(n) \ll \log^i n \ll (\lg n)^a \ll n^b \ll c^n \quad \forall i, a, b, c \)
            
            \item \( f(n) \ll g(n) \Rightarrow h(n)f(n) \ll h(n)g(n) \)
            
            \item \( f(n) \ll g(n) \Rightarrow f(n)^{h(n)} \ll g(n)^{h(n)} \)
            
            \item \( f(n) \ll g(n) \) and \( \lim_{n \to \infty} h(n) > 1 \Rightarrow h(n)^{f(n)} \ll h(n)^{g(n)} \)
            
            \item Assume \( f \) and \( h \) are eventually positive, i.e. \( \lim_{n \to \infty} f(n) > 0 \) and \( \lim_{n \to \infty} h(n) > 0 \). 
            \begin{itemize}
                \item Note: \( f(n) \ll g(n) \Rightarrow \lim_{n \to \infty} g(n) = \infty \)
            \end{itemize}
            
            \item \( f(n) \ll g(n) \) means \( f(n) = o(g(n)) \)
            
            \item Notice the direction of the replication.
        \end{itemize}    
    \end{intuition}

    \begin{derivation}
        In tutorial 2.
    \end{derivation}
    
        \subsubsection{Common justification steps}
        \begin{intuition}
            \begin{itemize}
                \item Exponential functions grow faster than polynomial functions, which grow faster than polylogarithmic functions.
                \item The base of a logarithm doesn’t matter asymptotically, but the base of an exponential and the degree of a polynomial do matter.
            \end{itemize}
        \end{intuition}

\subsection{Limit method}
    \begin{definition}
        Find the asymptotic relationship between two functions for which you might not have any intuition about. 
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0 \Rightarrow f(n) = o(g(n))
        \end{equation}
        
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty \Rightarrow f(n) = \omega(g(n))
        \end{equation}
        
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} < \infty \text{ i.e. is anything finite } \Leftrightarrow f(n) = O(g(n))
        \end{equation}
        
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} > 0 \text{ i.e. non-zero } \Leftrightarrow f(n) = \Omega(g(n))
        \end{equation}
        
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = C \text{ s.t. } 0 < C < \infty \Leftrightarrow f(n) = \Theta(g(n))
        \end{equation}        
    \end{definition}

    \subsubsection{L'Hôpital's rule}
    \begin{definition}
        $\text{if } \lim_{x \to a} \frac{f(x)}{g(x)} = \frac{0}{0} \text{ or } \frac{\infty}{\infty}$ then,
        \begin{equation}
            \lim_{x \to c} \frac{f(x)}{g(x)} = \lim_{x \to c} \frac{f'(x)}{g'(x)}
        \end{equation}
        
    \end{definition}

    \begin{warning}
        $f(n)$ and $g(n)$ need to be different functions. 
    \end{warning}

\subsection{Polynomially-bounded}
\begin{definition}
    \begin{itemize}
        \item \textbf{Polylogarithmically bounded:} \( f(n) = O((\lg n)^k) \quad \exists k > 0 \)
        
        \item \textbf{Polynomially bounded:} \( f(n) = O(n^k) \quad \exists k > 0 \)
        
        \item \textbf{Exponentially bounded:} \( f(n) = O(k^n) \quad \exists k > 0 \)
    \end{itemize}
\end{definition}

\begin{intuition}
    \begin{itemize}
        \item \textbf{Notation:} \( (\lg n)^2 = (\lg n) \cdot (\lg n) \) (polylogarithmic) and \( \lg^2 n = \lg(\lg n) \) (iterated log). 
    \end{itemize}
\end{intuition}

\begin{theorem}
    $f(n) = O(n^k) \Leftrightarrow \lg(f(n)) = O(\lg n)$
\end{theorem}

\begin{theorem}
    All logarithmically bounded functions are polynomially bounded, i.e. \((\lg n)^a = O(n^b) \quad \forall a, b > 0\)
\end{theorem}

\begin{theorem}
    All polynomially bounded functions are exponentially bounded, i.e. \( f(n) = O(n^a) \Rightarrow f(n) = O(b^n) \quad \forall a > 0 \text{ and } \forall b > 1\)
\end{theorem}

\begin{derivation}
    In tutorial 2.
\end{derivation}

\subsection{Logarithm method}
    \begin{definition}
        \textbf{Limit of Logs:} $\lim_{x \to a} (\log_b f(x)) = \log_b \left( \lim_{x \to a} f(x) \right)$

        $\text{Suppose we want to compute } \lim_{n \to \infty} \frac{f(n)}{g(n)} = L$

        \[
        \therefore \lg \left( \lim_{n \to \infty} \frac{f(n)}{g(n)} \right) = \lg L
        \]

        \[
        \therefore \lim_{n \to \infty} \left( \lg \frac{f(n)}{g(n)} \right) = \lg L
        \]

        \[
        \therefore \lim_{n \to \infty} \frac{f(n)}{g(n)} = L = 2^{\lim_{n \to \infty} \lg \left( \frac{f(n)}{g(n)} \right)}
        \]
    \end{definition}

    \begin{example}
        \begin{enumerate}

            \item \textbf{Apply logarithm to the limit.}
            Start by taking the logarithm of the limit:
            \[
            \lg \left( \lim_{n \to \infty} \frac{2^{n+1}}{4^n} \right)
            \]
        
            \item \textbf{Move the logarithm inside the limit.}
            Since the logarithm function is continuous, we can move it inside the limit:
            \[
            \lim_{n \to \infty} \left( \lg \frac{2^{n+1}}{4^n} \right)
            \]
        
            \item \textbf{Simplify the logarithmic expression.}
            Use the properties of logarithms to break down the expression:
            \[
            \lim_{n \to \infty} \left( (n+1) \lg 2 - n \lg 4 \right)
            \]
            Since \( \lg 4 = 2 \lg 2 \), we can rewrite the expression as:
            \[
            \lim_{n \to \infty} \left( (n+1) \lg 2 - n \cdot 2 \lg 2 \right)
            \]
        
            \item \textbf{Combine the terms.}
            Simplify the terms:
            \[
            \lim_{n \to \infty} \left( (n+1) \lg 2 - 2n \lg 2 \right) = \lim_{n \to \infty} \left( \lg 2 - n \lg 2 \right)
            \]
            This simplifies to:
            \[
            \lim_{n \to \infty} (1 - n) \lg 2 = \lim_{n \to \infty} 1 - n = -\infty
            \]
        
            \item \textbf{Conclusion for the ratio.}
            Therefore, the original limit of the ratio is:
            \[
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = 2^{-\infty} = 0
            \]
            \begin{itemize}
                \item Raise to the power of 2 since the log is base 2.
            \end{itemize}
            
        \end{enumerate}
    \end{example}
