Asymptotic efficiency focuses on understanding how the running time of an algorithm grows with the input size, particularly for large inputs.
\subsection{Big-O (Upper Bound)}
    \begin{definition} 
        $ O(g(n)) = \left\{ f(n) \text{: } \exists \text{ positive constants } c \text{ and } n_0 \text{ s.t. } 0 \leq f(n) \leq c g(n) \; \forall \; n \geq n_0 \right\} $
        \begin{itemize}
            \item \textbf{Asymptotic upper bound:} Grows \textbf{no faster} than a certain rate, based on the highest-order term.
        \end{itemize}
    \end{definition}

    \begin{warning}
        Every function $f(n)$ in the set $O(g(n))$ must be \textbf{asymptotically nonnegative} (i.e. $f(n)$ must be positive whenever $n$ is sufficiently large).
    \end{warning}

    \begin{example}
        To show that \( 13n + 7 \in O(n) \), we need to find constants \( c > 0 \) and \( n_0 > 0 \) such that:

        \[
        13n + 7 \leq c \cdot n \quad \text{for all } n \geq n_0.
        \]

        \begin{enumerate}
            \item Start with the inequality:
            
            \[
            13n + 7 \leq c \cdot n.
            \]
            
            \item Rearrange the inequality to isolate \( c \):
            
            \[
            13n + 7 \leq c \cdot n \implies 13 + \frac{7}{n} \leq c.
            \]
            
            \item For this inequality to hold for all \( n \geq n_0 \), choose \( n_0 \) such that \( \frac{7}{n_0} \) is sufficiently small.
            
            \item Set \( n_0 = 7 \):
            
            \[
            \frac{7}{n_0} = \frac{7}{7} = 1.
            \]
            
            Now the inequality becomes:
            
            \[
            13 + \frac{7}{7} = 13 + 1 = 14 \leq c.
            \]
            
            \item Choose \( c = 14 \). This choice satisfies:
            
            \[
            13 + \frac{7}{n} \leq 14 \quad \text{for all } n \geq 7.
            \]
            
            \item Therefore, with \( c = 14 \) and \( n_0 = 7 \), we have:
            
            \[
            13n + 7 \leq 14n \quad \text{for all } n \geq 7.
            \]
        \end{enumerate}
    \end{example}

    \begin{example}
        To prove that \( \frac{1}{2}n^2 - 3n \in O(n^2) \), we need to show:

        \[
        \frac{1}{2}n^2 - 3n \leq C \cdot n^2 \quad \text{for some constants } C > 0 \text{ and } n_0 > 0 \text{, for all } n \geq n_0.
        \]

        \begin{enumerate}
            \item Start with the inequality we want to prove:
            
            \[
            \frac{1}{2}n^2 - 3n \leq C \cdot n^2.
            \]
            
            \item Rearrange the inequality to isolate \( C \):
            
            \[
            \frac{1}{2}n^2 - 3n \leq C \cdot n^2 \implies \frac{1}{2} - \frac{3}{n} \leq C.
            \]
            
            \item For the inequality to hold for all \( n \geq n_0 \), we need to make \( \frac{3}{n} \) sufficiently small:
            
            \item Set \( n_0 = 1 \). For \( n \geq 1 \):
            
            \[
            \frac{3}{n} \leq 3 \quad \Rightarrow \quad \frac{1}{2} - \frac{3}{n} \leq \frac{1}{2} - 3.
            \]
            
            However, this inequality might lead to negative values for smaller \( n \), so we adjust \( C \) accordingly.
            
            \item Choose \( C = \frac{7}{2} \). With this choice:
            
            \[
            \frac{1}{2} - \frac{3}{n} \leq C \quad \text{for all } n \geq 1.
            \]
            
            Setting \( C = 3.5 \) ensures that:
            
            \[
            \frac{1}{2} - \frac{3}{n} \leq 3.5 \quad \text{holds for all } n \geq 1.
            \]
            
            \item Therefore, with \( C = 3.5 \) and \( n_0 = 1 \), we have:
            
            \[
            \frac{1}{2}n^2 - 3n \leq 3.5n^2 \quad \text{for all } n \geq 1.
            \]
        \end{enumerate}

        This confirms that \( \frac{1}{2}n^2 - 3n \in O(n^2) \).
    \end{example}

    \begin{warning}
        The choice of $C=3.5$ was just one option. Any constant C that satisfies the inequality works. It doesn't have to be the tightest bound for big-O.
    \end{warning}

    \begin{example}
        \[
        n! = 1 \cdot 2 \cdot 3 \cdot 4 \cdot \ldots \cdot n \leq n \cdot n \cdot n \cdot \ldots \cdot n = O(n^n) \quad \text{(with \( n \) terms)}
        \]

        Similarly:

        \[
        \log n! = O(n \log n) = O(\log n^n)
        \]
    \end{example}

\subsection{Big-Omega (Lower Bound)}
    \begin{definition}
        $ \Omega(g(n)) = \left\{ f(n) \text{: } \exists \text{ positive constants } c \text{ and } n_0 \text{ such that } 0 \leq c g(n) \leq f(n) \; \forall \; n \geq n_0 \right\} $
        \begin{itemize}
            \item \textbf{Asymptotic lower bound:} Grows \textbf{at least as fast} as a certain rate, based on the highest-order term.
        \end{itemize}
    \end{definition}

    \begin{example}
        \begin{enumerate}
            \item The sum can be approximated by considering that the sequence is bounded above by:
        
            \[
            1 + 2 + 3 + \ldots + n \geq \left\lceil \frac{n}{2} \right\rceil + \left( \left\lceil \frac{n}{2} \right\rceil + 1 \right) + \ldots + n.
            \]
        
            \item Grouping terms to form pairs around the middle:
        
            \[
                \left\lceil \frac{n}{2} \right\rceil + \left( \left\lceil \frac{n}{2} \right\rceil + 1 \right) + \ldots + n \geq \left\lceil \frac{n}{2} \right\rceil \times \left\lceil \frac{n}{2} \right\rceil = \left\lceil \frac{n^2}{4} \right\rceil
            \]
        
            \item To express this in terms of \( \Omega(n^2) \), note that:
        
            \[
            \left\lceil \frac{n^2}{4} \right\rceil \geq \frac{n^2}{4}
            \]
        
            \item This implies:
        
            \[
            \left( \frac{1}{n} \right)n^2 = \Omega(n^2)
            \]
        
            \item Finding \( n_1 \): We choose \( n_1 \) such that the inequality holds for all \( n \geq n_1 \). Let's find \( n_1 \) by ensuring:
        
            \[
            \left\lceil \frac{n^2}{4} \right\rceil \geq c \cdot n^2
            \]
        
            Choosing \( c = \frac{1}{4} \), we need \( \left\lceil \frac{n^2}{4} \right\rceil \geq \frac{n^2}{4} \). This inequality naturally holds for \( n \geq 1 \). Thus, we can choose \( n_1 = 1 \), which gives:
        
            \[
            \left\lceil \frac{n^2}{4} \right\rceil = \Omega(n^2)
            \]
        
            This satisfies the inequality for all \( n_1 \geq 1 \).
        \end{enumerate}
        
        Thus, the sum \( 1 + 2 + 3 + \ldots + n \) is \( \Omega(n^2) \), indicating a quadratic lower bound.        
    \end{example}

\subsection{Big-Theta (Tight Bound)}
    \begin{definition}
        $ \Theta(g(n)) = \left\{ f(n) \text{: } \exists \text{ positive constants } c_1, \; c_2, \; n_0 \text{ s.t. } 0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \; \forall \; n \geq n_0 \right\} $
        \begin{itemize}
            \item \textbf{Asymptotically tight bounds:} Grows \textbf{precisely} at a certain rate, based on the highest-order term.
        \end{itemize}
    \end{definition}

    \begin{example}
        Consider the sum \( 1 + 2 + 3 + \ldots + n \), prove that it is $\Theta(n^2)$
        \begin{enumerate}
            \item From the previous example with Big-Omega, we know that $\Omega(n^2)$
            \item The sum can be written as:

            \[
            1 + 2 + 3 + \ldots + n = \frac{n(n+1)}{2}
            \]
        
            \item Expanding the expression:
        
            \[
            = \frac{1}{2}n^2 + \frac{1}{2}n
            \]
        
            \item Therefore, this sum is:
        
            \[
            = O(n^2)
            \]
            \item By Theorem 3.1, $\Theta(n^2)$.
        \end{enumerate}
    \end{example}

    \begin{example}
        Let \( f(n) = \sum_{i=1}^n i^k \). We want to show that \( f(n) = \Omega(n^{k+1}) \) and \( f(n) = O(n^{k+1}) \), indicating that \( f(n) \) is asymptotically bounded both above and below by \( n^{k+1} \).

        \begin{enumerate}

            \item Prove that \( f(n) = O(n^{k+1}) \):
            
            \begin{enumerate}
                \item Start with the definition of \( f(n) \):
                
                \[
                f(n) = \sum_{i=1}^n i^k.
                \]
                
                \item Observe that each \( i^k \) is less than or equal to \( n^k \) for \( i \leq n \). Therefore, we can bound the sum from above:
                
                \[
                \sum_{i=1}^n i^k \leq \sum_{i=1}^n n^k = n^k + n^k + \ldots + n^k = n \cdot n^k = n^{k+1}.
                \]
                
                \item This implies:
                
                \[
                f(n) = O(n^{k+1}),
                \]
                
                where the constant \( C \) can be chosen as 1. Thus, there exists a constant \( C > 0 \) such that \( f(n) \leq C \cdot n^{k+1} \) for sufficiently large \( n \).
                
            \end{enumerate}

            \item Prove that \( f(n) = \Omega(n^{k+1}) \):
            
            \begin{enumerate}
                \item To establish a lower bound, consider using a double sequence method:
                
                \[
                2f(n) = \sum_{i=1}^n i^k + \sum_{i=1}^n (n-i+1)^k.
                \]
                
                \item Notice that the first sum is ascending (i.e., \( 1^k + 2^k + \ldots + n^k \)) and the second sum is descending (i.e., \( n^k + (n-1)^k + \ldots + 1^k \)):
                
                \[
                2f(n) = \sum_{i=1}^n i^k + \sum_{i=1}^n (n-i+1)^k.
                \]
                
                \item Each pair \( i^k + (n-i+1)^k \) is greater than or equal to \( \left(\frac{n}{2}\right)^k \):
                
                \[
                2f(n) \geq \sum_{i=1}^n \left(\frac{n}{2}\right)^k = \left(\frac{n}{2}\right)^k + \ldots + \left(\frac{n}{2}\right)^k = n \cdot \left(\frac{n}{2}\right)^k = \frac{n^{k+1}}{2^k}.
                \]
                
                \item This simplifies to:
                
                \[
                2f(n) \geq \frac{n^{k+1}}{2^k} \Rightarrow f(n) \geq \frac{n^{k+1}}{2^{k+1}}.
                \]
                
                \item Therefore:
                
                \[
                f(n) = \Omega(n^{k+1}),
                \]
                
                showing that \( f(n) \) has a quadratic lower bound.
                
            \end{enumerate}

        \end{enumerate}

        \textbf{Conclusion:} Since \( f(n) \) is bounded above and below by \( n^{k+1} \), we conclude:

        \[
        f(n) = \Theta(n^{k+1}).
        \]
    \end{example}

\customFigure[1]{00_Images/BigO_Omega_Theta_Notation.png}{Graphical examples of the Big-O, Big-Omega, and Big-Theta.}

\begin{example}
    Find the Big-O, Big-Omega, and Big-Theta of the following function: $7n^3 + 100n^2 - 20n + 6$. 

    \begin{enumerate}
        \item \textbf{Highest-order term:} $7n^3$
        \item \textbf{Remove constants:} $n^3$
        \item \textbf{Big-O notation:} $\text{O}(n^3)$
        \begin{itemize}
            \item In general, $\text{O}(n^c)$ for any constant $c\geq3$ because the function grows no faster than this.
        \end{itemize}
        \item \textbf{Big-Omega notation:} $\Omega (n^3)$
        \begin{itemize}
            \item In general, $\Omega (n^c)$ for any constant $c\leq3$ because the function grows at least as fast than this.
        \end{itemize}
        \item \textbf{Big-Theta:} Since O and $\Omega$ are the same, therefore, $\Theta (n^3)$ (by theorem)
    \end{enumerate}
\end{example}

\begin{intuition}
    In all asymptotic notations, you are trying to describe a function after $n_0$ (i.e. ignore all flunctuation before).
\end{intuition}

\subsection{Theorem 3.1}
    \begin{theorem}
        $f(n) = \Theta(g(n))$ iff $f(n) = O(g(n))$ and $f(n)=\Omega(g(n))$.
    \end{theorem}

\subsection{Asymptotic notation and running times}
    \begin{warning}
        Make sure that the asymptotic notation you use is as precise as possible without overstating which running time (i.e. worst-case, best-case, or any average/expected-case) it applies to.
    \end{warning}

    \begin{example}
        What asymptotic notation should you use for insertion sort's worst-case, best-case, and general running time? 
        \vspace{1em}

        \begin{itemize}
            \item \textbf{Worst-case:} $O(n^2)\text{, } \Omega(n^2)\text{, and } \Theta(n^2)$ can be used, but $\Theta(n^2)$ is the most precise. 
            \item \textbf{Best-case:} $O(n)\text{, } \Omega(n)\text{, and } \Theta(n)$ can be used, but $\Theta(n)$ is the most precise. 
            \item \textbf{Average-case:} $O(n^2)$ because in all cases, its running time grows no faster than $n^2$. Or $\Omega(n)$ because in all cases, its running time is at least as fast as $n$.
        \end{itemize}
    \end{example}

\subsection{Abuses of asymptotic notation}
    \begin{intuition}
        \begin{itemize}
            \item \textbf{Equality:}
            \begin{itemize}
                \item When asymptotic notation stands alone on the RS of an equation (or inequality), then $= \text{ means } \in$.
                \item When asymptotic notation is in a formula, it is an anonymous function (AF) that we do not care to name.
                \item When asymptotic notation appears on the LS of an equation: No matter how the AF is chosen on the LS, there is a way to choose the AF on the RS to make the equation valid.
            \end{itemize}
            \item \textbf{Variable tending toward $\infty$ must be inferred from context:}
            \begin{itemize}
                \item e.g. $O(g(n))$, then we are interested in the growth of $g(n)$ as $n$ grows.
                \item e.g. $f(n) = O(1)$, then $f(n)$ is bounded from above by a constant as $n$ goes to $\infty$.
                \item e.g. $T(n) = O(1) \text{ for } n<3$ is that there exists a positive constant $c$ such that $T(n) \leq c \text{ for } n<3$.
            \end{itemize}
        \end{itemize}
    \end{intuition}

\subsection{Comparing function properties}
    \begin{definition}
        
        \textbf{Transitivity:}
        \begin{itemize}
            \item $f(n) = \Theta(g(n)) \text{ and } g(n)=\Theta(h(n)) \text{ imply } f(n) = \Theta(h(n))$
            \item $f(n) = O(g(n)) \text{ and } g(n)=O(h(n)) \text{ imply } f(n) = O(h(n))$
            \item $f(n) = \Omega(g(n)) \text{ and } g(n)=\Omega(h(n)) \text{ imply } f(n) = \Omega(h(n))$
        \end{itemize}
        \vspace{1em}

        \textbf{Symmetry:}
        \begin{itemize}
            \item $f(n) = \Theta(g(n)) \text{ iff } g(n) = \Theta(f(n))$.
        \end{itemize}
        \vspace{1em}

        \textbf{Transpose symmetry:}
        \begin{itemize}
            \item $f(n) = O(g(n)) \text{ iff } g(n) = \Omega(f(n))$
        \end{itemize}
        \vspace{1em}

        \textbf{Different functions:}
        \begin{itemize}
            \item \( n^a \in O(n^b) \), iff \( a \leq b \).
            \item \( \log_a(n) \in O(\log_b(n)) \), $\forall$ \( a, b \).
            \item \( c^n \in O(d^n) \), iff \( c \leq d \).
            \item If \( f(n) \in O(f'(n)) \) and \( g(n) \in O(g'(n)) \), then:
            \begin{enumerate}
                \item \( f(n) \cdot g(n) \in O(f'(n) \cdot g'(n)) \).
                \item \( f(n) + g(n) \in O(\max\{f'(n), g'(n)\}) \).
                \begin{itemize}
                    \item ' is not a derivative, just another function. 
                \end{itemize}
            \end{enumerate}
        \end{itemize}
        
    \end{definition}

    \begin{intuition}
        \begin{itemize}
            \item $f(n) = O(g(n)) \text{ is like } a \leq b$
            \item $f(n) = \Omega(g(n)) \text{ is like } a \geq b$
            \item $f(n) = \Theta(g(n)) \text{ is like } a = b$
        \end{itemize}
    \end{intuition}

\subsection{Deterministic vs. Randomized}
\begin{definition}
    \begin{itemize}
        \item \textbf{Deterministic algorithms}: These algorithms produce the same output for a given input every time they are run, as their execution follows a predictable sequence of steps.
        \item \textbf{Randomized algorithms}: These algorithms incorporate random choices in their logic, which may lead to different outputs or different execution paths for the same input across multiple runs.
        \begin{itemize}
            \item Monte Carlo algorithms
            \item Las Vegas algorithms
        \end{itemize}
    \end{itemize}
\end{definition}

\subsection{Types of analysis}
\begin{definition}
    \begin{itemize}
        \item \textbf{Best-case}: The algorithm runs in the shortest time due to optimal input.
        \item \textbf{Worst-case}: The algorithm takes the longest time due to the most challenging input.
        \item \textbf{Average-case}: The running time averaged over all inputs, assuming equal likelihood.
        \item \textbf{Expected-case}: The running time based on a probability distribution of inputs.
        \item \textbf{Amortized}: The average time per operation over a sequence of operations.
    \end{itemize}
\end{definition}

\subsection{Polynomially-bounded}
    \begin{definition}
        $f(n)$ is polynomially-bounded if $f(n)=O\left(n^k\right)$ for some real value of $k$.
    \end{definition}

    \begin{theorem}
        $f(n)=O\left(n^k\right)$ iff $lg(f(n))=O(lg(n))$
    \end{theorem}

\subsection{Limit method}
    \begin{definition}
        Find the asymptotic relationship between two functions for which you might not have any intuition about. 
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0 \implies f(n) = \mathcal{O}(g(n))
        \end{equation}
        
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = c, \; 0 < c < \infty \implies f(n) = \Theta(g(n))
        \end{equation}
        
        \begin{equation}
            \lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty \implies f(n) = \Omega(g(n))
        \end{equation}
    \end{definition}

    \subsubsection{L'HÃ´pital's rule}
    \begin{definition}
        If $\lim_{x \to c} f(x) = \lim_{x \to c} g(x) = 0$ or $\pm \infty$, then:
        \begin{equation}
            \lim_{x \to c} \frac{f(x)}{g(x)} = \lim_{x \to c} \frac{f'(x)}{g'(x)}
        \end{equation}
        
    \end{definition}

    \begin{warning}
        $f(n)$ and $g(n)$ need to be different functions. 
    \end{warning}

    \subsubsection{Logs of limits and limits of logs}
        \begin{definition}
            \begin{equation}
                \lg \left( \lim_{x \to c} g(x) \right) = \lim_{x \to c} \lg(g(x))
            \end{equation}
        \end{definition}