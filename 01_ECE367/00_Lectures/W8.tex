\subsection{Least squares}
\begin{definition}
    The least squares problem is:
    \[
    \min_x \|Ax - b\|_2^2.
    \]
\end{definition}

\subsubsection{Motivation:}
\begin{intuition}
    Recall the system of linear equations: 
    \[
    A x = b
    \]
    where $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$ are given. \\

    The goal is to solve for the unknown $x \in \mathbb{R}^n$. \\

    If $A$ is a square matrix and has full rank (i.e., $A$ is invertible), then the solution is:
    \[
    x^* = A^{-1}b.
    \]

    What if $A$ is not square or not full-rank? What can we say about the solutions to the system of linear equations? \\

    There are two cases:
    \begin{enumerate}
        \item Overdetermined ($m > n$)
        \item Underdetermined ($m < n$)
    \end{enumerate}
\end{intuition}

\subsection{Overdetermined linear equation}
\begin{definition}
    Solve 
    \begin{equation*}
        \min_x \|Ax - b\|_2^2
    \end{equation*}

    The solution is  $x^* = (A^\top A)^{-1} A^\top b = A^\dagger b$.
\end{definition}
\begin{derivation} \textbf{Projection Method:}
    \begin{enumerate}
        \item \textbf{Setup:} \[
    \begin{bmatrix}
    A_{m \times n}
    \end{bmatrix}
    \begin{bmatrix}
    x_{n \times 1}
    \end{bmatrix}
    =
    \begin{bmatrix}
    b_{m \times 1}
    \end{bmatrix}
    \]

    \customFigure[0.5]{00_Images/LS3.png}{Tall matrix.}

    This represents a case where there are more equations than unknowns. \\

    \item \textbf{Note:} Assume $A$ has full column rank (columns of $A$ are linearly independent). This implies:
    \[
    \mathcal{N}(A) = \{x \mid A x = 0 \} = \{x \mid x_1 a^{(1)} + \cdots + x_n a^{(n)} = 0 \} = \{0\},
    \]
    where $\mathcal{N}(A)$ is the null space of $A$. \\

    If $A$ does not have full column rank, we can remove some of the columns of $A$ to achieve linear independence. \\

    \item \textbf{Projection Problem:} In general, there are no solutions to $Ax=b$, instead
    \begin{equation*}
        \min_x \|Ax - b\|_2^2 
    \end{equation*}

    Recall the range of $A$, denoted as $\mathcal{R}(A)$, is defined as:
    \[
    \mathcal{R}(A) = \{Ax \mid x \in \mathbb{R}^n\}.
    \]

    Let $A x^* \in \mathcal{R}(A)$, where $x^*$ minimizes the norm:
    \[
    \min_x \|A x - b\|_2
    \]
    or equivalently:
    \[
    \min_{y \in \mathcal{R}(A)} \|y - b\|_2.
    \]
    \customFigure[0.5]{00_Images/LS.png}{Least squares projection problem}

    Let $x^* = \begin{bmatrix} x_1^* \\ \vdots \\ x_n^* \end{bmatrix}$, then:
    \[
    A x^* = \sum_{i=1}^n x_i^* a^{(i)},
    \]
    where $a^{(i)}$ are the columns of $A$. \\

    \item \textbf{Orthogonality Principle:} By the orthogonality principle:
    \[
    b - A x^* \perp a^{(j)} \quad \forall j.
    \]

    This implies:
    \[
    \langle b - \sum_{i=1}^n x_i^* a^{(i)}, a^{(j)} \rangle = 0 \quad \forall j.
    \]

    Expanding:
    \[
    \langle b, a^{(j)} \rangle = \sum_{i=1}^n x_i^* \langle a^{(i)}, a^{(j)} \rangle \quad \forall j.
    \]

    In matrix form, this can be written as:
    \[
    \begin{bmatrix}
    \langle a^{(1)}, a^{(1)} \rangle & \cdots & \langle a^{(1)}, a^{(n)} \rangle \\
    \vdots & \ddots & \vdots \\
    \langle a^{(n)}, a^{(1)} \rangle & \cdots & \langle a^{(n)}, a^{(n)} \rangle
    \end{bmatrix}
    \begin{bmatrix}
    x_1^* \\
    \vdots \\
    x_n^*
    \end{bmatrix}
    =
    \begin{bmatrix}
    \langle a^{(1)}, b \rangle \\
    \vdots \\
    \langle a^{(n)}, b \rangle
    \end{bmatrix} \implies A^\top A x^* = A^\top b.
    \]

    Therefore, the solution to the overdetermined least squares problem is $x^* = (A^T A)^{-1} A^T b$

    \item \textbf{Full Column Rank:} If $A$ has full column rank, then $A^T A$ is invertible

    \[
    \mathcal{N}(A) = \mathcal{N}(A^\top A) = \{0\}.
    \]

    This implies that $\dim (\mathcal{R} (A^T A)) = n$, which is the number of non-zero eigenvalues. \\

    The inverse can be expressed as:
    \[
    (A^\top A)^{-1} = (U \Lambda U^\top)^{-1},
    \]
    where $U$ and $\Lambda$ are components of the spectral decomposition, and the rank $r = n$, which means that all the components in the SVD are invertible.
    \end{enumerate} 
    \vspace{1em}
\end{derivation}

\begin{derivation}
    \textbf{Analytic Method to find the solution (Alterantive Way to Find the Solution):}
    \begin{enumerate}
        \item  The objective is to minimize the function:
        \[
        \min_x \|A x - b\|_2^2,
        \]
        denoted as $f(x)$. \\
        
        \item Define:
        \[
        f(x) = \|A x - b\|_2^2 = (A x - b)^\top (A x - b).
        \]
        
        \item Expanding the terms:
        \[
        f(x) = x^\top A^\top A x - 2 b^\top A x + b^\top b.
        \]
        
        \item The gradient of $f(x)$ is:
        \[
        \nabla f(x) = 2 A^\top A x - 2 A^\top b.
        \]
        
        \item Setting $\nabla f(x) = 0$:
        \[
        A^\top A x = A^\top b.
        \]
        
        \item Solving for $x^*$:
        \[
        x^* = (A^\top A)^{-1} A^\top b.
        \]
    \end{enumerate}
\end{derivation}

\subsubsection{Why the solution has a pseudo inverse form?} Show $A^\dagger = (A^\top A)^{-1} A^\top$ 
\begin{intuition}
    \begin{enumerate}
        \item \textbf{Proving Equality} Given the singular value decomposition (SVD):
        \[
        A = U \Sigma V^\top, \quad \text{where} \quad \Sigma = 
        \begin{bmatrix}
        \tilde{\Sigma}_{n\times n} \\
        0_{(m-n) \times n}
        \end{bmatrix}_{m \times n}.
        \]
        
        \item \textbf{LS:} The pseudo-inverse of \(A\) is:
        \[
        A^\dagger = V \Sigma^{-1} U^\top.
        \]

        Expanding \( \Sigma^{-1} \):
        \[
        A^\dagger = V 
        \begin{bmatrix}
        \tilde{\Sigma}^{-1} & 0
        \end{bmatrix}
        U^\top,
        \]
        where:
        \[
        \tilde{\Sigma}^{-1} =
        \begin{bmatrix}
        \frac{1}{\sigma_1} & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & \frac{1}{\sigma_n}
        \end{bmatrix}.
        \]
        
        \item \textbf{RS:} For \( (A^\top A)^{-1} A^\top \), then
        \begin{align*}
            (A^\top A)^{-1} A^\top 
            &= \left(V \Sigma^\top U^\top U \Sigma V^\top \right)^{-1} V \Sigma^\top U^\top \\
            &= \left(V \Sigma^\top \Sigma V^\top \right)^{-1} V \Sigma^\top U^\top \\
            &= \left(V \tilde{\Sigma}^2 V^\top \right)^{-1} V \Sigma^\top U^\top \quad  \text{since } \Sigma^T \Sigma = 
            \begin{bmatrix}
            \tilde{\Sigma} & 0
            \end{bmatrix}
            \begin{bmatrix}
            \tilde{\Sigma} \\
            0
            \end{bmatrix}
            =
            \tilde{\Sigma}^2.\\
            &= V \tilde{\Sigma}^{-2} V^\top V 
            \begin{bmatrix}
            \tilde{\Sigma} & 0
            \end{bmatrix}
            U^\top \\
            &= V \tilde{\Sigma}^{-2} 
            \begin{bmatrix}
            \tilde{\Sigma} & 0
            \end{bmatrix}
            U^\top \\
            &= V 
            \begin{bmatrix}
            \tilde{\Sigma}^{-1} & 0
            \end{bmatrix}
            U^\top \\
            &= V \Sigma^{-1} U^\top = A^\dagger,
        \end{align*}
    \end{enumerate}

\end{intuition}

\subsubsection{Pseudo Inverse}
\begin{intuition}
    \begin{enumerate}
        \item \textbf{Key 1:} The pseudo-inverse \(A^\dagger\) is the "best" we can do for inverting \(A\) in \(A x = b\) (when \(A\) is a tall matrix). In fact,
        \[
        A^\dagger A = I \quad \text{(but } A A^\dagger \neq I\text{)},
        \]
        
        \item \textbf{Key 2:} Moreover, $AA^\dagger$ is the projection matrix onto $\mathcal{R}(A)$
        \[
        A A^\dagger = \text{projection matrix onto } \mathcal{R}(A).
        \]
        \begin{itemize}
            \item Since the projection is $Ax^*$, therefore, for a certain $b$ and you want to project onto the range space of $A$, then the projection of \(b\) onto \(\mathcal{R}(A)\) is:
            \[
            A x^* = A A^\dagger b.
            \]
            \item The solution \(x^*\) to the least squares problem is:
            \[
            x^* = (A^\top A)^{-1} A^\top b = A^\dagger b.
            \]
        \end{itemize}

        \customFigure[0.75]{00_Images/LS1.png}{Projection matrix onto $\mathcal{R}(A)$}
    \end{enumerate}
\end{intuition}

\subsubsection{Application: Linear Regression}
\begin{example}
    \begin{enumerate}
        \item Given data points \( \{(x_i, y_i)\}_{i=1}^n \), we want to fit a straight line \( y_i = ax_i + b \) by minimizing the squared error:

        \[
        \min_{(a, b)} \sum_{i=1}^n (y_i - ax_i - b)^2
        \]

        \customFigure[0.5]{00_Images/LS2.png}{Linear regression}
    
        \item Rewriting in matrix form (i.e. reformulating to a least squares problem):
        \[
        \min_{a, b} \left\| 
        \begin{bmatrix}
        y_1 \\ 
        \vdots \\ 
        y_n
        \end{bmatrix}
        - 
        \begin{bmatrix}
        x_1 & 1 \\ 
        \vdots & \vdots \\ 
        x_n & 1
        \end{bmatrix}
        \begin{bmatrix}
        a \\ 
        b
        \end{bmatrix}
        \right\|_2^2,
        \]
        where:
        \[
        y = \begin{bmatrix}
        y_1 \\ 
        \vdots \\ 
        y_n
        \end{bmatrix}, \quad
        A = \begin{bmatrix}
        x_1 & 1 \\ 
        \vdots & \vdots \\ 
        x_n & 1
        \end{bmatrix}, \quad
        x = \begin{bmatrix}
        a \\ 
        b
        \end{bmatrix}.
        \]
    
        \item Solution:
        \[
        \begin{bmatrix}
        a^* \\ 
        b^*
        \end{bmatrix}
        = (A^\top A)^{-1} A^\top y,
        \]
        assuming \( A \) has full column rank and $n > 2$. 
    
        \item \textbf{Gradient Derivation (Alternative way to solve the problem)}
    
        We minimize:
        \[
        \min_{a, b} \sum_{i=1}^n (y_i - ax_i - b)^2 = f(a, b).
        \]
    
        Compute the partial derivatives:
        \[
        \frac{\partial f}{\partial a} = \sum_{i=1}^n 2(y_i - ax_i - b)(-x_i) = 0,
        \]
        \[
        \frac{\partial f}{\partial b} = \sum_{i=1}^n 2(y_i - ax_i - b)(-1) = 0.
        \]
    
        This yields two equations and two unknowns, which can be solved for \( a \) and \( b \).
        \begin{itemize}
            \item \textbf{HW:} Check this YOURSELF.
        \end{itemize}
    \end{enumerate}

\end{example}

\begin{warning}
    EXAM QUESTION EXAM QUESTION!! He will do a linear regression problem on the exam. So verify the gradient derivation. 
\end{warning}

\subsubsection{Application: Polynomial Regression}
\begin{example}
    \begin{enumerate}
        \item \textbf{Data:} Given data points \( \{(x_i, y_i)\}_{i=1}^n \)
        \item \textbf{Model Representation} For an $m$-order polynomial:
        \[
        y_i^* = \alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + \cdots + \alpha_m x_i^m
        \]
    
        \item \textbf{Objective Function} The goal is to minimize the squared error:
        \[
        \min \sum_{i=1}^n \left( y_i - y_i^* \right)^2
        \]
    
        Substituting the polynomial:
        \[
        \min_{\alpha_0, \dots, \alpha_m} \sum_{i=1}^n \left( y_i - \alpha_0 - \alpha_1 x_i - \alpha_2 x_i^2 - \cdots - \alpha_m x_i^m \right)^2
        \]
    
        \item \textbf{Matrix Formulation} Expressing in matrix form:
        \[
        \min_{\boldsymbol{\alpha}} 
        \left\| 
        \begin{bmatrix} 
        y_1 \\ 
        y_2 \\ 
        \vdots \\ 
        y_n 
        \end{bmatrix} 
        - 
        \begin{bmatrix} 
        1 & x_1 & x_1^2 & \cdots & x_1^m \\ 
        1 & x_2 & x_2^2 & \cdots & x_2^m \\ 
        \vdots & \vdots & \vdots & \ddots & \vdots \\ 
        1 & x_n & x_n^2 & \cdots & x_n^m 
        \end{bmatrix} 
        \begin{bmatrix} 
        \alpha_0 \\ 
        \alpha_1 \\ 
        \vdots \\ 
        \alpha_m 
        \end{bmatrix} 
        \right\|_2^2
        \]
    
        Let:
        \[
        \mathbf{y} = 
        \begin{bmatrix} 
        y_1 \\ 
        y_2 \\ 
        \vdots \\ 
        y_n 
        \end{bmatrix}, \quad 
        \mathbf{X} = 
        \begin{bmatrix} 
        1 & x_1 & x_1^2 & \cdots & x_1^m \\ 
        1 & x_2 & x_2^2 & \cdots & x_2^m \\ 
        \vdots & \vdots & \vdots & \ddots & \vdots \\ 
        1 & x_n & x_n^2 & \cdots & x_n^m 
        \end{bmatrix}, \quad 
        \boldsymbol{\alpha} = 
        \begin{bmatrix} 
        \alpha_0 \\ 
        \alpha_1 \\ 
        \vdots \\ 
        \alpha_m 
        \end{bmatrix}
        \]
    
        Then the objective becomes:
        \[
        \min_{\boldsymbol{\alpha}} \| \mathbf{y} - \mathbf{X} \boldsymbol{\alpha} \|_2^2
        \]
    
        \item \textbf{Closed-Form Solution:} If $\mathbf{X}$ is full column rank and $n > m+1$, the least squares solution is given by:
        \[
        \boldsymbol{\alpha} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
        \]    
    \end{enumerate}
\end{example}

\subsubsection{Regularized LS:}
\begin{example}
    \begin{enumerate}
        \item Consider the overdetermined case:
        \[
        \min_x \|Ax - b\|_2^2.
        \]
        
        \item In certain cases, we prefer a "smaller" \(x\). To achieve this, we add a regularization term:
        \[
        \min_x \|Ax - b\|_2^2 + \gamma \|x\|_2^2,
        \]
        where \(\gamma > 0\) controls the strength of regularization.
        \begin{itemize}
            \item $\gamma$ helps tune the trade off between $Ax-b$ and $x$.
        \end{itemize}
        
        \item Rewriting:
        \begin{align*}
        \min_x \|Ax - b\|_2^2 + \gamma \|x\|_2^2 
        &= \min_x \left\|
            \begin{bmatrix}
            A x - b\\ 
            \sqrt{\gamma} x
            \end{bmatrix}
            \right\|_2^2, \\
        &= \min_x \left\|
        \begin{bmatrix}
        A \\ 
        \sqrt{\gamma} I
        \end{bmatrix}
        x - 
        \begin{bmatrix}
        b \\ 
        0
        \end{bmatrix}
        \right\|_2^2, \\
        &= \min_x \|A'x - b'\|_2^2,
        \end{align*}
        where:
        \[
        A' = 
        \begin{bmatrix}
        A \\ 
        \sqrt{\gamma} I
        \end{bmatrix},
        \quad 
        b' = 
        \begin{bmatrix}
        b \\ 
        0
        \end{bmatrix}
        \]
        
        \item The solution is:
        \begin{align*}
        x^* 
        &= (A'^\top A')^{-1} A'^\top b', \\
        &= \left(
        \begin{bmatrix}
        A^\top & \sqrt{\gamma} I
        \end{bmatrix}
        \begin{bmatrix}
        A \\ 
        \sqrt{\gamma} I
        \end{bmatrix}
        \right)^{-1}
        \begin{bmatrix}
        A^\top & \sqrt{\gamma} I
        \end{bmatrix}
        \begin{bmatrix}
        b \\ 
        0
        \end{bmatrix}, \\
        &= \left(A^\top A + \gamma I\right)^{-1} A^\top b.
        \end{align*}
        
        This is known as \textbf{ridge regression}.
    \end{enumerate}
\end{example}

\begin{example}
    \textbf{Regularized LS:} Analytical Method
    We aim to solve the regularized least squares problem:
    \[
    \min_{\mathbf{x}} \| A \mathbf{x} - \mathbf{b} \|_2^2 + \gamma \| \mathbf{x} \|_2^2,
    \]
    where \( A \) is an \( m \times n \) matrix, \(\mathbf{b}\) is an \( m \times 1 \) vector, \(\gamma > 0\) is the regularization parameter, and \(\mathbf{x}\) is the \( n \times 1 \) vector of variables.

    \begin{enumerate}
        \item \textbf{Define the objective function:} The cost function can be written as:
        \[
        f(\mathbf{x}) = \| A \mathbf{x} - \mathbf{b} \|_2^2 + \gamma \| \mathbf{x} \|_2^2.
        \]
        Expanding the terms:
        \[
        f(\mathbf{x}) = (A \mathbf{x} - \mathbf{b})^T (A \mathbf{x} - \mathbf{b}) + \gamma \mathbf{x}^T \mathbf{x}.
        \]
        Expanding further:
        \[
        f(\mathbf{x}) = \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T \mathbf{b} + \gamma \mathbf{x}^T \mathbf{x}.
        \]
        Simplify:
        \[
        f(\mathbf{x}) = \mathbf{x}^T (A^T A + \gamma I) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T \mathbf{b},
        \]
        where \( I \) is the identity matrix.

        \item \textbf{Take the gradient of \( f(\mathbf{x}) \) with respect to \( \mathbf{x} \):}
        The gradient of \( f(\mathbf{x}) \) is:
        \[
        \nabla f(\mathbf{x}) = 2 (A^T A + \gamma I) \mathbf{x} - 2 A^T \mathbf{b}.
        \]

        \item \textbf{Set the gradient to zero:}
        To find the minimizer, set \(\nabla f(\mathbf{x}) = 0\):
        \[
        2 (A^T A + \gamma I) \mathbf{x} - 2 A^T \mathbf{b} = 0.
        \]
        Simplify:
        \[
        (A^T A + \gamma I) \mathbf{x} = A^T \mathbf{b}.
        \]

        \item \textbf{Solve for \(\mathbf{x}\):}
        Assuming \( A^T A + \gamma I \) is invertible, the solution is:
        \[
        \mathbf{x} = (A^T A + \gamma I)^{-1} A^T \mathbf{b}.
        \]
    \end{enumerate}
\end{example}


\subsection{Underdetermined linear equation}
\begin{definition}
    Solve 
    \begin{equation*}
        \min_{x, \text{ s.t. } Ax=b} \|x\|_2^2
    \end{equation*}

    The solution is  $x^* = A^\top (A A^\top)^{-1} b$.
\end{definition}

\begin{derivation}
    \begin{enumerate}
        \item Consider a matrix \( A \in \mathbb{R}^{m \times n} \) with \( m < n \):
        \customFigure[0.5]{00_Images/LS4.png}{Fat matrix.}
        
        \begin{itemize}
            \item Assume \( A \) has full row rank, i.e., \(\text{rank}(A) = m\). 
        \end{itemize}
        \item There are many vectors \( x \) that satisfy \( A x = b \) (i.e. infinitely many solutions). Out of all these \( x \)'s, we aim to find the one with the least norm (i.e. the best solution):
        \[
        \min_{x \text{ s.t. } A x = b} \|x\|_2^2
        \]
        \begin{itemize}
            \item \textbf{Note:} More unknowns than equations, so some unknowns can be arbitrary, which can lead to infinitely many solutions. This is because of the full row rank assumption (i.e. rows are linearly independent). If \( A \) is not full row rank, then there are no solutions, but it can still be solved (HW5) IS THIS CORRECT?
        \end{itemize}
        
        \item \textbf{Note} If \( A \bar{x} = b \) where $\bar{x}$ is one solution to $Ax=b$, then for any \( x_0 \in \mathcal{N}(A) \) (null space of \( A \)):
        \[
        A (\bar{x} + x_0) = A \bar{x} + A x_0 = A \bar{x} = b.
        \]
        
        This means we can write any solution \( x \) as:
        \[
        x = \bar{x} + x_0, \quad \text{where } A x_0 = 0.
        \]
        Since $x_0$ is part of the null space. 
        
        \item \textbf{Projection:} The projection of a vector \(y\) onto a set \(S\) is defined as:
        \[
        \text{Proj}_S(y) = \arg \min_{z \in S} \|z - y\|_2.
        \]

        The projection of the zero vector onto \(S\) is:
        \[
        x^* = \text{Proj}_S(0) = \arg \min_{z \in S} \|z\|_2.
        \]
        \customFigure[0.5]{00_Images/LS5.png}{Projection}

        \item \textbf{Orthogonality Principle} By the orthogonality principle, the least-norm solution \(x^*\) should belong to the span of the rows of \(A\), i.e.:
        \[
        x^* \in \mathcal{R}(A^\top),
        \]
        where:
        \[
        \mathcal{R}(A^\top) = \mathcal{N}(A)^\perp,
        \]
        the orthogonal complement of the null space of \(A\).
        \begin{itemize}
            \item This makes sense because you want $x^*$ to be orthogonal to the null space of $A$ as shown in the diagram above. 
        \end{itemize}

        \item \textbf{Solution to the Underdetermined System}

        We express the least-norm solution \(x^*\) as:
        \[
        x^* = A^\top c \quad \text{for some } c.
        \]

        To satisfy \(A x^* = b\), we substitute:
        \[
        A A^\top c = b.
        \]

        Since \(A A^\top\) is invertible (because \(A\) has full row rank), we solve for \(c\):
        \[
        c = (A A^\top)^{-1} b.
        \]

        Substituting \(c\) back, we find:
        \[
        x^* = A^\top c = A^\top (A A^\top)^{-1} b.
        \]

        Thus, the solution to the underdetermined system is:
        \[
        x^* = A^\top (A A^\top)^{-1} b.
        \]

        \item \textbf{Note:} The expression \(A^\top (A A^\top)^{-1} = A^\dagger \) is the pseudo-inverse of \(A\), denoted as \(A^\dagger\). That is:
        \[
        A^\dagger = A^\top (A A^\top)^{-1}.
        \]

        Using the singular value decomposition (SVD), where \(A = U \Sigma V^\top\) with
        $
        \Sigma = 
        \begin{bmatrix}
        \tilde{\Sigma}_{m \times m} & 0
        \end{bmatrix},
        $
        we can show that:
        \[
        A^\top (A A^\top)^{-1} = A^\dagger.
        \]
        \begin{itemize}
            \item \textbf{Note:} This is similar to how we derived the pseudo-inverse for the overdetermined case.
            \item $\tilde{\Sigma}$ is $m \times m$ because it is full row rank. 
        \end{itemize}        
    \end{enumerate}
\end{derivation}

\begin{derivation}
    \textbf{Analytic Method to Solve Underdetermined Least Squares}

    \begin{enumerate}
        \item We aim to solve:
        \[
        \min \|x\|_2^2 \quad \text{subject to } A x = b,
        \]
        where \(\|x\|_2^2\) is the objective and \(A x = b\) represents the \(m\) constraints. This is a constrained optimization problem.
        
        \item \textbf{Lagrangian Formulation:} We form the Lagrangian function:
        \[
        \mathcal{L}(x, \lambda) = \|x\|_2^2 + \lambda^\top (A x - b),
        \]
        where \(\lambda\) can be interpreted as a "price" that penalizes the deviation of \(A x\) from \(b\).
        \begin{itemize}
            \item \textbf{Note:} The Lagrangian is the objective function plus the constraints multiplied by the Lagrange multipliers.
            \begin{itemize}
                \item $Ax-b$ is the constraint, which is multiplied by the Lagrange multiplier $\lambda$.
            \end{itemize}
        \end{itemize}
        
        \item \textbf{Solving the Problem}
        
        For fixed \(\lambda\), minimize \(\mathcal{L}(x, \lambda)\) over \(x\):
        \[
        \nabla_x \mathcal{L} = 2x + A^\top \lambda = 0 \quad \Rightarrow \quad x = -\frac{1}{2} A^\top \lambda.
        \]
        
        Substitute into the constraint \(A x = b\) to ensure that this $x$ satisfies the constraint:
        \[
        A \left(-\frac{1}{2} A^\top \lambda \right) = b \quad \Rightarrow \quad -\frac{1}{2} A A^\top \lambda = b.
        \]
        
        Solve for \(\lambda\):
        \[
        \lambda = -2 (A A^\top)^{-1} b.
        \]
        \begin{itemize}
            \item This is telling me the optimal penalty to minimize the Langrangian function.
        \end{itemize}
        
        Substitute back to find \(x\):
        \[
        x = -\frac{1}{2} A^\top \lambda = A^\top (A A^\top)^{-1} b.
        \]
        
        \item \textbf{Conclusion} The solution matches the least-norm solution derived earlier:
        \[
        x = A^\top (A A^\top)^{-1} b.
        \]
        \begin{itemize}
            \item \textbf{General:} Very difficult to find a closed form solution of $x$ in terms of $\lambda$, but in this case, we can find it. This is one of the few problems where we can find a closed form solution.
        \end{itemize}
    \end{enumerate}
\end{derivation}

\begin{warning}
    For constrained optimization, you cannot just set the gradient to zero because it's not gauranteed to satisfy the constraints. However, if you use the Lagrangian method, you are effectively solving an unconstrainted optimization problem, so that you can set the gradient to zero.
\end{warning}

\begin{process}
    \begin{enumerate}
        \item Form the lagrangian function
        \item Take the gradient of the lagrangian function with respect to $x$ and set it to zero
        \item Solve for $x$ in terms of $\lambda$
        \item Substitute $x$ back into the constraint to solve for $\lambda$
        \item Substitute $\lambda$ back into $x$ to solve for $x$
    \end{enumerate}
\end{process}

\subsubsection{Application: Optimal Control}
\begin{example}
    \begin{enumerate}
        \item \textbf{State of the Mass}

        Assume \( m = 1 \). The state of the mass is represented as:
        \[
        \begin{bmatrix}
        x[n] \\ 
        v[n]
        \end{bmatrix},
        \]
        where \(x[n]\) is the position and \(v[n]\) is the velocity.

        \customFigure[0.5]{00_Images/LS6.png}{Mass system}
        \begin{itemize}
            \item Pushing the mass with force \(p_n\) at time \(n\) from position $0$ to position $1$.
        \end{itemize}
        
        \item The equations of motion are:
        \[
        x(t) = x(0) + vt + \frac{1}{2} a t^2, \quad v(t) = v(0) + at.
        \]
        \begin{itemize}
            \item $p_n = ma = a$ since the mass is 1 (i.e. applied force is equal to the acceleration)
        \end{itemize}

        \item \textbf{Goal:} We want to find the optimal force \(p_n\) (i.e. least amount of force) to push the mass from position \(0\) to position \(1\).
        
        \item Discretizing, we have:
        \[
        x[n] = x[n-1] + v[n-1] + \frac{1}{2} p_n,
        \]
        \[
        v[n] = v[n-1] + p_n.
        \]
        Therefore, in matrix form, we can write it as:
        \[
        \begin{bmatrix}
        x[n] \\ 
        v[n]
        \end{bmatrix}
        =
        \begin{bmatrix}
        1 & 1 \\ 
        0 & 1
        \end{bmatrix}
        \begin{bmatrix}
        x[n-1] \\ 
        v[n-1]
        \end{bmatrix}
        +
        \begin{bmatrix}
        \frac{1}{2} \\ 
        1
        \end{bmatrix} p_n,
        \]
        
        Define:
        \[
        A = 
        \begin{bmatrix}
        1 & 1 \\ 
        0 & 1
        \end{bmatrix}, \quad 
        B = 
        \begin{bmatrix}
        \frac{1}{2} \\ 
        1
        \end{bmatrix}.
        \]
        
        The system evolves as:
        \[
        x_n = A x_{n-1} + B p_n.
        \]
        
        \item \textbf{Iterative Evolution} Suppose \(x_0 = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\) and the final state is \(\begin{bmatrix} 1\\ 0 \end{bmatrix}\) after 10 time steps. Then:
        \[
        \begin{bmatrix}
        x[10] \\ 
        v[10]
        \end{bmatrix}
        =
        A \begin{bmatrix}
        x[9] \\ 
        v[9]
        \end{bmatrix}
        + B p_{10}.
        \]
        
        Expanding recursively:
        \[
        \begin{bmatrix}
        x[10] \\ 
        v[10]
        \end{bmatrix}
        =
        A \left(A \begin{bmatrix}
        x[8] \\ 
        v[8]
        \end{bmatrix}
        + B p_9 \right) + B p_{10}.
        \]
        
        Simplify to:
        \[
        \begin{bmatrix}
        x[10] \\ 
        v[10]
        \end{bmatrix}
        =
        \begin{bmatrix}
        A^{10} B & A^9 B & A^8 B & \cdots & AB & B
        \end{bmatrix}
        \begin{bmatrix}
        p_0 \\ 
        \vdots \\ 
        p_{10}
        \end{bmatrix}.
        \]
        
        Let:
        \[
        C = \begin{bmatrix}
        A^{10} B & A^9 B & A^8 B & \cdots & AB & B
        \end{bmatrix}_{2 \times 11}, \quad 
        p = \begin{bmatrix}
        p_0 \\ 
        \vdots \\ 
        p_{10}
        \end{bmatrix}_{11 \times 1}.
        \]
        
        \item \textbf{Optimization Problem:} We want to solve:
        \[
        \min \|p\|_2^2 \quad \text{subject to } Cp = 
        \begin{bmatrix}
        1 \\ 
        0
        \end{bmatrix}.
        \]
        \begin{itemize}
            \item $Cp = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ because we want to reach the final state of position $1$ with velocity $0$.
            \item Minimize the force applied to the mass subject to the constraint of reaching the final state.
        \end{itemize}
        
        \item This is an underdetermined least squares problem, with the solution:
        \[
        p^* = C^\top (C C^\top)^{-1} 
        \begin{bmatrix}
        1 \\ 
        0
        \end{bmatrix}.
        \]
    \end{enumerate}
\end{example}
