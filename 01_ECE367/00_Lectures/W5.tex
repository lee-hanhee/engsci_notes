\subsection{Symmetric matrices}
\begin{definition}
    Set of real symmetric matrices is 
    \[
    S^n = \left\{ A \in \mathbb{R}^{n \times n} \mid A = A^T \right\}
    \]
    \begin{itemize}
        \item i.e. a matrix $A$ is symmetric if $a_{ij} = a_{ji} \; \forall i,j$
    \end{itemize}
    \vspace{1em}

    Set of complex symmetric matrices is
    \[
    S^n = \left\{ A \in \mathbb{C}^{n \times n} \mid A = A^H \right\}
    \]
    \begin{itemize}
        \item $A^H$ represents the Hermitian transpose (conjugate transpose) of $A$.
    \end{itemize}
    
\end{definition}

\subsubsection{Example of symmetric matrix: Hessian Matrix}
\begin{example}
    Recall: The Hessian matrix \(\nabla^2 f\) of a function \( f: \mathbb{R}^n \to \mathbb{R} \) with continuous partial derivatives (i.e. must be continuous for symmetric matrix) is a symmetric matrix:
    \[
    [\nabla^2 f]_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i} = [\nabla^2 f]_{ji}.
    \]

    \textbf{Example}: Consider a quadratic function \( q: \mathbb{R}^n \to \mathbb{R} \):

    \begin{align*}
        q(x) &= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j + \sum_{i=1}^n c_i x_i + d \\
    &= \frac{1}{2} 
    \begin{bmatrix}
    x_1 & \dots & x_n
    \end{bmatrix}
    \begin{bmatrix}
    a_{11} & \dots & a_{1n} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \dots & a_{nn}
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
    \end{bmatrix}
    + 
    \begin{bmatrix}
    c_1 & \dots & c_n
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
    \end{bmatrix}
    + d \\
    &= \frac{1}{2} x^\top A x + c^\top x + d.
    \end{align*}

    \vspace{1em}

    \textbf{Gradient of the Quadratic Function}

    Let 
    \[
    q(x) = \frac{1}{2} x^\top A x + c^\top x + d.
    \]
    Then,
    \begin{align*}
    \nabla(c^\top x) &= \begin{bmatrix}
    \frac{\partial}{\partial x_1} (c^\top x) \\
    \vdots \\
    \frac{\partial}{\partial x_n} (c^\top x)
    \end{bmatrix} 
    = \begin{bmatrix}
    c_1 \\
    \vdots \\
    c_n
    \end{bmatrix}
    = c.
    \end{align*}

    Now let's solve $\nabla\left(\frac{1}{2} x^\top A x\right)$, for the $kth$ component:
    \begin{align*}
     \frac{\partial}{\partial x_k} \left(\frac{1}{2} x^\top A x\right) &= \frac{\partial}{\partial x_k} \left( \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j \right) \\
    &= \frac{\partial}{\partial x_k} \left( \frac{1}{2} a_{kk} x_k^2 + \frac{1}{2} \sum_{i=1, i \neq k}^n a_{ik} x_i x_k + \frac{1}{2} \sum_{j=1, j \neq k}^n a_{kj} x_k x_j \right) \\
    & \text{1st term: $i=j=k$, 2nd term: $j=k$, 3rd term: $i=k$} \\
    &= a_{kk} x_k + \sum_{i=1,i \neq k}^n a_{ik} x_i \quad \text{since $a_{ij} = a_{ji}$}\\
    &= \sum_{i=1}^n a_{ik} x_i \quad \text{bringing the term in} \\ 
    &= \begin{bmatrix}
    a_{k1} & \dots & a_{kn}
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
    \end{bmatrix}.
    \end{align*}

    Therefore, generalizing this to all the components, then \[
        \nabla \left( \frac{1}{2} x^T A x \right) =
        \begin{bmatrix}
        a_{11} & \cdots & a_{1n} \\
        \vdots & \ddots & \vdots \\
        a_{n1} & \cdots & a_{nn}
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\
        \vdots \\
        x_n
        \end{bmatrix}
        = A x
        \]
        \vspace{1em}

    Thus,
    \begin{itemize}
        \item $\nabla\left(\frac{1}{2} x^\top A x\right) = A x$
        \item $\boxed{\nabla q(x) = A x + c}$ is the final gradient. 
    \end{itemize}
    \vspace{1em}

    \textbf{Hessian:}
    \begin{align*}
        \left[ \nabla^2 g(x) \right]_{ij} &= \frac{\partial}{\partial x_j} \left( \frac{\partial}{\partial x_i} g(x) \right) \\
        & = \frac{\partial}{\partial x_j} \left( \sum_{i=1}^{n} a_{ij} x_j \right) = a_{ji} = a_{ij} \\
        \end{align*}
    
    Therefore, generalizing this to the entire Hessian: 
    \begin{equation*}
        \nabla^2 g(x) = A 
    \end{equation*}
    \begin{itemize}
        \item \textbf{Note:} If $A$ is not symmetric, then the Hessian becomes $B = \frac{A + A^T}{2}$
    \end{itemize}
    \vspace{1em}

    \textbf{Taylor approximation:} The second-order Taylor approximation of \( g(x) \) around \( x_0 \):
        \begin{align*}
            q(x) &\approx q(x_0) + \nabla q(x_0)^T (x - x_0) + \frac{1}{2} (x - x_0)^T \nabla^2 q(x_0) (x - x_0) \\
            &= \left(\frac{1}{2} x_0^\top A x_0 + c^\top x_0 + d\right) + (A x_0 + c)^T (x - x_0) + \frac{1}{2} (x - x_0)^T A (x - x_0) \\
            &= \left(\frac{1}{2} x_0^\top A x_0 + c^\top x_0 + d\right) + (x_0^T A^T + c^T) (x-x_0) + \frac{1}{2} (x^T A x - x^T Ax_0 -x_0^T A x + x_0^T A x_0) \\
            &= \left(\frac{1}{2} x_0^\top A x_0 + c^\top x_0 + d\right) + x_0^T A x - x_0^T A x_0 + c^T x - c^T x_0 + \frac{1}{2} (x^T A x -2 x_0^T A x + x_0^T A x_0) \\
            &= \left(\frac{1}{2} x_0^\top A x_0 + c^\top x_0 + d\right) + x_0^T A x - x_0^T A x_0 + c^T x - c^T x_0 + \frac{1}{2}x^T A x - x_0^T A x + \frac{1}{2} x_0^T A x_0 \\
            &= x_0^\top A x_0 + c^\top x_0 + d + x_0^T A x - x_0^T A x_0 + c^T x - c^T x_0 + \frac{1}{2}x^T A x - x_0^T A x \\
            &= c^\top x + d + \frac{1}{2}x^T A x 
            &= q(x)
        \end{align*}
        \begin{itemize}
            \item This is independent of $x_0$ (check why this holds).
        \end{itemize}

\end{example}

\begin{warning}
    If \(A\) is not symmetric, we can replace \(A\) by \( \frac{A + A^\top}{2} \) without changing the evaluation of the function.

    \begin{itemize}
        \item e.g. \(4  x_1 x_2 + 5 x_2 x_1 = 9 x_1 x_ 2\). This is non-symmetric since $a_{12} \neq a_{21}$, but we can symmetrize this by taking the average of the two points 
        \begin{itemize}
            \item $4.5 x_1 x_2 + 4.5 x_2 x_1 = 9 x_1 x_2$. This symmetry was found as $\frac{a_{12} + a_{21}}{2}$
        \end{itemize}
    \end{itemize}
    \vspace{1em}

    Therefore, we can always assume that \(A\) is symmetric 
    \begin{itemize}
        \item Since we can always change a non-symmetric matrix $A$ into a symmetric matrix \( \frac{A + A^\top}{2} \) since \( \left(\frac{A + A^\top}{2}\right)^T =\frac{A + A^\top}{2} \)
    \end{itemize}
    \vspace{1em}

    Therefore, in general, always replace with \( \frac{A + A^\top}{2} \) because if it's symmetric already, then it becomes $A$. If not it becomes $B=\frac{A + A^\top}{2}$, which is a symmetric matrix by construction.
\end{warning}

\subsubsection{Theorem}

\begin{theorem}
    Let $A \in S^n$ be a real symmetric matrix. Let $\lambda_1, \dots, \lambda_n$ be its eigenvalues (not necessarily distinct). Then:
    \begin{enumerate}
        \item $\lambda_1, \dots, \lambda_n$ are real, and $\text{AM}(\lambda_i) = \text{GM}(\lambda_i)$ for all $i$.
        \item Eigenvectors corresponding to different eigenvalues are orthogonal.
    \end{enumerate}
\end{theorem}

\begin{warning}
    The implication from the theorem
    \begin{enumerate}
        \item This means all symmetric matrices are diagonalizable. 
        \item $U^T U = I$ (i.e. $U^{-1} = U^T$)
    \end{enumerate}
\end{warning}

\begin{derivation}
        \textbf{1)} Let $\lambda$ be an eigenvalue with eigenvector $u$.
        \[
        A u = \lambda u \tag{1}
        \]
        
        Let's consider the conjugate transpose:
        \[
        (A u)^H = (\lambda u)^H
        \]
        \[
        \implies u^H A^H = \bar{\lambda} u^H \tag{2}
        \]
        \vspace{1em}

        Now set up two equivalent equations starting with $u^H A u$:
        
        \begin{itemize}
            \item
            \[
            u^H A u = \lambda u^H u \quad \text{Multiply equation (1) by $u^H$}
            \]
            \item 
            \begin{align*}
                u^H A u &= u^H A^H u \quad \text{since A is symmetric } A^H = A\\
                &= \bar{\lambda} u^H u \quad \text{Multiply equation (2) by $u$}
            \end{align*}
            \begin{itemize}
                \item A is real and symmetric and $A = A^H$ since the Hermitian takes the transpose and the conjugage. 
                \begin{itemize}
                    \item $A^H = A^T$ since the conjugate of a real matrix is the real matrix 
                    \item $A=A^T$ because it's symmetric. 
                    \item Therefore, $A^H = A$
                \end{itemize}
            \end{itemize}
        \end{itemize}
        \vspace{1em}

        Thus, we have:
        \[
        (\lambda - \bar{\lambda}) u^H u = 0
        \]
        So, $\lambda = \bar{\lambda}$, meaning that $\lambda$ is real.
        \vspace{1em}

        For the proof of $\text{AM}(\lambda) = \text{GM}(\lambda)$, check Theorem 4.1 in the OptM book.
\end{derivation}

\begin{derivation}    
    \textbf{2)} Let $\lambda_i$ and $\lambda_j$ be two different eigenvalues, and let $v^{(i)}$ and $v^{(j)}$ be the corresponding eigenvectors. 
    \[
    A v^{(i)} = \lambda_i v^{(i)} \tag{1}
    \]
    \[
    A v^{(j)} = \lambda_j v^{(j)} \tag{2}
    \]
    
    Now set up two equivalent equations starting with $(v^{(j)})^T A v^{(i)}$:
    \begin{itemize}
        \item 
        \[
    (v^{(j)})^T A v^{(i)} = \lambda_i (v^{(j)})^T v^{(i)} \quad \text{Multiply (1) by $(v^{(j)})^T$}
    \]
        \item 
        \begin{align*}
            (v^{(j)})^T A v^{(i)} &= (v^{(j)})^T A^T v^{(i)} \quad \text{Since $A = A^T$} \\ 
            &= (Av^{(j)})^T v^{(i)} \quad \text{$(AB)^T = B^T A^T$} \\
            &= \lambda_j (v^{(j)})^T v^{(i)} 
        \end{align*}
    \end{itemize}
    \vspace{1em}
    
    Thus, we have:
    \[
    (\lambda_i - \lambda_j) (v^{(j)})^T v^{(i)} = 0
    \]
    
    Since $\lambda_i \neq \lambda_j$, we must have:
    \[
    (v^{(j)})^T v^{(i)} = 0
    \]
    Therefore, $v^{(i)}$ and $v^{(j)}$ are orthogonal.
\end{derivation}

\begin{warning}
    For these two derivations: $u^H A u$ and $(v^{(j)})^T A v^{(i)}$ were used to use the properties of symmetric matrices. 
    \begin{itemize}
        \item $(u^H A u)^H = u^H A u$
    \end{itemize}
\end{warning}

\subsection{Spectral decomposition}
\subsubsection{Spectral theorem}
\begin{theorem}
    Let $A \in S^n$ be a real symmetric matrix. Let $\lambda_1, \dots, \lambda_n \in \mathbb{R}$ be its eigenvalues. 
    \vspace{1em}

    Then, there exists a set of real eigenvectors $u^{(1)}, \dots, u^{(n)}$, which are orthonormal (i.e., $u^{(i)} \perp u^{(j)}$ for $i \neq j$, and $\| u^{(i)} \| = 1$ for all $i$). 
    \vspace{1em}

    Equivalently, the eigendecomposition of $A$ becomes:
    \[
    A = U \Lambda U^T
    \]
    \begin{itemize}
        \item $\Lambda = \begin{bmatrix}
    \lambda_1 & 0 & \dots & 0 \\
    0 & \lambda_2 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \lambda_n
    \end{bmatrix}$
    \item $U = \begin{bmatrix}
    u^{(1)} & \dots & u^{(n)}
    \end{bmatrix}$
    \item \textbf{Note:} This is called the "spectral decomposition" of $A$. 
    \begin{itemize}
        \item Previously, we had $A = U \Lambda U^{-1}$, but for a symmetric matrix $U^{-1} = U^T$.
    \end{itemize}
    \end{itemize}
\end{theorem}

\begin{derivation}
    We already proved that $\lambda_1, \dots, \lambda_n$ are real. 
    \vspace{1em}

    Without loss of generality (WLOG), we can take the eigenvectors to be real. Why?

    \begin{enumerate}
        \item $A\mathbf{v} = \lambda \mathbf{v}$ implies
        \[
        \Re \{A \mathbf{v} \} = \Re \{\lambda \mathbf{v} \}
        \]
        \item Since $\lambda$ is real, this becomes
        \[
        A \Re \{\mathbf{v} \} = \lambda \Re \{\mathbf{v} \}
        \]
        \item Therefore, $\Re \{\mathbf{v} \}$ is an eigenvector with eigenvalue $\lambda$.
    \end{enumerate}
    \vspace{1em}
    
    Now, let's prove that the set of real eigenvectors are orthonormal:

    \begin{enumerate}
        \item Now, since $\text{AM}(\lambda_i) = \text{GM}(\lambda_i) = m_i$ (since diagonalizable), we can take $m_i$ eigenvectors $u^{(i,1)}, \dots, u^{(i,m_i)}$ which form an orthonormal basis of $\mathcal{N}(A - \lambda_i I)$ 
        \begin{itemize}
            \item i.e. by construction, we can form an orthonormal basis for this space by using Gram-Schmidt for example
            \item $\lambda_1$: $u^{(1)},\ldots, u^{(1,m_1)}$ (these vectors can be chosen to be orthogonal within this space.)
            \item $\lambda_2$: $u^{(2)},\ldots, u^{(2,m_2)}$ (these vectors can be chosen to be orthogonal within this space.)
        \end{itemize}
        \item Previously, we showed that across different $\mathcal{N}(A - \lambda_i I)$'s, the $u^{(i)}$ are orthogonal. 
        \item Therefore, $\{u^{(1,1)}, \dots, u^{(1,m_1)}, u^{(2,1)}, \dots, u^{(2,m_2)}, \dots\}$ forms an orthonormal basis of $n$ eigenvectors
    \end{enumerate}
    \vspace{1em}

    Now we can assemble everything together: 
    \begin{enumerate}
        \item So, 
        \[
        A u^{(i,j)} = \lambda_i u^{(i,j)}
        \]
        \item Therefore, if we let $U = \begin{bmatrix} u^{(1,1)} & \cdots & u^{(n,m_n)} \end{bmatrix}$. Then,
        \[
        A U = U \Lambda
        \]
        \begin{itemize}
            \item $\Lambda = \begin{bmatrix} \lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & 0 \\ 0 & 0 & \cdots & \lambda_n \end{bmatrix}$.
        \end{itemize}
        \item Since the columns of $U$ form an orthonormal basis, we have
        \[
        U^\top U = \begin{bmatrix}
        (u^{(1)})^\top \\
        \vdots \\
        (u^{(n)})^\top
        \end{bmatrix}
        \begin{bmatrix}
        u^{(1)} & \cdots & u^{(n)}
        \end{bmatrix}
        = I
        \]
        \item Therefore, $U^{-1} = U^\top$, meaning $U$ is an orthogonal matrix.
        \item Multiply both sides of $A U = U \Lambda$ by $U^\top$:
        \[
        A = U \Lambda U^\top
        \]
    \end{enumerate}
\end{derivation}

\subsubsection{Difference between Eigendecomposition and Spectral Decomposition}
\begin{definition}
    Eigendecomposition of a diagonalizable matrix
    \[
    A = U \Lambda U^{-1}
    \]
    \begin{itemize}
        \item $U = \begin{bmatrix}
    u^{(1)} & \dots & u^{(n)}
    \end{bmatrix}
    \quad \text{eigenvectors form a basis for} \ \mathbb{R}^n$
    \item $\Lambda = \begin{bmatrix}
    \lambda_1 & 0 & \dots & 0 \\
    0 & \lambda_2 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & \lambda_n
    \end{bmatrix}$
    \end{itemize}
    \vspace{1em}

    Eigendecomposition of a symmetric matrix $A$
    \[
    A = U \Lambda U^T
    \]
    \begin{itemize}
        \item $U = \begin{bmatrix}
            u^{(1)} & \dots & u^{(n)}
            \end{bmatrix}
            \quad \text{eigenvectors form an orthonormal basis}$
    \end{itemize}  
\end{definition}

\subsection{Orthogonal matrices}
\begin{definition}
    U is an orthogonal matrix if
    \[
    U = \begin{bmatrix}
    u^{(1)} & \cdots & u^{(n)}
    \end{bmatrix},
    \]
    \begin{itemize}
        \item $
    (u^{(i)})^\top u^{(j)} = 
    \begin{cases} 
    0 & \text{if } i \neq j \\
    1 & \text{if } i = j
    \end{cases}$
    \item i.e. $U^{-1} = U^T$ or $UU^T = U^T U = I$
    \item \textbf{Note:} Columns of an orthogonal matrix are orthonormal.
    \end{itemize}
\end{definition}

\subsubsection{What happens when we multiply a vector by an orthogonal matrix?}
\begin{derivation}
\[
y = Ux
\]

\textbf{Answer}: $Ux$ is a rotation of $x$, plus a possible flip.
\vspace{1em}

\textbf{Why?} Let's start with proving two facts:
\begin{itemize}
    \item \textbf{Why? Fact 1}: If $U$ is orthogonal, then 
    \[
    \| Ux \|_2 = \| x \|_2.
    \]
    \begin{itemize}
        \item \textbf{Proof}: 
        \begin{align*}
        \| Ux \|_2^2 &= (Ux)^\top (Ux) \\
        &= x^\top U^\top U x \\
        &= x^\top I x \\
        &= \| x \|_2^2.
        \end{align*}
    \end{itemize}
    
    \item \textbf{Fact 2}: Suppose $x$ and $y$ have an angle $\Theta$ between them. Then $Ux$ and $Uy$ have the same angle $\Theta$ between them.
    
    \begin{itemize}
        \item \textbf{Proof (1-2)}: By previous lecture, we showed that 
        \[
        \cos \Theta = \frac{\langle x, y \rangle}{\| x \|_2 \| y \|_2}
        = \frac{x^\top y}{\| x \|_2 \| y \|_2}.
        \]
        \item Now, prove 2-1,
        \begin{align*}
        \frac{\langle Ux, Uy \rangle}{\| Ux \|_2 \| Uy \|_2} 
        &= \frac{(Uy)^\top (Ux)}{\| Ux \|_2 \| Uy \|_2} \\
        &= \frac{y^\top U^\top U x}{\| x \|_2 \| y \|_2} \quad \text{by fact 1} \\ 
        &= \frac{y^\top x}{\| x \|_2 \| y \|_2} = \cos \Theta \quad \text{by $U^T U = I$}
        \end{align*}
    \end{itemize}
\end{itemize}
\vspace{1em}

Thus, the angle remains unchanged after applying $U$.

\customFigure[0.75]{00_Images/OM.png}{Orthogonal matrix performed on two vectors causes a rotation and possibly a flip.}
\customFigure[0.75]{00_Images/Flip.png}{Flip.}
\end{derivation}

\begin{example}
    Examples of flipping in orthogonal matrices.
    \[
    \begin{bmatrix}
    0 & 1 & 0 \\
    1 & 0 & 0 \\
    0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
    x_2 \\
    x_1 \\
    x_3
    \end{bmatrix}
    \]
    This is flipping across the \(x_1\), \(x_2\) hyperplane.

    \[
    \begin{bmatrix}
    0 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
    x_3 \\
    x_2 \\
    x_1
    \end{bmatrix}
    \]
    This is flipping across the \(x_3\), \(x_1\) hyperplane.
\end{example}

\subsubsection{What kind of transformation corresponds to multiplying by a symmetric matrix?}
\begin{derivation}
    \begin{align*}
        y = A x &= U \Lambda U^\top x \\
    U^\top y &= \Lambda U^\top x \\
    \tilde{y} &= \Lambda \tilde{x}
    \end{align*}
    \begin{itemize}
        \item $\tilde{y}$: $U^T y$
        \item $\tilde{x}$: $U^T x$
    \end{itemize}
    \vspace{1em}

    So, an interpretation of \( y = A x \) is:

    \begin{enumerate}
        \item Rotation (or flip) of $x$ to get \(\tilde{x}\), where \(\tilde{x} = U^\top x\).
        \item The \(i\)-th component of \(\tilde{x}\) is scaled by \(\lambda_i\), where \(\tilde{y} = \Lambda \tilde{x}\).
        \item Rotation (or flip) \(\tilde{y}\) back to \textbf{y}, where \(y = U \tilde{y}\).
    \end{enumerate}
    \vspace{1em}


    Now, let's look at \(\tilde{x} = U^\top x\):
    \begin{align*}
    \tilde{x} = U^\top x &= 
    \begin{bmatrix}
    (u^{(1)})^\top \\
    \vdots \\
    (u^{(n)})^\top
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
    \end{bmatrix} =
    \begin{bmatrix}
    \langle u^{(1)}, x \rangle \\
    \vdots \\
    \langle u^{(n)}, x \rangle
    \end{bmatrix}
    \end{align*}

    Recall: The projection of $x$ onto a subspace spanned by \(u^{(i)}\) is just $\langle u^{(i)}, x \rangle u^{(i)}$
    \begin{itemize}
        \item So, we can think of \( U^\top x \) as the coordinates of \textbf{x} in the orthonormal basis \(\{ u^{(1)}, \dots, u^{(n)} \}\).
    \end{itemize}

    \customFigure[0.5]{00_Images/P.png}{Projection}
\end{derivation}

\begin{warning}
    The interpretation is the same as the eigendecomposition for non-defective (but not necessarily symmetric matrices), but the change of basis is represented by rotations and flips since we have orthonormal matrices. 
\end{warning}

\subsection{Positive semidefinite matrices and positive definite matrices}
\begin{definition}
    A symmetric matrix \( A \in S^n \) is Positive Semi-Definite (PSD) if:
    \[
    x^T A x \geq 0 \quad \forall x \in \mathbb{R}^n
    \]
    A symmetric matrix \( A \in S^n \) is Positive Definite (PD) if:
    \[
    x^T A x > 0 \quad \forall x \neq 0
    \]

    \begin{itemize}
        \item \textbf{Denote:} The set of PSD matrices as \( S^n_+ \), and the set of PD matrices as \( S^n_{++} \). 
        \item \textbf{Notation} 
        \[
        A \succeq 0 \quad \Leftrightarrow \quad A \in S^n_+
        \]
        \[
        A \succ 0 \quad \Leftrightarrow \quad A \in S^n_{++}
        \]
    \end{itemize}
\end{definition}

\begin{warning}
    $x \neq 0$ because if $x=0$, then the product is $0$ and $A$ can be anything. 
\end{warning}

\subsubsection{Theorem on PSD,PD for Eigenvalues}
\begin{theorem}
    A symmetric matrix \( A \in S^n \) is:
    \begin{itemize}
        \item PSD if and only if \( \lambda_i(A) \geq 0 \quad \forall i \)
        \item PD if and only if \( \lambda_i(A) > 0 \quad \forall i \)
    \end{itemize}
\end{theorem}

\begin{derivation}
    We can write \( A = U \Lambda U^T \), where \( \Lambda \) contains the eigenvalues of \( A \) since it's symmetric. Then:
\[
x^T A x = x^T U \Lambda U^T x = \tilde{x}^T \Lambda \tilde{x}
\]
Where \( \tilde{x} = U^T x \). Expanding the right-hand side:
\begin{align*}
    \tilde{x}^T \Lambda \tilde{x} &= 
    \begin{bmatrix}
    \tilde{x}_1 & \cdots & \tilde{x}_n
    \end{bmatrix}
    \begin{bmatrix}
    \lambda_1 & 0 & \cdots & 0 \\
    0 & \lambda_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \lambda_n
    \end{bmatrix}
    \begin{bmatrix}
    \tilde{x}_1 \\
    \tilde{x}_2 \\
    \vdots \\
    \tilde{x}_n
    \end{bmatrix} \\
    &= \sum_{i=1}^{n} \lambda_i \tilde{x}_i^2
    \end{align*}
\begin{itemize}
    \item \textbf{Right to left:} If \( \lambda_i \geq 0 \) for all \( i \), then \( x^T A x \geq 0 \) since $\lambda_i \tilde{x}_i^2 > 0$ as $ \tilde{x}_i^2>0$, proving that \( A \) is PSD.
    \item \textbf{Left to right:} Conversely, if \( x^T A x \geq 0 \) for all \( x \), then we must have \( \lambda_i \geq 0 \) for all \( i \).
    \begin{itemize}
        \item \textbf{Contradiction:} Since otherwise, we could find an \( x \) such that \( x^T A x < 0 \), which contradicts the definition of PSD (i.e. $\tilde{x} = 1$ corresponding to the eigenvalue that is $\lambda_i < 0$) to make the $x^T A x <0$.
    \end{itemize}
    \item Similar proof for PD.
\end{itemize}
\end{derivation}

\subsection{Ellipsoids}
\begin{definition}
    PSD and PD matrices are used to define ellipsoids. For a PD matrix,
    \begin{align*}
        \mathcal{E} &= \left\{ x \in \mathbb{R}^n \mid (x - x^{(0)})^T P^{-1} (x - x^{(0)}) \leq 1 \right\} \\
        \mathcal{E} &= \left\{ x \in \mathbb{R}^n \mid x^T P^{-1} x - 2(x^{(0)})^T P^{-1} x + (x^{(0)})^T P^{-1} x^{(0)}  \leq 1 \right\}
    \end{align*}       
    \begin{itemize}
        \item $P$: PD matrix (i.e. defines the shape)
        \item $x^{(0)}$: Centered at $x^{(0)}$
        \item \textbf{Note:} $(x^{(0)})^T P^{-1} x = x^T P^{-1} x^{(0)}$
        \item \textbf{Note:} This is a quadratic function in $x$.
    \end{itemize}
\end{definition}

\begin{derivation}
    How to visualize the ellipsoid? $P = U \Lambda U^T \quad \text{$U$ is orthogonal, $\Lambda$ has eigenvalues with $\lambda_i>0$}$
    \begin{align*}
        (x - x^{(0)})^T P^{-1} (x - x^{(0)}) &= \bar{x}^T P^{-1} \bar{x} \quad \text{where } \bar{x} = x - x^{(0)} \\
        &= \bar{x}^T \left( U \Lambda U^T \right)^{-1} \bar{x} \quad \text{since $P$ is symmetric}\\
        &= \bar{x}^T (U^{T})^{-1} \Lambda^{-1} U^{-1} \bar{x} \quad \text{since for $X$ and $Y$ invertible } (XY)^{-1} = Y^{-1} X^{-1} \\
        &= \bar{x}^T U \Lambda^{-1} U^T \bar{x} \quad \text{since } U^T = U^{-1} \\
        &= \tilde{x}^T \Lambda^{-1} \tilde{x} \quad \text{where $\tilde{x} = U^T \bar{x}$}\\
        &= \begin{bmatrix}
            \tilde{x}_1 \\
            \vdots \\
            \tilde{x}_n
            \end{bmatrix}^T
            \begin{bmatrix}
            \frac{1}{\lambda_1} & 0 & \cdots & 0 \\
            0 & \frac{1}{\lambda_2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \frac{1}{\lambda_n}
            \end{bmatrix}
            \begin{bmatrix}
            \tilde{x}_1 \\
            \vdots \\
            \tilde{x}_n
            \end{bmatrix} \\ 
        &= \sum_{i=1}^{n} \frac{\tilde{x}_i^2}{\lambda_i} \\
        &= \sum_{i=1}^{n} \left(\frac{\tilde{x}_i}{\sqrt{\lambda_i}}\right)^2
    \end{align*} 
\end{derivation}

\begin{process}
    \begin{enumerate}
        \item Plot in the $\tilde{x}$-axis: $\mathcal{E} = \left\{ \tilde{x} \in \mathbb{R}^n \; \middle| \; \left( \frac{\tilde{x}_1}{\sqrt{\lambda_1}} \right)^2 + \cdots + \left( \frac{\tilde{x}_n}{\sqrt{\lambda_n}} \right)^2 \leq 1 \right\}$
        \customFigure[0.75]{00_Images/TI.png}{Tilde axis.}

        \item Plot in the $\bar{x}$-axis with rotation: $\tilde{x} = U^T \bar{x}  \rightarrow \bar{x} = U \tilde{x}$
        \begin{align*}
            \tilde{x} = \begin{bmatrix} \tilde{x}_1 \\ \tilde{x}_2 \end{bmatrix} \xrightarrow[]{U} \bar{x} = \begin{bmatrix} u^{(1)} & u^{(2)} \end{bmatrix} \begin{bmatrix} \tilde{x}_1 \\ \tilde{x}_2 \end{bmatrix}
        \end{align*}

        \begin{enumerate}
            \item For $\tilde{x} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$: 
            \begin{align*}
                \bar{x} = \begin{bmatrix} u^{(1)} & u^{(2)} \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = u^{(1)}
            \end{align*}
            \item For $\tilde{x} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$: 
            \begin{align*}
                \bar{x} = \begin{bmatrix} u^{(1)} & u^{(2)} \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = u^{(2)}
            \end{align*}
        \end{enumerate}

        \customFigure[0.75]{00_Images/BAR.png}{Bar axes.}
        
        \item Plot in the $x$-axis with translation: $\bar{x} = x - x^{(0)} \implies x = \bar{x} + x^{(0)}$
        \customFigure[0.75]{00_Images/X.png}{X axes.}
        
    \end{enumerate}
\end{process}

\subsection{Multivariate Gaussian Varibles}
\begin{example}
    Another reason why PD matrices are important is that they are used to define multivariate Gaussian variables.
    \vspace{1em}

    \textbf{Single-variable Gaussian:}
    \[
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-m)^2}{2\sigma^2}}
    \]

    \begin{itemize}
        \item $\sigma$: standard deviation
        \item $\sigma^2$: variance
        \item $m$: mean
    \end{itemize}

    \customFigure[0.75]{00_Images/GD.png}{Single-variable Gaussian distribution}
    \vspace{1em}

    \textbf{Multivariate Gaussian:}

    \[
    f(x) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} e^{-\frac{1}{2}(x-m)^T \Sigma^{-1}(x-m)}
    \]

    \begin{itemize}
        \item $x \in \mathbb{R}^n$
        \item $m \in \mathbb{R}^n$: mean vector 

        \item $\Sigma =
        \begin{bmatrix}
        \mathbb{E}[(x_1 - m_1)^2] &  \cdots & \mathbb{E}[(x_1 - m_1)(x_n - m_n)] \\
        \vdots & \ddots & \vdots \\
        \mathbb{E}[(x_n - m_n)(x_1 - m_1)] & \cdots & \mathbb{E}[(x_n - m_n)^2]
        \end{bmatrix}$: covariance matrix (\textbf{PD matrix})
    \end{itemize}

    \customFigure[0.75]{00_Images/GD2D.png}{3D-variable Gaussian distribution}


    \vspace{1em}

    \textbf{How do we visualize the Gaussian distribution?} See the intersection of $f$ with $g$, which happens when $f$ is the constant. $f$ is a constant when the exponent term is equal to a constant, i.e., 

\[
(x - m)^T \Sigma^{-1} (x - m) = \text{constant}
\]

We take the cross-section of the Gaussian PDF with the horizontal hyperplane. This cross-section is defined by:

\[
(x - m)^T \Sigma^{-1} (x - m) \leq \text{constant}
\]
\begin{itemize}
    \item \textbf{Note:} This is the form of the ellipse.
\end{itemize}

\customFigure[0.75]{00_Images/CS.png}{Cross-section of GD (i.e. Intersection of Gaussian distribution with horizontal hyperplane).}
\end{example}

\subsection{Sqaure roots of PSD}
\begin{definition}
    PSD matrices always has square roots (i.e. if $B=C^2$, then $C$ is the square root of $B$)

If $A \in S_+^n$, then:
\[
A = U \Lambda U^T
\]
\begin{itemize}
    \item Since $\lambda_i > 0$, therefore, we can always define the square root of $\Lambda$: $\Lambda^{1/2} = \begin{bmatrix} \sqrt{\lambda_1} & 0 & \cdots & 0 \\ 0 & \sqrt{\lambda_2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sqrt{\lambda_n} \end{bmatrix}$
\end{itemize}

Then:
\[
A = U \Lambda^{1/2} \Lambda^{1/2} U^T = (U \Lambda^{1/2} U^T)(U \Lambda^{1/2} U^T) = B \cdot B
\]

Therefore, $B = U \Lambda^{1/2} U^T$. $B$ is the square root of $A$, denoted by $A^{1/2}$.
\end{definition}
