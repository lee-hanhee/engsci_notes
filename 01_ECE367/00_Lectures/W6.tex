\subsection{2nd Optimization Problem (Motivation for SVD)}
\begin{definition}
    Let \( A \in S^n \) (not necessarily PSD). We are interested in solving the optimization problem:

    \begin{align*}
    \max_{s.t. ||x||_2 = 1} \|A x\|_2  
    \end{align*}
    \begin{itemize}
        \item s.t. is subject to
    \end{itemize}
\end{definition}

\begin{intuition}
    \customFigure[0.75]{00_Images/OPT.png}{Optimization problem}
    \begin{itemize}
        \item We are trying to find the maximum $x$ after being transformed by $A$, subject to being contained within the unit circle (in 2D).
    \end{itemize}
\end{intuition}

\begin{derivation}
    Let \( y = A x \). Then, using the spectral decomposition \( A = U \Lambda U^T \), we can express:

    \begin{align*}
    y &= A x = U \Lambda U^T x \\
    U^T y &= \Lambda U^T x \quad \text{multiply by $U^T$}\\
    \tilde{y} &= \Lambda \tilde{x} \quad \text{where} \quad \tilde{x} = U^T x \text{ and } \tilde{y} = U^T y 
    \end{align*}

    Thus, we have:

    \begin{align*}
    \| \tilde{y} \|_2 &= \| U^T y \|_2 = \| y \|_2 \\
    \| \tilde{x} \|_2 &= \| U^T x \|_2 = \| x \|_2
    \end{align*}
    \begin{itemize}
        \item \textbf{Note:} This comes from a previous lecture saying how an orthogonal matrix applied to a vector doesn't change the l2-norm of it. 
    \end{itemize}
    \vspace{1em}

    So the optimization problem becomes:

    \begin{align*}
    \max_{ s.t. \|\tilde{x}\|_2 = 1} \| \Lambda \tilde{x} \|_2 
    \end{align*}

    Since \( \Lambda \tilde{x} \) is diagonal, we can write:

    \begin{align*}
    \Lambda \tilde{x} = 
    \begin{bmatrix}
    \lambda_1 \tilde{x}_1 \\
    \lambda_2 \tilde{x}_2 \\
    \vdots \\
    \lambda_n \tilde{x}_n
    \end{bmatrix}
    \end{align*}

    \textbf{Solution:} If \( |\lambda_1| \geq |\lambda_2| \geq \dots \geq |\lambda_n| \), then the optimal solution is:

    \begin{align*}
    \tilde{x}^* = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
    \end{align*}
    \begin{itemize}
        \item Putting all of our budget into the largest eigenvalue to maximize the product while staying within the constraint.
    \end{itemize}
    \vspace{1em}

    Therefore, the optimal \( x^* = U \tilde{x}\) is:

    \begin{align*}
    x^* &= U \tilde{x}^* = U
    \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} = u^{(1)}
    \end{align*}

    The maximum value is:

    \begin{align*}
    \|A x^*\|_2 = \|y\|_2 = \| \tilde{y} \|_2 = |\lambda_1|
    \end{align*}

    \textbf{Conclusion:} The optimal direction of \( x \) is the direction of the eigenvector corresponding to the largest eigenvalue. This is the solution where $A$ is symmetric.
\end{derivation}

\subsection{3rd Optimization Problem}
\begin{derivation}
    \textbf{Extending to rectangular matrices:} What if we need to maximize \( \| M x \|_2 \) for some \( M \in \mathbb{R}^{m \times n} \) (not necessarily square)? This leads us to the Singular Value Decomposition (SVD).
    \begin{enumerate}
        \item \textbf{Generalization for Non-Symmetric or Non-Square Matrices}:
            \begin{itemize}
                \item For a matrix $M \in \mathbb{R}^{m \times n}$, the optimization problem becomes:
                    \begin{align*}
                        \max_{\text{s.t. } \|x\|_2 = 1} \|Mx\|_2.
                    \end{align*}
                \item This can be rewritten as:
                    \begin{align*}
                        \max_{\text{s.t. } \|x\|_2 = 1} \|Mx\|_2^2 = \max_{\text{s.t. } \|x\|_2 = 1} x^T (M^T M) x,
                    \end{align*}
                    \begin{itemize}
                        \item $A = M^T M$ is symmetric and positive semi-definite (PSD).
                        \begin{itemize}
                            \item $A^T = (M^T M)^T = M^T M = A$
                            \item \textbf{Note:} $A$ is in fact a PSD matrix because for any $x \in \mathbb{R}^n$, $x^T A x = x^T M^T M x = ||Mx||_2^2 \geq 0$
                        \end{itemize}
                    \end{itemize}
            \end{itemize}
        \item \textbf{Rayleigh Quotient:}
        \begin{itemize}
            \item We aim to solve:
                \begin{align*}
                    \max_{\text{s.t. } \|x\|_2^2 = x^T x=1} x^T A x.
                \end{align*}
            \item This is equivalent to maximizing the Rayleigh quotient:
                \begin{align*}
                    \max_{\text{s.t. } x \neq 0} \frac{x^T A x}{x^T x}.
                \end{align*}
                \begin{itemize}
                    \item \textbf{Note:} This is equivalent bc if $x$ is not unit norm, the division by $x^T x$ will normalize the function, therefore, any scaling factor will be canceled out. As a result, the only condition needed is that $x$ is not zero, so we don't divide by 0.
                \end{itemize}
        \end{itemize}
        \item \textbf{Upper and Lower Bounds}
        \begin{itemize}
            \item \textbf{Upper bound} can be achieved when $\tilde{x}$ is given by:
            \begin{align*}
                \tilde{x} &= 
                \begin{bmatrix}
                    1 \\ 
                    0 \\ 
                    \vdots \\ 
                    0 
                \end{bmatrix},
            \end{align*}
            which corresponds to $x = u^{(1)}$, the eigenvector associated with the largest eigenvalue $\lambda_{\max}(A)$.
        
            \item \textbf{Lower bound} can be achieved when $\tilde{x}$ is given by:
            \begin{align*}
                \tilde{x} &= 
                \begin{bmatrix}
                    0 \\ 
                    0 \\ 
                    \vdots \\ 
                    1 
                \end{bmatrix},
            \end{align*}
            which corresponds to $x = u^{(n)}$, the eigenvector associated with the smallest eigenvalue $\lambda_{\min}(A)$.
        \end{itemize}
        
        \item \textbf{Optimization Solution}
        
        Thus, this gives us a solution to the optimization problem:
        \begin{align*}
            \max_{\text{s.t. }\|x\|_2 = 1} \|Mx\|_2.
        \end{align*}
        
        The solution is the eigenvector of $M^T M$ corresponding to the largest eigenvalue, and the maximum is:
        \begin{align*}
            \sqrt{\lambda_{\max}(M^T M)}.
        \end{align*}
    \end{enumerate}
\end{derivation}

\subsubsection{Theorem}
\begin{theorem}
    Given a symmetric matrix $A \in \mathbb{R}^{n \times n}$, we have 
    \begin{align*}
        \lambda_{\text{min}}(A) \leq \frac{x^T A x}{x^T x} \leq \lambda_{\text{max}}(A), \quad \forall x \in \mathbb{R}^n.
    \end{align*}
    \begin{itemize}
        \item $\lambda_{\text{max}}(A) = \max_{x \neq 0} \frac{x^T A x}{x^T x}$
        \item $\lambda_{\text{min}}(A) = \min_{x \neq 0} \frac{x^T A x}{x^T x}$
    \end{itemize}    
\end{theorem}

\begin{derivation}
    \begin{enumerate}
        \item Given a symmetric matrix $A$, we can write its spectral decomposition as:
        \begin{align*}
            A &= U \Lambda U^T,
        \end{align*}
        where $U$ is orthogonal and $\Lambda$ is diagonal.
    
        \item For any vector $x$, we have:
        \begin{align*}
            x^T A x &= x^T U \Lambda U^T x \\
                    &= \tilde{x}^T \Lambda \tilde{x},
        \end{align*}
        where $\tilde{x} = U^T x$.
    
        \item Expanding this expression, we get:
        \begin{align*}
            x^T A x &= \sum_{i=1}^n \lambda_i \tilde{x}_i^2.
        \end{align*}
    
        \item Since $x^T x = \|x\|_2^2 = \|\tilde{x}\|_2^2$, we have:
        \begin{align*}
            \|x\|_2^2 &= \sum_{i=1}^n \tilde{x}_i^2
        \end{align*}
    
        \item The Rayleigh quotient becomes:
        \begin{align*}
            \frac{x^T A x}{x^T x} &= \frac{\sum_{i=1}^n \lambda_i \tilde{x}_i^2}{\sum_{i=1}^n \tilde{x}_i^2}.
        \end{align*}
        \begin{itemize}
            \item \textbf{Upper Bound}: Using the properties of eigenvalues, we get:
            \begin{align*}
                \frac{x^T A x}{x^T x} &\leq \frac{\sum_{i=1}^n \lambda_{\max} \tilde{x}_i^2}{\sum_{i=1}^n \tilde{x}_i^2} = \lambda_{\max}(A).
            \end{align*}
            \begin{itemize}
                \item \textbf{Note:} $\lambda_{\text{max}}$ doesn't rely on index, so we can bring it out.
            \end{itemize}
        
            \item \textbf{Lower Bound}: Similarly, we have:
            \begin{align*}
                \frac{x^T A x}{x^T x} &\geq \frac{\sum_{i=1}^n \lambda_{\min} \tilde{x}_i^2}{\sum_{i=1}^n \tilde{x}_i^2} = \lambda_{\min}(A).
            \end{align*}
            \begin{itemize}
                \item \textbf{Note:} $\lambda_{\text{min}}$ doesn't rely on index, so we can bring it out.
            \end{itemize}
        \end{itemize}
    
        \item Therefore, 
        \begin{align*}
            \lambda_{\min}(A) \leq \frac{x^T A x}{x^T x} \leq \lambda_{\max}(A).
        \end{align*}
    
        \item \textbf{Note:} If $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$, the maximum value is achieved when $\tilde{x} = \begin{bmatrix} 1 & 0 & \dots & 0 \end{bmatrix}^T$, giving:
        \begin{align*}
            x^T A x = \sum_{i=1}^{n} \lambda_i \tilde{x}_i^2 = \lambda_{\max} \|x\|_2^2
        \end{align*}
    \end{enumerate}    
\end{derivation}

\subsection{Singular value decomposition}
\begin{intuition}
    For any matrix $A \in \mathbb{R}^{m \times n}$, we can express it using Singular Value Decomposition (SVD) as:
    \begin{align*}
        A = U \Sigma V^T,
    \end{align*}
    where:
    \begin{itemize}
        \item $U$ is an orthogonal matrix of size $m \times m$.
        \item $V$ is an orthogonal matrix of size $n \times n$.
        \item $\Sigma$ is a "nearly diagonal" matrix of size $m \times n$ containing the singular values of $A$ along its main diagonal.
    \end{itemize}

    The matrix $\Sigma$ has the form:
    \[
    \Sigma = \begin{bmatrix}
    \sigma_1 & 0       & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    0       & \sigma_2 & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    \vdots  & \vdots   & \ddots & \vdots  & \vline & \vdots  & \ddots & \vdots \\
    0       & 0        & \cdots & \sigma_r & \vline & 0       & \cdots & 0 \\
    \hline
    0       & 0        & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    \vdots  & \vdots   & \ddots & \vdots  & \vline & \vdots  & \ddots & \vdots \\
    0       & 0        & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    \end{bmatrix}
    \]
    \begin{itemize}
        \item $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$ are the singular values of $A$, and $r = \text{rank}(A)$. The remaining entries in $\Sigma$ are zero.
    \end{itemize}
\end{intuition}

\begin{definition}
        Any matrix \( A \in \mathbb{R}^{m \times n} \) can be decomposed into
    \begin{equation*}
        A = U \Sigma V^T,
    \end{equation*}
    \begin{itemize}
        \item \( U \in \mathbb{R}^{m \times m} \): Orthogonal matrix. 
        \item \( V^T \in \mathbb{R}^{n \times n} \): Orthogonal matrix. 
        \begin{itemize}
            \item $U$ and $V^T$ live in different spaces.
        \end{itemize}
        \item \( \Sigma \in \mathbb{R}^{m \times n} \): Its first \( r \) diagonal entries \( \sigma_1, \dots, \sigma_r \) as positive, where \( r = \text{rank}(A) \), and the rest is zero.
    \end{itemize}

    \[
    \begin{bmatrix} A \end{bmatrix}_{m \times n} = \begin{bmatrix} U \end{bmatrix}_{m \times m} \begin{bmatrix} \Sigma \end{bmatrix}_{m \times n} \begin{bmatrix} V^T \end{bmatrix}_{n \times n}
    \]
    where
   \[
    \Sigma = \begin{bmatrix}
    \sigma_1 & 0       & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    0       & \sigma_2 & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    \vdots  & \vdots   & \ddots & \vdots  & \vline & \vdots  & \ddots & \vdots \\
    0       & 0        & \cdots & \sigma_r & \vline & 0       & \cdots & 0 \\
    \hline
    0       & 0        & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    \vdots  & \vdots   & \ddots & \vdots  & \vline & \vdots  & \ddots & \vdots \\
    0       & 0        & \cdots & 0       & \vline & 0       & \cdots & 0 \\
    \end{bmatrix}
    \]
\end{definition}

\subsubsection{Intuition on SVD:}
\begin{intuition}
    The decomposition \( y = A x \) can be written as
    \begin{align*}
        y &= A x \\
        &= U \Sigma V^T x,
    \end{align*}
    and
    \begin{align*}
        U^T y &= \Sigma V^T x \\
        \tilde{y} &= \Sigma \tilde{x}.
    \end{align*}

    The steps in the transformation are as follows:
    \begin{enumerate}
        \item Rotate/Flip using \( V^T \),
        \item Scale using \( \Sigma \),
        \item Rotate/Flip using \( U \).
    \end{enumerate}
    \vspace{1em}

\begin{itemize}
    \item \textbf{Note:} Since a linear map can be represented as $y=Ax$, therefore, SVD states that all linear maps are essentially scaling each component if 
    \begin{itemize}
        \item We allow some pre-processing of \( x \) by a rotation/flip into \( \tilde{x} \). 
        \item And then some post-processing of \( \tilde{y} \) by a rotation/flip to get \( y \).
    \end{itemize}
\end{itemize}
\end{intuition}

\begin{warning}
    Whenever you get the matrix, do the SVD.
\end{warning}

\subsection{Proof of SVD}
\subsubsection{$AA^T$ and $A^T A$}
\begin{derivation}
    Since we want \( A = U \Sigma V^T \), let's consider:

    \begin{enumerate}
        \item For \( A A^T \in m \times m\):
        \begin{align*}
            A A^T &= U \Sigma V^T (U \Sigma V^T)^T \\
            &= U \Sigma V^T V \Sigma^T U^T \\
            &= U \Sigma \Sigma^T U^T,
        \end{align*}
        \begin{itemize}
            \item \( A A^T \): Positive semi-definite (PSD) matrix (from previous lecture)
            \item Hence, \( \sigma_i = \sqrt{\lambda_i(A A^T)} \) because we are dealing with a spectral decomposition and $\Sigma \Sigma^T \text{ acts as the } \Lambda$ in this case. Same goes for $A^TA$
            \[
            \begin{bmatrix}
            \sigma_1^2 & 0         & \cdots & 0       & 0       & 0       \\
            0          & \sigma_2^2 & \cdots & 0       & 0       & 0       \\
            \vdots     & \vdots    & \ddots & \vdots  & \vdots  & \vdots  \\
            0          & 0         & \cdots & \sigma_r^2 & 0       & 0       \\
            0          & 0         & \cdots & 0       & \ddots  & \vdots  \\
            0          & 0         & \cdots & 0       & 0       & 0
            \end{bmatrix} \in m \times m
            \]

        \end{itemize}
    
        \item For \( A^T A \in n \times n\):
        \begin{align*}
            A^T A &= (U \Sigma V^T)^T U \Sigma V^T \\
            &= V \Sigma^T U^T U \Sigma V^T \\
            &= V \Sigma^T \Sigma V^T,
        \end{align*}
        \begin{itemize}
            \item Hence, \( \sigma_i = \sqrt{\lambda_i(A^T A)} \)
            \[
            \begin{bmatrix}
            \sigma_1^2 & 0         & \cdots & 0       & 0       & 0       \\
            0          & \sigma_2^2 & \cdots & 0       & 0       & 0       \\
            \vdots     & \vdots    & \ddots & \vdots  & \vdots  & \vdots  \\
            0          & 0         & \cdots & \sigma_r^2 & 0       & 0       \\
            0          & 0         & \cdots & 0       & \ddots  & \vdots  \\
            0          & 0         & \cdots & 0       & 0       & 0
            \end{bmatrix} \in n \times n
            \]
        \end{itemize}
    \end{enumerate}
    \begin{itemize}
        \item \textbf{Same eigenvalues:} \( A A^T \) and \( A^T A \) share the same set of non-zero eigenvalues, which will be proven as a fact, however, the total number of eigenvalues may differ.
    \end{itemize}
\end{derivation}

\subsubsection{Fact}
\begin{definition}
    For any matrices \( A \) and \( B \), \( AB \) and \( BA \) share the same set of non-zero eigenvalues.
\end{definition}

\begin{derivation}
    \begin{align*}
        A B v &= \lambda v, \\
        B A B v &= \lambda B v, 
    \end{align*}
    which implies that \( \lambda \) is an eigenvalue of \( BA \) with eigenvector $Bv$.
\end{derivation}\

\subsubsection{How are u and v related?}
\begin{derivation}
    We don’t need to do two spectral decompositions because \( u^{(i)} \)'s and \( v^{(i)} \)'s are related.
    \vspace{1em}

    \begin{enumerate}
        \item If \( v^{(i)} \) is an eigenvector of \( A^T A \), then
        \begin{align*}
            A^T A v^{(i)} &= \lambda_i v^{(i)}, \\
            A A^T (A v^{(i)}) &= \lambda_i (A v^{(i)}),
        \end{align*}
        which implies that \( A v^{(i)} \) is an eigenvector of \( A A^T \).
        \vspace{1em}
    
        \item Thus, we can find $u^{(i)}$ by normalizing \( A v^{(i)} \) by the corresponding singular value \( \sigma_i \) since $u$ lives in $AA^T$ space.
    
        \begin{align*}
            u^{(i)} &= \frac{A v^{(i)}}{\| A v^{(i)} \|_2} \\
            &= \frac{A v^{(i)}}{\sqrt{(A v^{(i)})^T (A v^{(i)})}} \\
            &= \frac{A v^{(i)}}{\sqrt{v^{(i) T} A^T A v^{(i)}}} \\
            &= \frac{A v^{(i)}}{\sqrt{v^{(i) T} \lambda_i v^{(i)}}} \quad \text{since $v^{(i)}$ is an eigenvector of $A^T A$}\\
            &= \frac{A v^{(i)}}{\sqrt{\lambda_i \, v^{(i) T} v^{(i)}}} \\
            &= \frac{A v^{(i)}}{\sqrt{\lambda_i}} \quad \text{(since } v^{(i)} \text{ is orthonormal vector}) \\
            &= \frac{A v^{(i)}}{\sigma_i}.
        \end{align*}
    \end{enumerate}
\end{derivation}

\subsubsection{Summarization of Steps for SVD Construction}
\begin{derivation}
    \begin{enumerate}
        \item Do a spectral decomposition of \( A^T A \) to get \( \{ v^{(1)}, \dots, v^{(n)} \} \) and \( \lambda_1, \dots, \lambda_r \), where \( r = \text{rank}(A^T A) \).

        \item Set \( \sigma_i = \sqrt{\lambda_i} \), for \( i = 1, \dots, r \).
        
        \item Set \( u^{(i)} = \frac{A v^{(i)}}{\sigma_i} \), for \( i = 1, \dots, r \).
    \end{enumerate}
    \begin{itemize}
        \item \textbf{Note:} We want to extend this to complete the orthonormal basis of \( \mathbb{R}^m \).
    \end{itemize}
\end{derivation}

\subsubsection{Orthogonality of \( u^{(i)} \)'s}
\begin{derivation}
    \( u^{(i)} \)'s are orthonormal because
    \begin{align*}
        u^{(i) T} u^{(j)} &= \left( \frac{A v^{(i)}}{\sigma_i} \right)^T \left( \frac{A v^{(j)}}{\sigma_j} \right) \\
        &= \frac{(v^{(i)})^T A^T A v^{(j)}}{\sigma_i \sigma_j} \\
        &= \frac{(v^{(i)})^T (\lambda_j v^{(j)})}{\sigma_i \sigma_j} \quad \text{since $v^{(i)}$ is an eigenvector of $A^T A$} \\
        &= \frac{\lambda_j \, (v^{(i)})^T v^{(j)}}{\sigma_i \sigma_j} \\
        &= \frac{\sigma_j^2 \, (v^{(i)})^T v^{(j)}}{\sigma_i \sigma_j} \\
        &= \begin{cases} 
            1 & \text{if } i = j, \\
            0 & \text{if } i \neq j.
        \end{cases}
    \end{align*}
\end{derivation}

\subsubsection{Why does this correspond to SVD?}
\begin{derivation}
    \[
    u^{(i) T} A v^{(j)} = u^{(i) T} u^{(j)} \cdot \sigma_j = 
    \begin{cases} 
        \sigma_j & \text{if } i = j, \\
        0 & \text{if } i \neq j.
    \end{cases}
    \]
    \begin{itemize}
        \item 1st to 2nd line comes from $u^{(i)} = \frac{A v^{(i)}}{\sigma_i}$.
    \end{itemize}
    \vspace{1em}

    We can represent this in matrix form as follows:
    \[
    \begin{bmatrix}
    u^{(1) T} \\
    \vdots \\
    u^{(r) T} \\
    \end{bmatrix}
    A 
    \begin{bmatrix}
    v^{(1)} & \dots & v^{(r)}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \sigma_1 & 0 & \cdots & 0 \\
    0 & \sigma_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \sigma_r
    \end{bmatrix}.
    \]
\end{derivation}

\subsubsection{Completing SVD:}
\begin{derivation}
    If \( r = \text{rank}(A) < m \), i.e., \( \{ u^{(1)}, \dots, u^{(r)} \} \) does not span \( \mathbb{R}^m \), we can find \( u^{(r+1)}, \dots, u^{(m)} \) to complete the orthonormal basis of \( \mathbb{R}^m \).

    \[
    \begin{bmatrix}
    u^{(1) T} \\
    \vdots \\
    u^{(r) T} \\
    u^{(r+1) T} \\
    \vdots \\
    u^{(m) T}
    \end{bmatrix}
    A 
    \begin{bmatrix}
    v^{(1)} & \dots & v^{(r)} & v^{(r+1)} & \dots & v^{(n)}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \sigma_1 & 0 & \cdots & 0 & \cdots & 0 \\
    0 & \sigma_2 & \cdots & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \sigma_r & \cdots & 0 \\
    0 & 0 & \cdots & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 0 & \cdots & 0
    \end{bmatrix}.
    \]
    \begin{itemize}
        \item 
    \end{itemize}

    This matrix is denoted by \( \Sigma \), where
    \[
    U^T A V = \Sigma \Rightarrow A = U \Sigma V^T.
    \]
\end{derivation}

\subsubsection{Fact:}
\begin{definition}
    \[
    \mathcal{N}(A^T A) = \mathcal{N}(A)
    \]
\end{definition}

\begin{derivation}
    
    \begin{enumerate}
        \item If \( x \in \mathcal{N}(A) \), then:
        \[
        A x = 0 \Rightarrow A^T A x = 0 \Rightarrow x \in \mathcal{N}(A^T A).
        \]
     
        \item Conversely, if \( x \in \mathcal{N}(A^T A) \), then:
        \begin{align*}
            A^T A x &= 0, \\
            \Rightarrow x^T A^T A x &= 0, \\
            \Rightarrow \| A x \|_2^2 &= 0, \\
            \Rightarrow A x &= 0, \\
            \Rightarrow x &\in \mathcal{N}(A).
        \end{align*}
    \end{enumerate}
    
    Therefore, \( \mathcal{N}(A^T A) = \mathcal{N}(A) \).
\end{derivation}

\subsection{SVD in Two Forms}
\begin{definition}
    We can write SVD in two forms:
    \begin{enumerate}
        \item Full SVD:
        \[
        A = U \Sigma V^T
        \]
        where
        \begin{itemize}
            \item \( A \) is \( m \times n \),
            \item \( U \) is \( m \times m \) (orthogonal matrix),
            \item \( \Sigma \) is \( m \times n \) (diagonal matrix with singular values),
            \item \( V^T \) is \( n \times n \) (orthogonal matrix).
        \end{itemize}
        \vspace{1em}
    
        \item Compact or Reduced SVD:
        \[
        \begin{bmatrix} A \end{bmatrix} = 
        \begin{bmatrix} 
        u^{(1)} & \cdots & u^{(r)} & \vline & u^{(r+1)} & \cdots & u^{(m)} 
        \end{bmatrix}
        \begin{bmatrix}
            \sigma_1 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
            0 & \cdots & \sigma_r & \vline & 0 & \cdots & 0 \\
            \hline
            0 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
            0 & \cdots & 0 & \vline & 0 & \cdots & 0 
        \end{bmatrix}
        \begin{bmatrix}
            u^{(1) T} \\
            \vdots \\
            u^{(r) T} \\
            \hline
            u^{(r+1) T} \\
            \vdots \\
            u^{(n) T}
        \end{bmatrix}
        \]
        where
        \begin{itemize}
            \item \( \tilde{U} \) is \( m \times r \), the first \( r \) columns of \( U \) (not orthogonal matrix)
            \item \( \tilde{\Sigma} \) is \( r \times r \), containing the non-zero singular values \( \sigma_1, \dots, \sigma_r \),
            \item \( \tilde{V}^T \) is \( r \times n \), the first \( r \) columns of \( V^T \) (not orthogonal matrix)
        \end{itemize}
    
        Thus,
        \[
        A = \tilde{U} \tilde{\Sigma} \tilde{V}^T.
        \]
        \item \textbf{Summation:}
        \begin{equation*}
            A = \sum_{i=1}^r \sigma_i u^{(i)} v^{(i) T}.
        \end{equation*}
        \begin{itemize}
            \item $u^{(i)}$: $m \times 1$
            \item $v^{(i)T}$: $1 \times n$
            \item $\sigma_i u^{(i)} v^{(i) T}$: $m \times n$
        \end{itemize}
    \end{enumerate}
\end{definition}

\subsection{Maximization Problem}
\begin{derivation}
    The goal is to maximize \( \|A x\|_2 \) subject to \( \|x\|_2 = 1 \).
    \begin{enumerate}
        \item Formulation
        \begin{align*}
            y &= A x \\
              &= U \Sigma V^T x,
        \end{align*}
        where \( U^T y = \Sigma V^T x \), leading to 
        \begin{equation*}
            \tilde{y} = \Sigma \tilde{x}
        \end{equation*}
        \vspace{1em}

        \item Optimization Reformulation:
        \begin{enumerate}
            \item Rewrite \( \|y\|_2 = \|\tilde{y}\|_2 \) and \( \|\tilde{x}\|_2 = \|x\|_2 \) since orthogonal matrix preserves the norms.
            \item The optimization problem becomes:
              \[
              \max_{\text{s.t. } \|\tilde{x}\|_2 = 1.} \| \Sigma \tilde{x} \|_2 
              \]
        \end{enumerate}
     
        \item Solution:
        \[
        \Sigma \tilde{x} = 
        \begin{bmatrix}
            \sigma_1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
            0 & \sigma_2 & \cdots & 0 & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \sigma_r & 0 & \cdots & 0 \\
            0 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0 & 0 & \cdots & 0
        \end{bmatrix}
        \begin{bmatrix}
            \tilde{x}_1 \\
            \tilde{x}_2 \\
            \vdots \\
            \tilde{x}_r \\
            \tilde{x}_{r+1} \\
            \vdots \\
            \tilde{x}_n
        \end{bmatrix}
        =
        \begin{bmatrix}
            \sigma_1 \tilde{x}_1 \\
            \sigma_2 \tilde{x}_2 \\
            \vdots \\
            \sigma_r \tilde{x}_r \\
            0 \\
            \vdots \\
            0
        \end{bmatrix}.
        \]
     
        \item Notation:
        \begin{itemize}
            \item \( \sigma_1, \dots, \sigma_r \): Singular values.
            \item \( u^{(1)}, \dots, u^{(m)} \): Left singular vectors.
            \item \( v^{(1)}, \dots, v^{(n)} \): Right singular vectors.
        \end{itemize}
     
        \item Result:
        If \( \sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r \), then the solution is 
        \[
        \tilde{x}^* = 
        \begin{bmatrix}
            1 \\
            0 \\
            \vdots \\
            0
        \end{bmatrix}.
        \]
     
     Thus, \( x^* = V \tilde{x}^* = v^{(1)} \), and we have:
     \[
     \| y^* \|_2 = \| A x^* \|_2 = \| \tilde{y}^* \|_2 = \| \Sigma \tilde{x}^* \|_2 = \sigma_1(A).
     \]
    \end{enumerate}
\end{derivation}

\subsection{SVD Relation to Range space and Null space of A}
\begin{definition}
    \begin{equation*}
        \mathcal{R}(A) = \text{span} \{ u^{(1)}, \dots, u^{(r)} \}
    \end{equation*}
    \begin{equation*}
        \mathcal{N}(A) = \text{span} \{ v^{(r+1)}, \dots, v^{(n)} \}
    \end{equation*}
\end{definition}
\begin{derivation}
    Given the decomposition:
    \[
    A x = U \Sigma V^T x
    \]
    we have:
    \[
    A x = 
    \begin{bmatrix}
    u^{(1)} & \cdots & u^{(r)} & \vline & u^{(r+1)} & \cdots & u^{(m)}
    \end{bmatrix}
    \begin{bmatrix}
        \sigma_1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
        0 & \sigma_2 & \cdots & 0 & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \sigma_r & 0 & \cdots & 0 \\
        0 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & 0 & \cdots & 0
    \end{bmatrix}
    \begin{bmatrix}
    v^{(1) T} \\
    \vdots \\
    v^{(r) T} \\
    \hline
    v^{(r+1) T} \\
    \vdots \\
    v^{(n) T}
    \end{bmatrix} x
    \]

    \textbf{Range of \( A \)}
    The range of \( A \), \( \mathcal{R}(A) \), is defined as the span of the columns of \( A \):
    \[
    \mathcal{R}(A) = \text{span of columns of } A = \{ A x \mid x \in \mathbb{R}^n \}.
    \]
    We can write this as:
    \[
    A x = 
    \begin{bmatrix}
    u^{(1)} & \cdots & u^{(r)} & u^{(r+1)} & \cdots & u^{(m)}
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_r \\
    0 \\
    \vdots \\
    0
    \end{bmatrix}.
    \]
    \begin{itemize}
        \item \textbf{Note:} $0$ for $r+1$ to $n$ (i.e. $n-r$ entries) because they get multiplied by $0$ in $\Sigma$. 
        \item \textbf{Note:} $x_i$ for $1$ to $r$ (i.e. $r$ entries) because they get multiplied by $\sigma_i$ in $\Sigma$.
    \end{itemize}
    Therefore,
    \[
    \mathcal{R}(A) = \text{span} \{ u^{(1)}, \dots, u^{(r)} \}.
    \]
    \vspace{1em}

    \textbf{Null Space of \( A \)}
    The null space of \( A \), \( \mathcal{N}(A) \), is defined as:
    \[
    \mathcal{N}(A) = \{ x \mid A x = 0 \}.
    \]
    If \( x = \sum_{i=r+1}^n \beta_i v^{(i)} \), then
    \[
    A x = U \Sigma 
    \begin{bmatrix}
    v^{(1) T} \\
    \vdots \\
    v^{(r) T} \\
    v^{(r+1) T} \\
    \vdots \\
    v^{(n) T}
    \end{bmatrix} x =  U \Sigma \begin{bmatrix}
        0 \\
        \vdots 
        0 \\
        x_{r+1} \\
        \vdots \\
        x_n
        \end{bmatrix}.
    \]
    \begin{itemize}
        \item \textbf{Note:} First $r$ entries are $0$ because of inner product with $v^{(i)}$ for $i=r+1$ to $n$ (i.e. how $x$ is defined)
        \item \textbf{Note:} Last $n-r$ entries are $x_{r+1}$ to $x_n$ because of inner product with $v^{(i)}$ for $i=r+1$ to $n$ (i.e. how $x$ is defined)
    \end{itemize}
    Thus,
    \[
    \mathcal{N}(A) = \text{span} \{ v^{(r+1)}, \dots, v^{(n)} \}.
    \]
\end{derivation}

\begin{derivation}
    \customFigure[0.4]{00_Images/RS1.png}{Another Derivation}
\end{derivation}

\subsection{What about A transpose?}
\begin{definition}
    \begin{equation*}
        A = U \Sigma V^T
        \end{equation*}
        
        \begin{equation*}
        A^T = (U \Sigma V^T)^T = V \Sigma^T U^T
        \end{equation*}
        \begin{itemize}
            \item $V$: $n \times n$
            \item $\Sigma^T$: $n \times m$
            \item $U^T$: $m \times m$
        \end{itemize}
        \begin{equation*}
            \begin{bmatrix}
            v^{(1)} & \ldots & v^{(r)} & | & v^{(r+1)} & \ldots & v^{(n)}
            \end{bmatrix}
            \Sigma^T
            \begin{bmatrix}
                u^{(1)T} \\ \vdots \\ u^{(r)T} \\ u^{(r+1)T} \\ \vdots \\ u^{(m)T}
            \end{bmatrix}
        \end{equation*}
\end{definition}

\subsubsection{Range and Null Space}
\begin{definition}
    \begin{align*}
        \mathcal{R}(A^T) &= \text{span}\{v^{(1)}, \ldots, v^{(r)}\} \\
        \mathcal{N}(A^T) &= \text{span}\{u^{(r+1)}, \ldots, u^{(m)}\}
    \end{align*}
    
    \vspace{1em}

    \textbf{FToLA (Use as a check):}
    \begin{equation*}
    \mathcal{N}(A) \oplus \mathcal{R}(A^T) = \mathbb{R}^n, \quad \mathcal{R}(A) \oplus \mathcal{N}(A^T) = \mathbb{R}^m
    \end{equation*}
\end{definition}

\subsection{Matrix Inverse}
\begin{definition}
    If \( A \) is a square matrix and invertible, then the SVD of $A^{-1}$
    \begin{equation*}
    A^{-1} = (U \Sigma V^T)^{-1} = V \Sigma^{-1} U^T
    \end{equation*}
    \begin{itemize}
        \item \textbf{Note:} $U^{-1} = U^T$ and $V^{-1} = V^T$ because they are orthogonal matrices.
    \end{itemize}

    \begin{equation*}
    A^{-1} = V 
    \begin{bmatrix}
    \frac{1}{\sigma_1} & 0 & \ldots & 0 \\
    0 & \frac{1}{\sigma_2} & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & \frac{1}{\sigma_r}
    \end{bmatrix}
    U^T
    \end{equation*}
    \begin{itemize}
        \item \textbf{Note on $A^{-1}$:} Rotate/Flip using \( U^T \), Scale using \( \Sigma^{-1} \), Rotate/Flip using \( V \).
        \item \textbf{Note on A:} Rotate/Flip using \( V^T \), Scale using \( \Sigma \), Rotate/Flip using \( U \).
        \item \textbf{Conclusion:} The inverse of A does the opposite transformation of A in the SVD.
    \end{itemize}
\end{definition}

\subsection{Pseudo-Inverse}
\begin{definition}
    For any matrix \( A = U \Sigma V^T \), the pseudo-inverse is given by:
    \begin{equation*}
    A^{\dagger} = V 
    \begin{bmatrix}
        1/\sigma_1 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
        \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
        0 & \cdots & 1/\sigma_r & \vline & 0 & \cdots & 0 \\
        \hline
        0 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
        \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
        0 & \cdots & 0 & \vline & 0 & \cdots & 0 
    \end{bmatrix}
    U^T
    \end{equation*}
    \begin{itemize}
        \item \textbf{Note:} It turns out this can be generalized to any matrix (not necessarily invertible) to the Moore-Penrose pseudo-inverse.
    \end{itemize}
\end{definition}

\subsubsection{Least Squares Solution}
\begin{definition}
    \begin{align*}
        A &= 
        \begin{bmatrix}
        \text{tall matrix with linearly independent columns}
        \end{bmatrix} 
        \Rightarrow A^{\dagger} = (A^T A)^{-1} A^T, \quad A^{\dagger} A = I \\
        A &= 
        \begin{bmatrix}
        \text{fat matrix with linearly independent rows}
        \end{bmatrix}
        \Rightarrow A^{\dagger} = A^T (A A^T)^{-1}, \quad A A^{\dagger} = I
    \end{align*}
\end{definition}

\subsection{Matrix Norms}
\subsubsection{Frobenius Norm}
\begin{definition}
    \begin{equation*}
        \|A\|_F = \sqrt{\sum_{i,j} a_{ij}^2} = \sqrt{\text{tr}(A^T A)} = \sqrt{\sum_{i=1}^r \sigma_i^2}
        \end{equation*}
\end{definition}

\begin{intuition}
    \customFigure[0.75]{00_Images/SVD.png}{Frobenius Norm}
    \begin{itemize}
        \item $\text{tr}(A^TA)$ makes sense because we are taking first row of $A^T$ and multiplying it with first column of $A$ and summing it up. Repeating this for all rows and columns gives us the sum of squares of all elements.
    \end{itemize}
\end{intuition}

\begin{derivation}
    \begin{align*}
        A &= U \Sigma V^T \\
        A^T A &= (V \Sigma^T U^T)(U \Sigma V^T) = V \Sigma^T \Sigma V^T \\
        &= V 
        \begin{bmatrix}
            \sigma_1^2 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
            0 & \cdots & \sigma_r^2 & \vline & 0 & \cdots & 0 \\
            \hline
            0 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
            0 & \cdots & 0 & \vline & 0 & \cdots & 0 
        \end{bmatrix} V^T
        \end{align*}
        
        \begin{align*}
        \text{tr}(A^T A) &= \text{tr} \left( V 
        \begin{bmatrix}
            \sigma_1^2 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
            0 & \cdots & \sigma_r^2 & \vline & 0 & \cdots & 0 \\
            \hline
            0 & \cdots & 0 & \vline & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vline & \vdots & \ddots & \vdots \\
            0 & \cdots & 0 & \vline & 0 & \cdots & 0 
        \end{bmatrix} V^T \right) \\
        &= \text{tr} \left( 
            V V^T
            \begin{bmatrix}
                \sigma_1^2 & \cdots & 0 \\
                \vdots & \ddots & \vdots \\
                0 & \cdots & \sigma_r^2 
            \end{bmatrix} \right) \quad \text{since tr($AB$) = tr($BA$)}\\
        &= \text{tr} \left( 
        \begin{bmatrix}
            \sigma_1^2 & \cdots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \cdots & \sigma_r^2 
        \end{bmatrix} \right) \\
        &= \sum_{i=1}^{r} \sigma_i^2
        \end{align*}
        
        \begin{equation*}
        \|A\|_F = \sqrt{\sum_{i=1}^{r} \sigma_i^2}
        \end{equation*}
\end{derivation}

\subsubsection{Operator Norm}
\begin{definition}
    \begin{equation*}
        \|A\|_2 = \max_{\|x\|_2 = 1} \|Ax\|_2 = \sigma_{\max}(A)
        \end{equation*}
\end{definition}

\subsection{Conditioning Number of a Matrix}
\begin{definition}
    The condition number \( \kappa(A) \) measures the numerical stability of solving \( A x = b \). 
    \begin{equation*}
        \kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
        \end{equation*}
\end{definition}

\subsubsection{Stability}
\begin{derivation}
        The solution is \( x = A^{-1} b \) (if A is invertible). If we change by some small perturbation:
        \begin{align*}
        b &\rightarrow b + \Delta b \\
        x &\rightarrow x + \Delta x
        \end{align*}
    
        We are interested in comparing:
        \begin{equation*}
        \frac{\|\Delta x\|_2}{\|x\|_2} \quad \text{vs} \quad \frac{\|\Delta b\|_2}{\|b\|_2}
        \end{equation*}
        \vspace{1em}

    \textbf{Observation}: If \( Ax = b \), and \( A(x + \Delta x) = b + \Delta b \), then 
\[
A \cdot \Delta x = \Delta b \implies \Delta x = A^{-1} \Delta b
\]
\begin{itemize}
    \item \( Ax = b \implies \| b \|_2 = \| A x \|_2 \leq \sigma_{\max}(A) \| x \|_2 \)
    \begin{itemize}
        \item Multiply by the norm of $x$ because $x$ is not assumed to have unit norm (i.e. $1$), so there is no limitation on $x$. Same idea for $\Delta b$
        \item \textbf{Note:} This was proven in the previous section.
    \end{itemize}
    \item \( \Delta x = A^{-1} \Delta b \implies \| \Delta x \|_2 = \| A^{-1} \Delta b \|_2 \leq \sigma_{\max}(A^{-1}) \| \Delta b \|_2 \)
\end{itemize}
\vspace{1em}

Now we can compare the quantity we are interested in: 

\begin{equation*}
    \frac{\|\Delta x\|_2}{\|x\|_2} \leq \frac{\sigma_{\max}(A^{-1}) \|\Delta b\|_2}{\frac{\|b\|_2}{\sigma_{\max}(A)}} 
    = \sigma_{\max}(A) \cdot \sigma_{\max}(A^{-1}) \cdot \frac{\|\Delta b\|_2}{\|b\|_2}
\end{equation*}
\begin{itemize}
    \item \textbf{QUESTION:} How is this upper bounded because in the previous derivations one is lower bounded and the other is upper bounded. 
\end{itemize}

Let \( A = U \Sigma V^T \), then 
\[
A^{-1} = V \Sigma^{-1} U^T = V 
\begin{bmatrix}
\frac{1}{\sigma_1} & 0 & \ldots & 0 \\
0 & \frac{1}{\sigma_2} & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \frac{1}{\sigma_r}
\end{bmatrix} 
U^T
\]

\[
\sigma_{\max}(A^{-1}) = \frac{1}{\sigma_{\min}(A)}
\]

Thus,
\[
\frac{\| \Delta x \|_2}{\| x \|_2} \leq \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} \cdot \frac{\| \Delta b \|_2}{\| b \|_2}
\]

Define the condition number as:
\[
\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
\]

If \( b \) changes by 10\%, then \( x \) changes by at most \( \kappa(A) \cdot 10\% \).

\begin{itemize}
    \item If \( \kappa(A) \) is small, solving \( Ax = b \) is numerically stable, implying \( A \) is a \textbf{well-conditioned} matrix.
    \item If \( \kappa(A) \) is large, solving \( Ax = b \) is numerically unstable, implying \( A \) is an \textbf{ill-conditioned} matrix.
\end{itemize}
\vspace{1em}

\textbf{Note:} The smallest value of $\kappa$ is $1$ (i.e. $\sigma_{\min} = \sigma_{\max}$), which is the best case scenario.
\end{derivation}

\subsection{Latent Semantic Indexing}
\begin{example}
    In homework 1, we compared documents. Assume we have a dictionary of size \( N \). Represent each document by a vector 
    \[
    d = \begin{bmatrix} d_1 \\ \vdots \\ d_N \end{bmatrix}
    \]
    where \( d_i = \) \# of occurrences of word \( i \) in the document.
    \vspace{1em}

    In the homework, we compared documents \( i \) and \( j \) by comparing their corresponding vectors \( d^{(i)} \) and \( d^{(j)} \). Instead, it would be nice to extract the meaning (i.e., the "semantics") of the words in the document.
    \vspace{1em}

    \textbf{Idea}: Think about a document as a linear combination of "concepts". 
    \begin{enumerate}
        \item Construct a data matrix 
        \[
        A = \begin{bmatrix} d^{(1)} & \ldots & d^{(m)} \end{bmatrix}
        \]
        where \( N \times m \) represents the number of words in the dictionary and the number of documents, respectively.
        \vspace{1em}
        
        \item Perform Singular Value Decomposition (SVD) on the data matrix \( A \):
        \[
        A = U \Sigma V^T = \tilde{U} \tilde{\Sigma} \tilde{V}^T = \begin{bmatrix} u^{(1)} & \ldots & u^{(r)} \end{bmatrix} 
        \begin{bmatrix}
        \sigma_1 & 0 & \ldots & 0 \\
        0 & \sigma_2 & \ldots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \ldots & \sigma_r
        \end{bmatrix} 
        \begin{bmatrix} v^{(1)T} \\ \vdots \\ v^{(r)T} \end{bmatrix}
        \]
    
        \item To analyze the \( i \)-th document:
        \[
        d^{(i)} = A e^{(i)} 
        \]
        where \( e^{(i)} \) is a standard basis vector with 1 in the \( i \)-th position. Thus,
        \[
        d^{(i)} = \tilde{U} \tilde{\Sigma} \tilde{V}^T e^{(i)}
        \]
        \[
        = \begin{bmatrix} u^{(1)} & \ldots & u^{(r)} \end{bmatrix}
        \begin{bmatrix}
        \sigma_1 & 0 & \ldots & 0 \\
        0 & \sigma_2 & \ldots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \ldots & \sigma_r
        \end{bmatrix}
        \begin{bmatrix} v^{(1)T} \\ \vdots \\ v^{(r)T} \end{bmatrix} e^{(i)}
        \]
        \[
        = \begin{bmatrix} u^{(1)} & \ldots & u^{(r)} \end{bmatrix}
        \begin{bmatrix} \sigma_1 v^{(1)T} \\ \vdots \\ \sigma_r v^{(r)T} \end{bmatrix} e^{(i)}
        \]
        \[
        = \begin{bmatrix} u^{(1)} & \ldots & u^{(r)} \end{bmatrix}
        \begin{bmatrix} \alpha_1 \\ \vdots \\ \alpha_r \end{bmatrix}
        = \sum_{j=1}^{r} \alpha_j u^{(j)}
        \]
        \begin{itemize}
            \item \textbf{Note:} Each $u$ vector is a concept, so we are writing the document as a linear combination of concepts weighted by $\alpha$, which is different for each document. This is what we are using to compare documents.
            \begin{itemize}
                \item $\alpha$ represents the importance of each concept in the document..
            \end{itemize}
            \textbf{Motivation:} $r \ll N$ (i.e. $r$ is much smaller than the size of the dictionary), so we are reducing the dimensionality of the problem, making it less computationally expensive to compare the similarity of documents. 
            \begin{itemize}
                \item HW1: Compared between length $N$ and here it's only $r$.
            \end{itemize}
        \end{itemize}
    
        \item To compare documents, we can compare the corresponding coefficient vector \( \alpha \).
    
        \item \textbf{New Document:} When a new document \( g \) comes along, we can add \( g \) to \( A \) and rerun the SVD, but this is computationally expensive.
    
        Instead, we project \( g \) onto the span of \( \mathcal{R} = \{ u^{(1)}, \ldots, u^{(r)} \} \) (i.e. projecting onto the range of \( A \)):
        \[
        g^* = \sum_{j=1}^{r} \beta_j u^{(j)}, \quad \beta_j = \langle g, u^{(j)} \rangle
        \]
    
        \item We then use the vectors \( \{ \alpha_j \} \) and \( \{ \beta_j \} \) to compare documents, for example, by finding the angle between them.
    \end{enumerate}

\end{example}

\subsection{Principle component analysis}
\begin{example}
    \textbf{Example of PCA: Eigenfaces for face recognition (HW3)}
    \begin{enumerate}
        \item Vectorize each image: Each face image of size \( 32 \times 32 \) pixels is vectorized into a column vector \( x^{(i)} \) of dimension \( m = 32 \times 32 \).

        \item Center each vector: 
           \[
           \tilde{x}^{(i)} = x^{(i)} - \bar{x}
           \]
           where 
           \[
           \bar{x} = \frac{1}{N} \sum_{i=1}^{N} x^{(i)}
           \]
        
        \item Construct the data matrix:
           \[
           \tilde{X} = \begin{bmatrix} \tilde{x}^{(1)} & \ldots & \tilde{x}^{(N)} \end{bmatrix}
           \]
        
        \item Perform spectral decomposition on the covariance matrix:
           \[
           C = \tilde{X} \tilde{X}^T = U \Lambda U^T
           \]
           \[
            C= 
            \begin{bmatrix}
            u^{(1)} & \ldots & u^{(m)}
            \end{bmatrix}
            \Lambda
            \begin{bmatrix}
            u^{(1)T} \\
            \vdots \\
            u_{(m)T}
            \end{bmatrix}
            \]
        
        \item Alternatively, we could perform SVD on \( \tilde{X} \):
        \[
        \tilde{X} = U \Sigma V^T
        \]
        \begin{itemize}
            \item \textbf{Note:} This results in the same matrix \( U \) as obtained from the spectral decomposition of \( \tilde{X} \tilde{X}^T \).
        \end{itemize}
    \end{enumerate}
\end{example}

\begin{example} \textbf{PCA:}
    \begin{enumerate}
        \item Given data points \( x^{(i)} \in \mathbb{R}^n \), $i=1,\ldots,m$
        
        \item \textbf{Goal of PCA:} Find a direction \( z \in \mathbb{R}^n \) that explains most of the variance in \( x^{(i)} \)'s.
        \customFigure[0.5]{00_Images/PCA.png}{PCA}
        \item \textbf{How to formulate this problem?}
        
        \begin{enumerate}
            \item Center the data:
            \[
            \tilde{x}^{(i)} = x^{(i)} - \bar{x}, \quad \bar{x} = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}
            \]
            
            \item Fix a direction \( z \). Project \( \tilde{x}^{(i)} \) onto \( \text{span}\{z\} \):
            \[
            \text{Proj}_{\text{span}\{z\}} \tilde{x}^{(i)} = \langle \tilde{x}^{(i)}, z \rangle z = \alpha_i z \quad (\text{assuming } \|z\|_2 = 1)
            \]
            
            \item Look for $z$ that maximizes the variance of the \( \alpha_i \)'s:
            \[
            \text{Variance} = \frac{1}{m} \sum_{i=1}^{m} \alpha_i^2 = \frac{1}{m} \sum_{i=1}^{m} \left( z^T \tilde{x}^{(i)} \right) \left( \tilde{x}^{(i)T} z \right)
            \]
            \[
            = \frac{1}{m} z^T \left( \sum_{i=1}^{m} \tilde{x}^{(i)} \tilde{x}^{(i)T} \right) z
            \]
            \[
            = \frac{1}{m} z^T \tilde{X} \tilde{X}^T z
            \]
            \begin{itemize}
                \item $\tilde{X} = \begin{bmatrix} \tilde{x}^{(1)} & \ldots & \tilde{x}^{(m)} \end{bmatrix}$
                \item $
            \tilde{X} \tilde{X}^T = 
            \begin{bmatrix}
            \tilde{x}^{(1)} & \ldots & \tilde{x}^{(m)}
            \end{bmatrix}
            \begin{bmatrix}
            \tilde{x}^{(1)T} \\
            \vdots \\
            \tilde{x}^{(m)T}
            \end{bmatrix}
            = \sum_{i=1}^{m} \tilde{x}^{(i)} \tilde{x}^{(i)T}$
            \end{itemize}
        \end{enumerate}
        
        \item We want to solve:
        \[
        \max_{z} z^T \tilde{X} \tilde{X}^T z \quad \text{subject to} \quad \|z\|_2 = 1
        \]
        This can be expressed as maximizing the Rayleigh quotient:
        \[
        \max \frac{z^T \tilde{X} \tilde{X}^T z}{\|z\|_2^2}
        \]
        
       \item The solution is given by the eigenvector corresponding to the largest eigenvalue of \( \tilde{X} \tilde{X}^T \).
        
        \item \textbf{Alternatively}, we could have used SVD on \( \tilde{X} \):
        \[
        \tilde{X} = U \Sigma V^T
        \]
        \[
        z^T \tilde{X} \tilde{X}^T z = z^T \left( U \Sigma V^T \right) \left( V \Sigma^T U^T \right) z = z^T U \Sigma^2 U^T z = \omega^T \Sigma^2 \omega = \sum_{i=1}^{r} \sigma_i^2 \omega_i^2
        \]
        
        \item So, the problem becomes:
        \[
        \max \sum_{i=1}^{r} \sigma_i^2 \omega_i^2 \quad \text{subject to} \quad \sum_{i=1}^{r} \omega_i^2 = 1
        \]
        
        \item The solution is \( \omega^* = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \), corresponding to the first singular vector, assuming $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r$, so the maximizing $z^* = U \omega^* u^{(1)}$.
        In addition, the maximum is \( \sigma_1^2 = \lambda_{\max} (\tilde{X} \tilde{X}^T) \).
        
        \item To find the second principal component, remove the effect of the direction $u^{(1)}$ with the largest variance, and then compute

        \begin{enumerate}
            \item \textbf{Computation}

            We compute:
            \begin{align*}
            \tilde{\tilde{x}}^{(i)} &= \tilde{x}^{(i)} - \langle \tilde{x}^{(i)}, u^{(1)} \rangle u^{(1)} \\
            &= \tilde{x}^{(i)} - u^{(1)} u^{(1)\top} \tilde{x}^{(i)} \\
            &= \left(I - u^{(1)} u^{(1)\top} \right) \tilde{x}^{(i)}.
            \end{align*}
    
            Next, define:
            \[
            \tilde{\tilde{X}} = 
            \begin{bmatrix}
            \tilde{\tilde{x}}^{(1)} & \cdots & \tilde{\tilde{x}}^{(m)}
            \end{bmatrix}.
            \]
    
            Then:
            \[
            \tilde{\tilde{X}} = \left(I - u^{(1)} u^{(1)\top} \right) \tilde{X}.
            \]
    
            \item \textbf{Using Singular Value Decomposition (SVD)}
    
            Let \(\tilde{X} = U \Sigma V^\top\), where:
            \[
            \tilde{X} = \sum_{i=1}^r \sigma_i u^{(i)} v^{(i)\top}.
            \]
    
            Compute \( \tilde{\tilde{X}} \):
            \begin{align*}
            \tilde{\tilde{X}} &= \left(I - u^{(1)} u^{(1)\top} \right) \tilde{X} \\
            &= \tilde{X} - u^{(1)} u^{(1)\top} \tilde{X} \\
            &= \sum_{i=1}^r \sigma_i u^{(i)} v^{(i)\top} - u^{(1)} u^{(1)\top} \sum_{i=1}^r \sigma_i u^{(i)} v^{(i)\top}.
            \end{align*}
    
            Simplify:
            \begin{align*}
            \tilde{\tilde{X}} &= \sum_{i=1}^r \sigma_i u^{(i)} v^{(i)\top} - \sum_{i=1}^r \sigma_i \left(u^{(1)} u^{(1)\top} u^{(i)} \right) v^{(i)\top}.
            \end{align*}
    
            Since \(u^{(1)\top} u^{(i)} = 0\) for \(i \neq 1\), we get:
            \begin{align*}
            \tilde{\tilde{X}} &= \sum_{i=1}^r \sigma_i u^{(i)} v^{(i)\top} - \sigma_1 u^{(1)} v^{(1)T} = \sum_{i=2}^r \sigma_i u^{(i)} v^{(i)\top} .
            \end{align*}
    
            
            So, the next strongest direction that explains the most variance is the singular vector corresponding to the second-largest singular value.
        \end{enumerate}
    \end{enumerate}
\end{example}