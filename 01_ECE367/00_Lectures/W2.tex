\subsection{Projection onto subspaces}
    \begin{definition}
        \begin{equation}
            x^* = \text{Proj}_S(x) = \arg\min_{y \in S} \|x - y\|_2
        \end{equation}


        $\text{If} \ \{v^{(1)}, \dots, v^{(d)}\} \ \text{is an orthonormal basis of} \ S$ then 
        \begin{equation}
            x^* = \sum_{i=1}^{d} \langle x, v^{(i)} \rangle v^{(i)}
        \end{equation}
        \begin{itemize}
            \item The error vector should be orthogonal to each vector in the subspace.
        \end{itemize}

        \customFigure[0.5]{00_Images/EP.png}{Error vector being perp. to S.}
    \end{definition}

    \begin{example}
        For $v = \begin{bmatrix}
        v_1 \\
        v_2 \\
        v_3
        \end{bmatrix}$
        \vspace{1em}

        The ith component can be extracted by doing the inner product with the ith standard basis:
        \[
        \begin{bmatrix}
        v_1 & v_2 & v_3
        \end{bmatrix}
        \begin{bmatrix}
        1 \\
        0 \\
        0
        \end{bmatrix}
        = v_1
        \]

        \[
        \begin{bmatrix}
        v_1 & v_2 & v_3
        \end{bmatrix}
        \begin{bmatrix}
        0 \\
        1 \\
        0
        \end{bmatrix}
        = v_2
        \]

        \[
        \begin{bmatrix}
        v_1 & v_2 & v_3
        \end{bmatrix}
        \begin{bmatrix}
        0 \\
        0 \\
        1
        \end{bmatrix}
        = v_3
        \]
        \vspace{1em}

        Therefore, analogous to x*, we can write them as the sum of the inner product times the standard basis.
        \[
        v = v_1
        \begin{bmatrix}
        1 \\
        0 \\
        0
        \end{bmatrix}
        + v_2
        \begin{bmatrix}
        0 \\
        1 \\
        0
        \end{bmatrix}
        + v_3
        \begin{bmatrix}
        0 \\
        0 \\
        1
        \end{bmatrix}
        \]
    \end{example}
    
    \subsubsection{Basic problem}
        \begin{intuition}
            Given $x\in \mathcal{V}$ and a subspace $S$. Find the closest point (in norm) in $S$ to $x$: 
            \begin{equation}
                \text{Proj}_S(x) = \arg\min_{y \in S} \| y - x \|
            \end{equation}
            \begin{itemize}
                \item $\| y - x \|$: Some norm.
                \item \textbf{Subspace:} $S$ doesn't have to be a subspace. 
                \item $\arg\min$: Vector $y$ that minimizes $\lVert x - y\rVert$
            \end{itemize}
        \end{intuition}
        
    \subsubsection{Projection onto a 1D subspace}
        \begin{example}
            Projection onto a 1-dimensional subspace.

            Let \( S = \text{span}(\mathbf{v}) \), and we denote the projection of \( \mathbf{x} \) onto \( S \) as:
            \[
            \text{Proj}_S(\mathbf{x}) = \mathbf{x}^*
            \]
            Under the Euclidean norm (i.e. l2 norm), we have nice geometry: we should have
            \[
            \langle \mathbf{x} - \mathbf{x}^*, \mathbf{v} \rangle = 0
            \]
            Since \( \mathbf{x}^* \in S \), \( \mathbf{x}^* = \alpha \mathbf{v} \) for some scalar \( \alpha \).

            We need to find \( \alpha \).

            So,
            \[
            \langle \mathbf{x} - \alpha \mathbf{v}, \mathbf{v} \rangle = 0
            \]
            \[
            \Rightarrow \langle \mathbf{x}, \mathbf{v} \rangle - \alpha \langle \mathbf{v}, \mathbf{v} \rangle = 0
            \]
            \[
            \Rightarrow \alpha = \frac{\langle \mathbf{x}, \mathbf{v} \rangle}{\langle \mathbf{v}, \mathbf{v} \rangle}
            \]
            Thus, 
            \[
            \mathbf{x}^* = \alpha \mathbf{v} = \frac{\langle \mathbf{x}, \mathbf{v} \rangle}{\langle \mathbf{v}, \mathbf{v} \rangle} \mathbf{v}
            \]
            which simplifies to:
            \[
            \mathbf{x}^* = \frac{\mathbf{x}^\top \mathbf{v}}{\|\mathbf{v}\|_2^2} \mathbf{v} = \left\langle \mathbf{x}, \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\rangle \frac{\mathbf{v}}{\|\mathbf{v}\|_2}
            \]
            \begin{itemize}
                \item \textbf{Orthonormal Basis for S:} \( \left\{ \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\} \) since \( \left\lVert \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\rVert_2 = 1 \)
                \item \textbf{Projection Coefficient:} $\left\langle \mathbf{x}, \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\rangle$
                \item \textbf{Note:} $x^*$ is the point we are looking for in the projection problem.
            \end{itemize}
            \customFigure[0.5]{00_Images/Example_Projection.png}{Visual representation of the projection problem.}
        \end{example}

    \subsubsection{Projection onto an n dimensional space}
        \begin{example}
        This can be generalized to higher dimensions. Let \( S \) be a subspace of \( \mathcal{V} \), and let \( \{ \mathbf{v}_1, \dots, \mathbf{v}_d \} \) be an orthonormal basis of \( S \).
        \begin{enumerate}
            \item \textbf{Problem setup}
            Let 
            \[
            \mathbf{x}^* = \sum_{i=1}^{d} \alpha_i \mathbf{v}_i
            \]
            \textbf{Goal:} Find \( \alpha_1, \dots, \alpha_d \) so as to minimize the norm \( \|\mathbf{x} - \mathbf{x}^*\|_2 \).

            \item \textbf{Derivation:}
            By geometry, we require that 
            \[
            \langle \mathbf{e}, \mathbf{v}_j \rangle = 0 \quad \forall \, j = 1, \dots, d
            \]
            which implies:
            \[
            \langle \mathbf{x} - \mathbf{x}^*, \mathbf{v}_j \rangle = 0 \quad \forall \, j
            \]
            \[
            \Rightarrow \langle \mathbf{x} - \sum_{i=1}^{d} \alpha_i \mathbf{v}_i, \mathbf{v}_j \rangle = 0 \quad \forall \, j
            \]
            Using linearity of the inner product:
            \[
            \Rightarrow \langle \mathbf{x}, \mathbf{v}_j \rangle = \sum_{i=1}^{d} \alpha_i \langle \mathbf{v}_i, \mathbf{v}_j \rangle
            \]
            Since \( \langle \mathbf{v}_i, \mathbf{v}_j \rangle = 0 \) if \( i \neq j \) and \( 1 \) if \( i = j \), this simplifies to:
            \[
            \alpha_j = \langle \mathbf{x}, \mathbf{v}_j \rangle \quad \text{b/c only the i=j term survives}
            \]
            Thus,
            \[
            \mathbf{x}^* = \sum_{i=1}^{d} \alpha_i \mathbf{v}_i = \sum_{i=1}^{d} \langle \mathbf{x}, \mathbf{v}_i \rangle \mathbf{v}_i
            \]

            \item \textbf{Solution:}
            \[
            = \sum_{i=1}^{d} (\mathbf{x}^\top \mathbf{v}_i) \mathbf{v}_i
            \]
            \begin{itemize}
                \item \( \mathbf{v}_i \in \mathbb{R}^n \).
                \item \textbf{Projection Coefficients:} $\mathbf{x}^\top \mathbf{v}_i$
            \end{itemize}

            \item \textbf{Example of Orthogonal Decomposition:}
            \[
            \mathbf{e} = \mathbf{x} - \mathbf{x}^* \in S^\perp, \quad \mathbf{x}^* \in S
            \]
            So, 
            \[
            \mathbf{x} = \mathbf{x}^* + \mathbf{e}, \quad \text{where} \quad \mathbf{x}^* \in S, \quad \mathbf{e} \in S^\perp
            \]
        \end{enumerate}

        \customFigure[0.5]{00_Images/General.png}{Generalization of projection.}
        \end{example}

    \subsubsection{Application of projections: Fourier series}
        \begin{example}
            \textbf{Fourier series:}
            \begin{enumerate}
                \item Suppose we have a periodic function $x(t)$ with period $T_0$.

                \begin{center}
                \customFigure[0.5]{00_images/function_graph.png}{Periodic triangle function.}
                \end{center}

                \item \textbf{Inner product for time domain (complex version):} $a_k = \langle x(t), y(t) \rangle = \frac{1}{T} \int_{T} x(t) \overline{y(t)} \, dt$
                \begin{itemize}
                    \item \textbf{Note:} Real version is without the conjugate.
                \end{itemize}
                \item \textbf{Projection (i.e. one component of the sum):} $\text{Proj}_{\underline{v}_i} (\underline{x}) = \langle \underline{x}, \underline{v}_i \rangle \underline{v}_i$
                
                \item \textbf{Goal:} Express $x(t)$ (i.e. any periodic function) as a sum of complex exponentials:
                
                \[
                x^{*}(t) = \sum_{k=-\infty}^{\infty} a_k e^{jk\omega_0 t}
                \]
                
                \begin{itemize}
                    \item \textbf{Projection:} $\text{Proj}_{e^{j k \omega_0 t}} \left( x(t) \right) = \left\langle x(t), \exp\left( j k \omega_0 t \right) \right\rangle e^{j k \omega_0 t} = a_k e^{j k \omega_0 t}$ for a certain value of $k$.
                    \item \textbf{Projection coefficient:} $a_k = \left\langle x(t), e^{j k \omega_0 t} \right\rangle = \frac{1}{T_0} \int_{0}^{T} x(t) e^{-jk \omega_0 t} dt$
                    \item \textbf{Fundamental frequency:} $\omega_0 = \frac{2\pi}{T_0}$.
                \end{itemize}
        
                \item \textbf{Prove orthonormal basis for the complex exponentials:} To prove it's a orthnormal basis, must prove it has unit norm 1 and each pair of vectors are orthogonal (i.e. inner product is 0).
                \begin{enumerate}
                    \item \textbf{Magnitude of exp:} $\abs{e^{j\theta}}=1$. Therefore, it has unit norm.
                    \item \textbf{Orthogonality:}
                        \[
                        \left\langle e^{j i \omega_0 t}, e^{j l \omega_0 t} \right\rangle = 
                        \begin{cases}
                        1, & i = l \\
                        0, & i \neq l
                        \end{cases}
                        \]
                        Therefore, for each pair of basis vectors, they are orthogonal.
                    \begin{itemize}
                        \item \textbf{Conjugate of exp:} $(e^{j\theta}) = e^{-j \theta}$
                    \end{itemize}
                \end{enumerate}
                \item \textbf{Conclusion:} Fourier series is a projection of a function onto the set of othonormal basis functions $\text{exp}(jk\omega_0 t)$, where $k$ is an integer.
                \begin{itemize}
                    \item \textbf{Optimal:} This projection is optimal as it minimizes the approximation error \( \| x(t) - x^*(t) \| \), i.e.
                    
                    \[
                    \frac{1}{T} \int_0^T \left( x(t) - x^*(t) \right)^2 dt
                    \]
                    As the number of terms in the summation increases to infinity, the error goes to 0. 
                \end{itemize}
            \end{enumerate}
        \end{example}

\subsection{Gram-Schmidt and QR decomposition}
    \subsubsection{What if the set of basis vectors is not orthonormal?}
    \begin{intuition}
        Let $\{u^{(1)}, \dots, u^{(d)}\} \text{ be a set of basis vectors for a subspace } S (\text{ not necessarily orthonormal})$
        \vspace{1em}

        $\text{We can still use the orthogonality principle, i.e.,}$
        \[
        e = x - x^* \perp S
        \]
        \vspace{1em}

        Therefore, 
        \[
        \langle x - x^*, u^{(j)} \rangle = 0 \quad \forall j = 1, \dots, d 
        \]
        
        $\text{Also, } x^* \in S \text{ so } x^* \text{can be written as a linear combination of basis vectors, so } x^* = \sum_{i=1}^{d} \alpha_i u^{(i)}$
        \vspace{1em}
        
        $\text{Need to find} \ \alpha_1, \dots, \alpha_d$ s.t.

        \[
        \langle x - \sum_{i=1}^{d} \alpha_i u^{(i)}, u^{(j)} \rangle = 0 \quad \forall j = 1, \dots, d
        \]
        \[
        \Rightarrow \langle x, u^{(j)} \rangle = \sum_{i=1}^{d} \alpha_i \langle u^{(i)}, u^{(j)} \rangle \quad \forall j = 1, \dots, d
        \]

        \[
        \begin{bmatrix}
        \langle u^{(1)}, u^{(1)} \rangle & \langle u^{(2)}, u^{(1)} \rangle & \dots & \langle u^{(d)}, u^{(1)} \rangle \\
        \langle u^{(1)}, u^{(2)} \rangle & \langle u^{(2)}, u^{(2)} \rangle & \dots & \langle u^{(d)}, u^{(2)} \rangle \\
        \vdots & \vdots & \ddots & \vdots \\
        \langle u^{(1)}, u^{(d)} \rangle & \langle u^{(2)}, u^{(d)} \rangle & \dots & \langle u^{(d)}, u^{(d)} \rangle
        \end{bmatrix}
        \begin{bmatrix}
        \alpha_1 \\
        \alpha_2 \\
        \vdots \\
        \alpha_d
        \end{bmatrix}
        =
        \begin{bmatrix}
        \langle x, u^{(1)} \rangle \\
        \langle x, u^{(2)} \rangle \\
        \vdots \\
        \langle x, u^{(d)} \rangle
        \end{bmatrix}
        \]

        $\text{Solve for} \ \alpha_1, \dots, \alpha_d$, $\text{Then, we get} \ x^* = \sum_{i=1}^{d} \alpha_i u^{(i)}$
        \customFigure[0.25]{00_Images/NO.png}{Not orthogonal, but similar to projection with orthonormal basis.}
        \begin{itemize}
            \item \textbf{Note:} $\text{If } \{u^{(1)}, \dots, u^{(d)}\} \text{ is an orthonormal basis, then the matrix is the identity matrix, and we get } \alpha_j = \langle x, u^{(j)} \rangle \text{ as before.}$
        \end{itemize}
    \end{intuition}

    \begin{example}
        \textbf{Function approximation.}
        Let $B$ be the set of basis functions that is not orthonormal:
        \[
        \mathcal{B} = \{1, t, \dots, t^d\}
        \]

        $\text{Let} \ x(t) \ \text{be a function over} \ [0, 1].$
        \begin{itemize}
            \item \textbf{1st Goal} Approximate $x(t) \text{ by } x^*(t) = \sum_{n=0}^{d} \alpha_n t^n$
            \item $\text{To find } \alpha_0, \alpha_1, \dots, \alpha_d$, need to solve the $Ax=b$.
            \item \textbf{2nd Goal:} Minimize the approximation error $\|x(t) - x^*(t)\|_2 = \left( \int_0^1 (x(t) - x^*(t))^2 dt \right)^{1/2}$
        \end{itemize}

        \textbf{Recall: Taylor series expansion}
        \[
        x(t) \approx x(0) + x'(0) t + \frac{x''(0)}{2} t^2 + \dots
        \]

        \begin{itemize}
            \item Taylor series expansion is completely different from the projection method, and the reason is that Taylor series expansion is a local approximation.
        \end{itemize}

    \end{example}

    \subsubsection{Gram-Schmidt Procedure}
    \textbf{Motivation:} This is to get an orthonormal basis, so we can use the easier projection method.
    \begin{intuition}
        Another way to find the projection of \(x\) onto $S = \text{span} \{u^{(1)}, \dots, u^{(d)}\}$ is to first find an orthonormal basis of \(S\), and then the projection problem becomes easier.
        \customFigure[0.5]{00_Images/GS_2D.png}{Gram-Schmidt Process for 2D.}
        \begin{enumerate}
            \item Normalize $u^{(1)}$ 
            \item Find the error vector by projecting $u^{(2)}$ onto the subspace $v^{(1)}$.
            \item Normalize the error vector.
            \item Now you have two vectors that form an orthonormal basis in 2D.
        \end{enumerate}
    \end{intuition}

    \subsubsection{QR decomposition}

    \subsubsection{Eigen-composition}

    \subsubsection{Singular value decomposition}

    
        
\subsection{Hyperplanes and half-spaces}
\subsection{Non-euclidean projection}