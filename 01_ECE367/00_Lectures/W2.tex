\subsection{Projection onto subspaces}
    \begin{definition}
        \begin{equation}
            x^* = \text{Proj}_S(x) = \arg\min_{y \in S} \|x - y\|_2
        \end{equation}


        $\text{If} \ \{v^{(1)}, \dots, v^{(d)}\} \ \text{is an orthonormal basis of} \ S$ then 
        \begin{equation}
            x^* = \sum_{i=1}^{d} \langle x, v^{(i)} \rangle v^{(i)}
        \end{equation}
        \begin{itemize}
            \item The error vector should be orthogonal to each vector in the subspace.
        \end{itemize}

        \customFigure[0.5]{00_Images/EP.png}{Error vector being perp. to S.}
    \end{definition}

    \begin{example}
        For $v = \begin{bmatrix}
        v_1 \\
        v_2 \\
        v_3
        \end{bmatrix}$
        \vspace{1em}

        The ith component can be extracted by doing the inner product with the ith standard basis:
        \[
        \begin{bmatrix}
        v_1 & v_2 & v_3
        \end{bmatrix}
        \begin{bmatrix}
        1 \\
        0 \\
        0
        \end{bmatrix}
        = v_1
        \]

        \[
        \begin{bmatrix}
        v_1 & v_2 & v_3
        \end{bmatrix}
        \begin{bmatrix}
        0 \\
        1 \\
        0
        \end{bmatrix}
        = v_2
        \]

        \[
        \begin{bmatrix}
        v_1 & v_2 & v_3
        \end{bmatrix}
        \begin{bmatrix}
        0 \\
        0 \\
        1
        \end{bmatrix}
        = v_3
        \]
        \vspace{1em}

        Therefore, analogous to x*, we can write them as the sum of the inner product times the standard basis.
        \[
        v = v_1
        \begin{bmatrix}
        1 \\
        0 \\
        0
        \end{bmatrix}
        + v_2
        \begin{bmatrix}
        0 \\
        1 \\
        0
        \end{bmatrix}
        + v_3
        \begin{bmatrix}
        0 \\
        0 \\
        1
        \end{bmatrix}
        \]
    \end{example}
    
    \subsubsection{Basic problem}
        \begin{intuition}
            Given $x\in \mathcal{V}$ and a subspace $S$. Find the closest point (in norm) in $S$ to $x$: 
            \begin{equation}
                \text{Proj}_S(x) = \arg\min_{y \in S} \| y - x \|
            \end{equation}
            \begin{itemize}
                \item $\| y - x \|$: Some norm.
                \item \textbf{Subspace:} $S$ doesn't have to be a subspace. 
                \item $\arg\min$: Vector $y$ that minimizes $\lVert x - y\rVert$
            \end{itemize}
        \end{intuition}
        
    \subsubsection{Projection onto a 1D subspace}
        \begin{derivation}
            Projection onto a 1-dimensional subspace.

            Let \( S = \text{span}(\mathbf{v}) \), and we denote the projection of \( \mathbf{x} \) onto \( S \) as:
            \[
            \text{Proj}_S(\mathbf{x}) = \mathbf{x}^*
            \]
            Under the Euclidean norm (i.e. l2 norm), we have nice geometry: we should have
            \[
            \langle \mathbf{x} - \mathbf{x}^*, \mathbf{v} \rangle = 0
            \]
            Since \( \mathbf{x}^* \in S \), \( \mathbf{x}^* = \alpha \mathbf{v} \) for some scalar \( \alpha \).

            We need to find \( \alpha \).

            So,
            \[
            \langle \mathbf{x} - \alpha \mathbf{v}, \mathbf{v} \rangle = 0
            \]
            \[
            \Rightarrow \langle \mathbf{x}, \mathbf{v} \rangle - \alpha \langle \mathbf{v}, \mathbf{v} \rangle = 0
            \]
            \[
            \Rightarrow \alpha = \frac{\langle \mathbf{x}, \mathbf{v} \rangle}{\langle \mathbf{v}, \mathbf{v} \rangle}
            \]
            Thus, 
            \[
            \mathbf{x}^* = \alpha \mathbf{v} = \frac{\langle \mathbf{x}, \mathbf{v} \rangle}{\langle \mathbf{v}, \mathbf{v} \rangle} \mathbf{v}
            \]
            which simplifies to:
            \[
            \mathbf{x}^* = \frac{\mathbf{x}^\top \mathbf{v}}{\|\mathbf{v}\|_2^2} \mathbf{v} = \left\langle \mathbf{x}, \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\rangle \frac{\mathbf{v}}{\|\mathbf{v}\|_2}
            \]
            \begin{itemize}
                \item \textbf{Orthonormal Basis for S:} \( \left\{ \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\} \) since \( \left\lVert \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\rVert_2 = 1 \)
                \item \textbf{Projection Coefficient:} $\left\langle \mathbf{x}, \frac{\mathbf{v}}{\|\mathbf{v}\|_2} \right\rangle$
                \item \textbf{Note:} $x^*$ is the point we are looking for in the projection problem.
            \end{itemize}
            \customFigure[0.5]{00_Images/Example_Projection.png}{Visual representation of the projection problem.}
        \end{derivation}

    \subsubsection{Projection onto an n dimensional space}
        \begin{derivation}
            Let \( S \) be a subspace of \( \mathcal{V} \), and let \( \{ \mathbf{v}_1, \dots, \mathbf{v}_d \} \) be an orthonormal basis of \( S \).
        \begin{enumerate}
            \item \textbf{Problem setup}
            \[
            \mathbf{x}^* = \sum_{i=1}^{d} \alpha_i \mathbf{v}_i
            \]
            \textbf{Goal:} Find \( \alpha_1, \dots, \alpha_d \) so as to minimize the norm \( \|\mathbf{x} - \mathbf{x}^*\|_2 \).

            \item \textbf{Derivation:}
            By geometry, we require that 
            \[
            \langle \mathbf{e}, \mathbf{v}_j \rangle = 0 \quad \forall \, j = 1, \dots, d
            \]
            which implies:
            \[
            \langle \mathbf{x} - \mathbf{x}^*, \mathbf{v}_j \rangle = 0 \quad \forall \, j
            \]
            \[
            \Rightarrow \langle \mathbf{x} - \sum_{i=1}^{d} \alpha_i \mathbf{v}_i, \mathbf{v}_j \rangle = 0 \quad \forall \, j
            \]
            Using linearity of the inner product:
            \[
            \Rightarrow \langle \mathbf{x}, \mathbf{v}_j \rangle = \sum_{i=1}^{d} \alpha_i \langle \mathbf{v}_i, \mathbf{v}_j \rangle
            \]
            Since \( \langle \mathbf{v}_i, \mathbf{v}_j \rangle = 0 \) if \( i \neq j \) and \( 1 \) if \( i = j \), this simplifies to:
            \[
            \alpha_j = \langle \mathbf{x}, \mathbf{v}_j \rangle \quad \text{b/c only the i=j term survives}
            \]
            Thus,
            \[
            \mathbf{x}^* = \sum_{i=1}^{d} \alpha_i \mathbf{v}_i = \sum_{i=1}^{d} \langle \mathbf{x}, \mathbf{v}_i \rangle \mathbf{v}_i
            \]

            \item \textbf{Solution:}
            \[
            = \sum_{i=1}^{d} (\mathbf{x}^\top \mathbf{v}_i) \mathbf{v}_i
            \]
            \begin{itemize}
                \item \( \mathbf{v}_i \in \mathbb{R}^n \).
                \item \textbf{Projection Coefficients:} $\mathbf{x}^\top \mathbf{v}_i$
            \end{itemize}

            \item \textbf{Example of Orthogonal Decomposition:}
            \[
            \mathbf{e} = \mathbf{x} - \mathbf{x}^* \in S^\perp, \quad \mathbf{x}^* \in S
            \]
            So, 
            \[
            \mathbf{x} = \mathbf{x}^* + \mathbf{e}, \quad \text{where} \quad \mathbf{x}^* \in S, \quad \mathbf{e} \in S^\perp
            \]
        \end{enumerate}

        \customFigure[0.5]{00_Images/General.png}{Generalization of projection.}

        \end{derivation}

    \subsubsection{Application of projections: Fourier series}
        \begin{example}
            \textbf{Fourier series:}
            \begin{enumerate}
                \item Suppose we have a periodic function $x(t)$ with period $T_0$.

                \begin{center}
                \customFigure[0.5]{00_images/function_graph.png}{Periodic triangle function.}
                \end{center}

                \item \textbf{Inner product for time domain (complex version):} $a_k = \langle x(t), y(t) \rangle = \frac{1}{T} \int_{T} x(t) \overline{y(t)} \, dt$
                \begin{itemize}
                    \item \textbf{Note:} Real version is without the conjugate.
                \end{itemize}
                \item \textbf{Projection (i.e. one component of the sum):} $\text{Proj}_{\underline{v}_i} (\underline{x}) = \langle \underline{x}, \underline{v}_i \rangle \underline{v}_i$
                
                \item \textbf{Goal:} Express $x(t)$ (i.e. any periodic function) as a sum of complex exponentials:
                
                \[
                x^{*}(t) = \sum_{k=-\infty}^{\infty} a_k e^{jk\omega_0 t}
                \]
                
                \begin{itemize}
                    \item \textbf{Projection:} $\text{Proj}_{e^{j k \omega_0 t}} \left( x(t) \right) = \left\langle x(t), \exp\left( j k \omega_0 t \right) \right\rangle e^{j k \omega_0 t} = a_k e^{j k \omega_0 t}$ for a certain value of $k$.
                    \item \textbf{Projection coefficient:} $a_k = \left\langle x(t), e^{j k \omega_0 t} \right\rangle = \frac{1}{T_0} \int_{0}^{T} x(t) e^{-jk \omega_0 t} dt$
                    \item \textbf{Fundamental frequency:} $\omega_0 = \frac{2\pi}{T_0}$.
                \end{itemize}
        
                \item \textbf{Prove orthonormal basis for the complex exponentials:} To prove it's a orthnormal basis, must prove it has unit norm 1 and each pair of vectors are orthogonal (i.e. inner product is 0).
                \begin{enumerate}
                    \item \textbf{Magnitude of exp:} $\abs{e^{j\theta}}=1$. Therefore, it has unit norm.
                    \item \textbf{Orthogonality:}
                        \[
                        \left\langle e^{j i \omega_0 t}, e^{j l \omega_0 t} \right\rangle = 
                        \begin{cases}
                        1, & i = l \\
                        0, & i \neq l
                        \end{cases}
                        \]
                        Therefore, for each pair of basis vectors, they are orthogonal.
                    \begin{itemize}
                        \item \textbf{Conjugate of exp:} $(e^{j\theta}) = e^{-j \theta}$
                    \end{itemize}
                \end{enumerate}
                \item \textbf{Conclusion:} Fourier series is a projection of a function onto the set of othonormal basis functions $\text{exp}(jk\omega_0 t)$, where $k$ is an integer.
                \begin{itemize}
                    \item \textbf{Optimal:} This projection is optimal as it minimizes the approximation error \( \| x(t) - x^*(t) \| \), i.e.
                    
                    \[
                    \frac{1}{T} \int_0^T \left( x(t) - x^*(t) \right)^2 dt
                    \]
                    As the number of terms in the summation increases to infinity, the error goes to 0. 
                \end{itemize}
            \end{enumerate}
        \end{example}

\subsection{Gram-Schmidt and QR decomposition}
    \subsubsection{What if the set of basis vectors is not orthonormal?}
    \begin{derivation}
        Let $\{u^{(1)}, \dots, u^{(d)}\} \text{ be a set of basis vectors for a subspace } S (\text{ not necessarily orthonormal})$
        \vspace{1em}

        $\text{We can still use the orthogonality principle, i.e.,}$
        \[
        e = x - x^* \perp S
        \]
        \vspace{1em}

        Therefore, 
        \[
        \langle x - x^*, u^{(j)} \rangle = 0 \quad \forall j = 1, \dots, d 
        \]
        
        $\text{Also, } x^* \in S \text{ so } x^* \text{can be written as a linear combination of basis vectors, so } x^* = \sum_{i=1}^{d} \alpha_i u^{(i)}$
        \vspace{1em}
        
        $\text{Need to find} \ \alpha_1, \dots, \alpha_d$ s.t.

        \[
        \langle x - \sum_{i=1}^{d} \alpha_i u^{(i)}, u^{(j)} \rangle = 0 \quad \forall j = 1, \dots, d
        \]
        \[
        \Rightarrow \langle x, u^{(j)} \rangle = \sum_{i=1}^{d} \alpha_i \langle u^{(i)}, u^{(j)} \rangle \quad \forall j = 1, \dots, d
        \]

        \[
        \begin{bmatrix}
        \langle u^{(1)}, u^{(1)} \rangle & \langle u^{(2)}, u^{(1)} \rangle & \dots & \langle u^{(d)}, u^{(1)} \rangle \\
        \langle u^{(1)}, u^{(2)} \rangle & \langle u^{(2)}, u^{(2)} \rangle & \dots & \langle u^{(d)}, u^{(2)} \rangle \\
        \vdots & \vdots & \ddots & \vdots \\
        \langle u^{(1)}, u^{(d)} \rangle & \langle u^{(2)}, u^{(d)} \rangle & \dots & \langle u^{(d)}, u^{(d)} \rangle
        \end{bmatrix}
        \begin{bmatrix}
        \alpha_1 \\
        \alpha_2 \\
        \vdots \\
        \alpha_d
        \end{bmatrix}
        =
        \begin{bmatrix}
        \langle x, u^{(1)} \rangle \\
        \langle x, u^{(2)} \rangle \\
        \vdots \\
        \langle x, u^{(d)} \rangle
        \end{bmatrix}
        \]

        $\text{Solve for} \ \alpha_1, \dots, \alpha_d$, $\text{Then, we get} \ x^* = \sum_{i=1}^{d} \alpha_i u^{(i)}$
        \customFigure[0.25]{00_Images/NO.png}{Not orthogonal, but similar to projection with orthonormal basis.}
        \begin{itemize}
            \item \textbf{Note:} $\text{If } \{u^{(1)}, \dots, u^{(d)}\} \text{ is an orthonormal basis, then the matrix is the identity matrix, and we get } \alpha_j = \langle x, u^{(j)} \rangle \text{ as before.}$
        \end{itemize}
    \end{derivation}

    \begin{example}
        \textbf{Function approximation.}
        Let $B$ be the set of basis functions that is not orthonormal:
        \[
        \mathcal{B} = \{1, t, \dots, t^d\}
        \]

        $\text{Let} \ x(t) \ \text{be a function over} \ [0, 1].$
        \begin{itemize}
            \item \textbf{1st Goal} Approximate $x(t) \text{ by } x^*(t) = \sum_{n=0}^{d} \alpha_n t^n$
            \item $\text{To find } \alpha_0, \alpha_1, \dots, \alpha_d$, need to solve the $Ax=b$.
            \item \textbf{2nd Goal:} Minimize the approximation error $\|x(t) - x^*(t)\|_2 = \left( \int_0^1 (x(t) - x^*(t))^2 dt \right)^{1/2}$
        \end{itemize}

        \textbf{Recall: Taylor series expansion}
        \[
        x(t) \approx x(0) + x'(0) t + \frac{x''(0)}{2} t^2 + \dots
        \]

        \begin{itemize}
            \item Taylor series expansion is completely different from the projection method, and the reason is that Taylor series expansion is a local approximation.
        \end{itemize}

    \end{example}

    \subsubsection{Gram-Schmidt Procedure}
    \textbf{Motivation:} This is to get an orthonormal basis, so we can use the easier projection method.
    \begin{intuition}
        Another way to find the projection of \(x\) onto $S = \text{span} \{u^{(1)}, \dots, u^{(d)}\}$ is to first find an orthonormal basis of \(S\), and then the projection problem becomes easier.
        \customFigure[0.75]{00_Images/GS_2D.png}{Gram-Schmidt Process for 2D.}
        \begin{enumerate}
            \item Normalize $u^{(1)}$ 
            \item Find the error vector by projecting $u^{(2)}$ onto the subspace $v^{(1)}$.
            \item Normalize the error vector.
            \item Now you have two vectors that form an orthonormal basis in 2D.
        \end{enumerate}
    \end{intuition}

    \begin{definition}
        Turns any set of basis vectors of a subspace into an \textbf{orthonormal} set of basis vectors.
    \end{definition}

    \begin{process}
        \begin{enumerate}
            \item Normalize \( u^{(1)} \) to get \( v^{(1)} \):
            \[
            v^{(1)} = \frac{u^{(1)}}{\|u^{(1)}\|_2}
            \]
        
            \item 
            \begin{enumerate}
                \item Project \( u^{(2)} \) onto \( S = \text{span}\{v^{(1)}\} \) to get:
                \[
                w^{(2)} = \langle u^{(2)}, v^{(1)} \rangle v^{(1)}
                \]
        
                \item Set:
                \[
                v^{(2)} = \frac{u^{(2)} - w^{(2)}}{\|u^{(2)} - w^{(2)}\|_2}
                \]
            \end{enumerate}
        
            \item Continue similarly:
            \begin{enumerate}
                \item Project \( u^{(3)} \) onto \( S = \text{span}\{v^{(1)}, v^{(2)}\} \) to get:
                \[
                w^{(3)} = \langle u^{(3)}, v^{(1)} \rangle v^{(1)} + \langle u^{(3)}, v^{(2)} \rangle v^{(2)}
                \]
        
                \item Set:
                \[
                v^{(3)} = \frac{u^{(3)} - w^{(3)}}{\|u^{(3)} - w^{(3)}\|_2}
                \]
            \end{enumerate}
            
            \item Continue this process for higher dimensions. Therefore, \( \{v^{(1)}, \dots, v^{(d)}\} \) is an orthonormal basis for \( \text{span}\{u^{(1)}, \dots, u^{(d)}\} \).
        
        \end{enumerate}
    \end{process}

    \begin{intuition}
        \begin{itemize}
            \item Create the Gram matrix by making an orthonormal basis.
            \item Then this terms the matrix into the identity.
        \end{itemize}
    \end{intuition}

    \begin{warning}
        Not every set of vectors in $\mathbb{R}^n$ turns into the standard basis. It depends on how you apply the Gram-Schmidt. 
        \begin{itemize}
            \item For example if you have the vectors $(1,0)$ and $(1,1)$. 
            \item If you apply Gram-Schmidt starting with $(1,0)$, then you will get the standard basis. 
            \item But if you apply Gram-Schmidt starting with $(1,1)$, then you will get $(1/\sqrt{2},1/\sqrt{2}),(1/\sqrt{2},1/\sqrt{2})$.
        \end{itemize}
    \end{warning}

    \subsubsection{QR decomposition}
    Another way to see Gram-Schmidt procedure is through matrix multiplication.
    \begin{definition}
        $\text{Stack all } u^{(i)} \text{ vectors as columns of a matrix}$

        \begin{align*}
            \begin{bmatrix}
            u^{(1)} & \cdots & u^{(d)}
            \end{bmatrix}
            &= QR \\
            \begin{bmatrix}
            u^{(1)} & \cdots & u^{(d)}
            \end{bmatrix}
            &=
            \begin{bmatrix}
            v^{(1)} & \cdots & v^{(d)}
            \end{bmatrix}
            \begin{bmatrix}
            r_{11} & r_{12} & \cdots & r_{1d} \\
            0      & r_{22} & \cdots & r_{2d} \\
            \vdots &        & \ddots & \vdots \\
            0      & \cdots & 0      & r_{dd}
            \end{bmatrix} \\
            &=
            \begin{bmatrix}
            r_{11}v^{(1)} & r_{12}v^{(1)} + r_{22}v^{(2)} & \cdots
            \end{bmatrix}
        \end{align*}        

        \begin{itemize}
            \item \( Q \): Orthonomral matrix (i.e., its columns are orthogonal to each other and have unit norm) 
            \item \( R \): Upper triangular.
        \end{itemize}        
    \end{definition}
    
    \begin{intuition}
        For $Ax=b$, therefore, 
        \begin{equation*}
            QRx=b
        \end{equation*}
        \begin{itemize}
            \item Since $Q$ has columns of orthonormal basis, then it has an inverse, which is $Q^{-1} = Q^T$. 
        \end{itemize}
        \vspace{1em}

        Then, 
        \begin{equation*}
            Rx=Q^T b
        \end{equation*}
    \end{intuition}

    \begin{example}
        \[
        \{1, t, t^2, \cdots, t^d\}
        \]
        is \textit{not an orthonormal basis}, which as an example is defined from $[0,1]$
        \vspace{1em}

        The \( L^2 \)-norm for this example is given by
        \[
        \|f\|_{2} = \left( \int_0^1 f^2(t) \, dt \right)^{\frac{1}{2}}.
        \]
        \vspace{1em}

        The inner product between two functions \( f(t) \) and \( g(t) \) is defined as:
        \[
        \langle f, g \rangle = \int_0^1 f(t) g(t) \, dt.
        \]

        \begin{enumerate}                
            \item Start with \( u^{(1)} = 1 \), which is equivalent to \( v^{(1)} \) because it's unit norm.
        
            \item For \( u^{(2)} \), calculate the projection:
            \[
            \omega^{(2)} = \text{Proj}_{\text{span}\{v^{(1)}\}} u^{(2)} = \langle u^{(2)}, v^{(1)} \rangle = \int_0^1 t \cdot 1 \, dt = \frac{1}{2}..
            \]
            
        
            So, the projection of \( u^{(2)} \) onto \( u^{(1)} \) is:
            \[
            \frac{1}{2} v^{(1)}.
            \]
            
            \item Now subtract the projection from \( u^{(2)} \) and normalize:
            \[
            v^{(2)} = \frac{u^{(2)} - \omega^{(2)}}{\|u^{(2)} - \omega^{(2)}\|_{2}} = \frac{t - \frac{1}{2}}{\left( \int_0^1 \left(t - \frac{1}{2}\right)^2 dt \right)^{\frac{1}{2}}}.
            \]
            
        \end{enumerate}
    \end{example}

\subsection{Projection of a subspace defined by its orthogonal vectors}
    \subsubsection{Subspace defined by its orthogonal vectors}
    \begin{intuition}
        \begin{enumerate}

            \item So far, we have defined a subspace by its basis vectors:
            \[
            S = \text{span}\{v^{(1)}, \dots, v^{(d)}\}.
            \]
            
            \item But, in many cases, we can define \( S \) in terms of the set of vectors that are orthogonal to it. 
        \end{enumerate}
    \end{intuition}
    
    \begin{definition}
        \begin{equation*}
            S = \left\{ x \mid \left( a^{(i)} \right)^T x = 0, \, i = 1, \dots, m \right\}
        \end{equation*}
        then the vectors \( a^{(1)}, \dots, a^{(m)} \) are orthogonal to all vectors in \( S \) (i.e. the inner products are $0$ for all vectors $x$ with $a^{(i)}$). Therefore, $S^\perp = \text{span} \{ a^{(1)}, \dots, a^{(m)} \}$.
    \end{definition}

    \subsubsection{Projection}
    \begin{derivation}
        \begin{enumerate}
            \item Projecting a vector $x$ onto a subspace $S$ spanned by the vectors $\{a^{(1)}, \dots, a^{(m)}\}$. 
            The projection $x^*$ is given by:
            \[
            x^* = \text{Proj}_S(x) = \arg \min_{y \in S} \| x - y \|_2
            \]
            \customFigure[0.5]{00_Images/OS.png}{Projection onto a subspace defined by its orthogonal vectors}

            \item Using the orthogonality principle, the error $e = x - x^*$ must be orthogonal to the subspace $S$, i.e.,
            \[
            e \perp S
            \]
            This implies that:
            \[
            e \in \text{span}\{a^{(1)}, \dots, a^{(m)}\}
            \]
            
            \item The error can be written as a linear combination of the basis vectors:
            \[
            e = x - x^* = \sum_{i=1}^{m} \beta_i a^{(i)}
            \]
            We need to find the coefficients $\beta_1, \dots, \beta_m$.
            
            \item Since $x^* \in S$, we have the condition:
            \[
            \langle x^*, a^{(j)} \rangle = 0 \quad \forall j = 1, \dots, m
            \]
            which leads to the following equation:
            \[
            (a^{(j)})^T x^* = 0 \quad \forall j = 1, \dots, m
            \]
            
            \item Substituting $x^* = x - \sum_{i=1}^{m} \beta_i a^{(i)}$ into the above equation, we get:
            \[
            (a^{(j)})^T \left( x - \sum_{i=1}^{m} \beta_i a^{(i)} \right) = 0 \quad \forall j
            \]
            
            \item Expanding the terms using linearity in the first argument for inner products:
            \[
            (a^{(j)})^T x = \sum_{i=1}^{m} \beta_i (a^{(j)})^T a^{(i)}
            \]
            This system of equations can be written in matrix form as:
            \[
            \begin{bmatrix}
            (a^{(1)})^T a^{(1)} & \cdots & (a^{(1)})^T a^{(m)} \\
            \vdots & \ddots & \vdots \\
            (a^{(m)})^T a^{(1)} & \cdots & (a^{(m)})^T a^{(m)}
            \end{bmatrix}
            \begin{bmatrix}
            \beta_1 \\
            \vdots \\
            \beta_m
            \end{bmatrix}
            =
            \begin{bmatrix}
            (a^{(1)})^T x \\
            \vdots \\
            (a^{(m)})^T x
            \end{bmatrix}
            \]
            We can solve this system of linear equations to obtain the values of $\beta_1, \dots, \beta_m$.
            
            \item Once we have the values of $\beta_i$, we can compute the projection as:
            \[
            x^* = x - \sum_{i=1}^{m} \beta_i a^{(i)}
            \]
            
            \item \textbf{Note:} If the set $\{a^{(i)}\}$ is orthonormal, the matrix on the left-hand side becomes the identity matrix $I$, and the coefficients simplify to:
            \[
            \beta_j = (a^{(j)})^T x = \langle x, a^{(j)} \rangle
            \]
        \end{enumerate}
    \end{derivation}

\subsection{Projection onto affine sets}
\subsubsection{Affine spaces}
\begin{definition}
    An affine space (or affine set) is a translation (or shift) of a subspace $S$.
\end{definition}

\begin{example}
    Consider a vector $x^{(0)}$ (not necessarily in $S$). The affine space $\mathcal{A}$ is defined as:
    \[
    \mathcal{A} = \{ u + x^{(0)} \mid u \in S \}
    \]
    where $x^{(0)}$ is the shifting vector and $S$ is the original subspace. This represents a shifted version of the subspace $S$.

    \customFigure[0.5]{00_Images/AS.png}{Affine space of a 2D space, where the $x^{(0)}$ is the constant (i.e. shifting origin to the Affine set) that we are adding to shift all the vectors to the affine space.}
\end{example}

\subsubsection{Projection of Affine space defined in terms of basis vectors of corresponding subspace}
\begin{derivation}
    \begin{enumerate}
        \item The affine space is described by:
        \[
        \mathcal{A} = \left\{ x \mid x = \sum_{i=1}^{d} \alpha_i v^{(i)} + c \right\}
        \]
        \begin{itemize}
            \item $\{v^{(1)}, \dots, v^{(d)}\}$: Basis vectors of the subspace $S$
            \item $c$: Vector (i.e. shift).
        \end{itemize}
        \customFigure[0.5]{00_Images/PR.png}{Projection problem visualization, where we are projecting the vector onto the Affine space.}
    
        \item Using the orthogonality principle, we must have:
        \[
        \langle x - x^*, v^{(j)} \rangle = 0 \quad \forall j = 1, \dots, d
        \]
        where $x^* \in \mathcal{A}$. Therefore:
        \[
        x^* = \sum_{i=1}^{d} \alpha_i v^{(i)} + c
        \]
    
        \item This leads to the condition:
        \[
        \left\langle x - \sum_{i=1}^{d} \alpha_i v^{(i)} - c, v^{(j)} \right\rangle = 0 \quad \forall j = 1, \dots, d
        \]
    
        \item Simplifying this expression using the linearity in first argument for inner product, we obtain:
        \[
        \langle x - c, v^{(j)} \rangle = \sum_{i=1}^{d} \alpha_i \langle v^{(i)}, v^{(j)} \rangle \quad \forall j = 1, \dots, d
        \]
    
        \item To solve for $\alpha_1, \dots, \alpha_d$, we set up the following system of linear equations in matrix form:
        \[
        \begin{bmatrix}
        \langle v^{(1)}, v^{(1)} \rangle & \cdots & \langle v^{(1)}, v^{(d)} \rangle \\
        \vdots & \ddots & \vdots \\
        \langle v^{(d)}, v^{(1)} \rangle & \cdots & \langle v^{(d)}, v^{(d)} \rangle
        \end{bmatrix}
        \begin{bmatrix}
        \alpha_1 \\
        \vdots \\
        \alpha_d
        \end{bmatrix}
        =
        \begin{bmatrix}
        \langle x - c, v^{(1)} \rangle \\
        \vdots \\
        \langle x - c, v^{(d)} \rangle
        \end{bmatrix}
        \]
        \begin{itemize}
            \item \textbf{Note:} We are projecting onto $x-c$, which we can see on the RS, which is the subspace.
        \end{itemize}
    
        \item Solving this system gives us the values for $\alpha_1, \dots, \alpha_d$. Finally, the projection $x^*$ onto the affine space $\mathcal{A}$ is:
        \[
        x^* = \sum_{i=1}^{d} \alpha_i v^{(i)} + c
        \]
        \textbf{Note:} We are projecting onto $x-c$ (i.e. subspace), then adding the shift into the end to be back on the Affine set.
    \end{enumerate}
\end{derivation}

\subsubsection{Projection of Affine space defined in terms of orthogonal vectors to corresponding subspace}
\begin{derivation}
    \begin{enumerate}
        \item The affine set $\mathcal{A}$ is defined as:
        \[
        \mathcal{A} = \left\{ x \mid \langle x, a^{(i)} \rangle = d_i, \; i = 1, \dots, m \right\}
        \]
        \begin{itemize}
            \item $d_i$: Scalars
            \item $\{a^{(1)}, \dots, a^{(m)}\}$: A set of vectors spanning the affine space. (Check why this is equivalent to the previous definition of an affine set.)
        \end{itemize}

        \customFigure[0.5]{00_Images/PR1.png}{Projection problem visualization, where we are projecting the vector onto the affine space, which is in line with the orthogonal vectors.}
        
        \item Since $x-x^*$ lies in the span of $\{a^{(1)}, \dots, a^{(m)}\}$:
        \[
        x - x^* = \sum_{i=1}^{m} \beta_i a^{(i)}
        \]
        where $\beta_1, \dots, \beta_m$ are the coefficients to be determined.
        
        \item Since $x^* \in \mathcal{A}$, we also have:
        \[
        \langle x^*, a^{(j)} \rangle = d_j \quad \forall j = 1, \dots, m
        \]
        This implies the orthogonality condition for the projection:
        \[
        \langle x - \sum_{i=1}^{m} \beta_i a^{(i)}, a^{(j)} \rangle = d_j \quad \forall j = 1, \dots, m
        \]
    
        \item Expanding the above expression using the linearity in first argument for inner product, we get:
        \[
        \langle x, a^{(j)} \rangle - \sum_{i=1}^{m} \beta_i \langle a^{(i)}, a^{(j)} \rangle = d_j \quad \forall j = 1, \dots, m
        \]
        
        \item This leads to the system of linear equations:
        \[
        \langle x, a^{(j)} \rangle - d_j = \sum_{i=1}^{m} \beta_i \langle a^{(i)}, a^{(j)} \rangle \quad \forall j
        \]
        
        \item We now solve this system of linear equations for the coefficients $\beta_1, \dots, \beta_m$. The system can be written in matrix form as:
        \[
        \begin{bmatrix}
        \langle a^{(1)}, a^{(1)} \rangle & \cdots & \langle a^{(1)}, a^{(m)} \rangle \\
        \vdots & \ddots & \vdots \\
        \langle a^{(m)}, a^{(1)} \rangle & \cdots & \langle a^{(m)}, a^{(m)} \rangle
        \end{bmatrix}
        \begin{bmatrix}
        \beta_1 \\
        \vdots \\
        \beta_m
        \end{bmatrix}
        =
        \begin{bmatrix}
        \langle x, a^{(1)} \rangle - d_1 \\
        \vdots \\
        \langle x, a^{(m)} \rangle - d_m
        \end{bmatrix}
        \]
        
        \item Solving this system gives the values for $\beta_1, \dots, \beta_m$. Once the $\beta_i$ values are known, the projection $x^*$ is given by:
        
        \[
        x^* = x - \sum_{i=1}^{m} \beta_i a^{(i)} 
        \]
        \begin{itemize}
            \item \textbf{Intuition:} This is subtracting the orthogonal components of $x$ (i.e. removing the error vector) to get $x$ in the subspace.
        \end{itemize}
    \end{enumerate}            
\end{derivation}

\begin{example}
    \begin{enumerate}
        \item Consider the case where \( m = 1 \). The affine set \( \mathcal{A} \) is defined as:
        \[
        \mathcal{A} = \{ x \mid a^T x = d \}
        \]
        where \( a \) is a vector and \( d \) is a scalar.
    
        \item To project \( x \) onto the affine subspace, we start by using the orthogonality condition:
        \[
        \langle x, a \rangle - d = \beta \langle a, a \rangle
        \]
        This ensures that the difference between \( x \) and its projection \( x^* \) lies in the direction of \( a \).
    
        \item Solving for \( \beta \), we get:
        \[
        \beta = \frac{\langle x, a \rangle - d}{\langle a, a \rangle} = \frac{a^T x - d}{\| a \|_2^2}
        \]
        This provides the scalar \( \beta \), which tells us how much of the vector \( a \) needs to be subtracted from \( x \).
    
        \item The projection \( x^* \) onto the affine subspace is then:
        \[
        x^* = x - \beta a = x - \left( \frac{a^T x - d}{\| a \|_2^2} \right) a
        \]
        This gives the final expression for the projection of \( x \) onto the affine set \( \mathcal{A} \).
    \end{enumerate}            
\end{example}

\subsubsection{Show that the two affine sets are equal}
\begin{derivation}
    We define set \( A \) as follows:

    \[
    A = \left\{ x \in \mathbb{R}^n \ \middle|\ x = \sum_{i=1}^d \alpha_i v^{(i)} + c \right\}
    \]
    where \( v^{(i)} \) are the basis vectors, \( \alpha_i \) are scalar coefficients, and \( c \) is a fixed vector (the translation vector of the affine set).

    Now, we define set \( B \) as:

    \[
    B = \left\{ x \in \mathbb{R}^n \ \middle|\ a^{(i)T} x = d_i, \, i = 1, 2, \dots, m \right\}
    \]
    where \( a^{(i)} \) are orthogonal vectors, and \( d_i \) are scalars defining the affine constraints.

    \textbf{Step 1: Show \( A \subseteq B \).}

    Assume \( x \in A \), then we can write:
    \[
    x = \sum_{i=1}^d \alpha_i v^{(i)} + c
    \]
    Substitute this into the condition for set \( B \):
    \[
    a^{(i)T} x = a^{(i)T} \left( \sum_{j=1}^d \alpha_j v^{(j)} + c \right)
    \]
    Expanding the expression:
    \[
    a^{(i)T} x = \sum_{j=1}^d \alpha_j a^{(i)T} v^{(j)} + a^{(i)T} c
    \]
    Since the vectors \( a^{(i)} \) are orthogonal to the vectors \( v^{(j)} \), we have:
    \[
    a^{(i)T} v^{(j)} = 0 \quad \text{for all} \ i, j
    \]
    Therefore, the equation simplifies to:
    \[
    a^{(i)T} x = a^{(i)T} c
    \]
    We define \( d_i = a^{(i)T} c \). Hence,
    \[
    a^{(i)T} x = d_i \quad \text{for all} \ i
    \]
    Thus, \( x \in B \).

    \textbf{Step 2: Show \( B \subseteq A \).}

    Assume \( x \in B \), then we know:
    \[
    a^{(i)T} x = d_i \quad \text{for all} \ i = 1, 2, \dots, m
    \]
    This implies that the vector \( x \) satisfies all the affine constraints defined by the vectors \( a^{(i)} \) and scalars \( d_i \). Now, consider the vector \( c \) such that:
    \[
    a^{(i)T} c = d_i
    \]
    Subtracting this from the affine constraint for \( x \), we get:
    \[
    a^{(i)T} (x - c) = 0 \quad \text{for all} \ i
    \]
    This shows that \( x - c \) lies in the null space of the vectors \( a^{(i)} \). Therefore, \( x - c \) must lie in the span of the vectors \( v^{(i)} \), meaning:
    \[
    x - c = \sum_{i=1}^d \alpha_i v^{(i)}
    \]
    for some scalars \( \alpha_1, \alpha_2, \dots, \alpha_d \). Thus,
    \[
    x = \sum_{i=1}^d \alpha_i v^{(i)} + c
    \]
    Therefore, \( x \in A \).

    \textbf{Conclusion:}

    Since we have shown that \( A \subseteq B \) and \( B \subseteq A \), we conclude that:
    \[
    A = B
    \]
    This proves that the definition of the affine set in terms of orthogonal vectors and the definition in terms of basis vectors are equivalent.
\end{derivation}

\subsection{Summary}
\begin{definition}

    \textbf{Subspace:}
    \[
    S = \left\{ \mathbf{x} \ \middle| \ \mathbf{x} = \sum_{i=1}^{d} \alpha_i \mathbf{v}^{(i)} \right\}
    \]
    \[
    S = \left\{ \mathbf{x} \ \middle| \ (\mathbf{a}^{(i)})^\top \mathbf{x} = 0 , \ i=1, \dots, m \right\}
    \]

    \textbf{Affine Space:}
    \[
    A = \left\{ \mathbf{x} \ \middle| \ \mathbf{x} = \sum_{i=1}^{d} \alpha_i \mathbf{v}^{(i)} + \mathbf{c} \right\}
    \]
    \[
    A = \left\{ \mathbf{x} \ \middle| \ (\mathbf{a}^{(i)})^\top \mathbf{x} = d_i , \ i=1, \dots, m \right\}
    \]
\end{definition}

\begin{process}
    You have three types of Gram Matrix that you can have, which can all be used to solve the projection problems.
    \begin{itemize}
        \item If you have a set of vectors that are linearly independent, then Gram matrix (i.e. the matrix in every projection problem) is invertible.
        \item If Gram matrix is orthogonal, then Gram Matrix is diagonal matrix (i.e. identity scaled by some factor)
        \item If Gram matrix is orthonormal, you have an identity matrix.
    \end{itemize}
\end{process}
        
\subsection{Hyperplanes and half-spaces}
\begin{definition}

    \begin{itemize}
        \item \textbf{Hyperplane:} an affine space for the special case \( m = 1 \).
            \begin{equation}
                \mathcal{H} = \left\{ \mathbf{x} \ \middle| \ \mathbf{a}^\top \mathbf{x} = b \right\}
            \end{equation}
    
        \item \textbf{Half-space:}
            \begin{equation}
                \mathcal{H}_{+} = \left\{ \mathbf{x} \ \middle| \ \mathbf{a}^\top \mathbf{x} \geq b \right\}
            \end{equation}
            \begin{equation}
                \mathcal{H}_{-} = \left\{ \mathbf{x} \ \middle| \ \mathbf{a}^\top \mathbf{x} \leq b \right\}
            \end{equation}
            \begin{itemize}
                \item $\mathbf{x} \in \mathcal{H}_{+} \implies \text{angle between } \mathbf{a} \text{ and } \mathbf{x} \text{ is acute.}$
                \item $\mathbf{x} \in \mathcal{H}_{-} \implies \text{angle between } \mathbf{a} \text{ and } \mathbf{x} \text{ is obtuse.}$
            \end{itemize}
    \end{itemize}

    \customFigure[0.5]{00_Images/HPHS.png}{2D Hyperplane which is the line then the half space, which is separating the line into the above and below b.}
\end{definition}

\begin{intuition}
    \begin{itemize}
        \item In 2D: A hyperplane is a straight line dividing the plane into two regions.
        \item In 2D: A half-space is the region on one side of a line, which describes all points on one side of the line.
        \item In 3D: A hyperplane is a flat plane dividing the space into two regions.
        \item In 3D: A half-space is the region on one side of a plane, which includes all points on one side of the plane.
    \end{itemize}
\end{intuition}

