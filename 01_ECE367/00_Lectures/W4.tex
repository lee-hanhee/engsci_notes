\subsection{Matrices}
\begin{definition}
    Matrices are two-dimensional arrays of numbers. A general matrix \( A \) is denoted as:
    \begin{equation}
        A = \begin{bmatrix}
            a_{11} & \cdots & a_{1n} \\
            \vdots & \ddots & \vdots \\
            a_{m1} & \cdots & a_{mn}
            \end{bmatrix} 
            = \begin{bmatrix}
                a^{(1)} & \cdots & a^{(n)}
                \end{bmatrix}
            \in \mathbb{R}^{m \times n} \quad \text{(or $\mathbb{C}^{m \times n}$ for complex numbers)}
    \end{equation}
\end{definition}

    \subsubsection{Matrix Transpose}
    \begin{definition}
        Given a matrix \( A \) with columns \( a^{(i)} \), its transpose \( A^\top \) is:
        \begin{equation}
            A^\top = \begin{bmatrix}
                (a^{(1)})^\top \\
                \vdots \\
                (a^{(n)})^\top
                \end{bmatrix} \in \mathbb{R}^{n \times m}
        \end{equation}
        \begin{itemize}
            \item \textbf{Note:} $(A^T A)^T = A^T A$
        \end{itemize}
    \end{definition}

    \subsubsection{Matrix Multiplication}
    \begin{definition}
        The multiplication of \( A^\top \) and a matrix \( B \) is given by:
        \begin{equation}
            A^\top B = \begin{bmatrix}
                (a^{(1)})^\top \\
                \vdots \\
                (a^{(n)})^\top
                \end{bmatrix}
                \begin{bmatrix}
                b^{(1)} & \cdots & b^{(n)}
                \end{bmatrix}
                = \begin{bmatrix}
                (a^{(1)})^\top b^{(1)} & \cdots & (a^{(1)})^\top b^{(n)} \\
                \vdots & \ddots & \vdots \\
                (a^{(n)})^\top b^{(1)} & \cdots & (a^{(n)})^\top b^{(n)}
                \end{bmatrix} \in \mathbb{R}^{n \times n}
        \end{equation}

        For regular matrix multiplication \( AB \), we have:
        \begin{equation}
            AB = \begin{bmatrix}
                a^{(1)} & \cdots & a^{(n)}
                \end{bmatrix}
                \begin{bmatrix}
                (b^{(1)})^\top \\
                \vdots \\
                (b^{(n)})^\top
                \end{bmatrix}
                = \sum_{i=1}^{n} a^{(i)} (b^{(i)})^\top \in \mathbb{R}^{m \times m}
        \end{equation}
        (Note: Try to think why this is the same as before.)
    \end{definition}

    \subsubsection{Matrix vector multiplication}
    \begin{definition}
        \begin{equation*}
            Ax = \begin{bmatrix}
                a_{11} & \cdots & a_{1n} \\
                \vdots & \ddots & \vdots \\
                a_{m1} & \cdots & a_{mn} 
            \end{bmatrix}
            \begin{bmatrix}
                    x_{1} \\
                    \vdots \\ 
                    x_{n}
            \end{bmatrix}
            = \sum_{i=1}^{n} x_i a^{(i)} \in \mathbb{R}^{m \times 1}
        \end{equation*}
    \end{definition}

    \subsubsection{Block Matrix Product}
    \begin{definition}
        For a block matrix product:
        \begin{equation}
            \begin{bmatrix}
                A & B
                \end{bmatrix}
                \begin{bmatrix}
                X \\ Y
                \end{bmatrix}
                = AX + BY
        \end{equation}
        (Analogous to vectors: \( [a \ b] \begin{bmatrix} x \\ y \end{bmatrix} = a x + b y \))
    \end{definition}

    \subsubsection{Types of Matrices}
    \begin{definition}
        \begin{itemize}
            \item \textbf{Square matrix}: \(m = n\)
            \item \textbf{Diagonal matrix}: \(a_{ij} = 0 \quad \forall i \neq j\)
            \item \textbf{Identity matrix}: $a_{ij} = 0 \; \forall i \neq j, \; a_{ii} = 1 \; \forall i$
            \item \textbf{Triangular matrix}: $A=QR$ ($R$ is upper triangular)
            \item \textbf{Orthogonal matrix}: \(A^\top A = A A^\top = cI\) 
            \item \textbf{Orthonormal matrix}: \(A^\top A = A A^\top = I\) (i.e. inverse of A is $A^T$)
            \item \textbf{Symmetric matrix}: \(A = A^\top\)
        \end{itemize}
    \end{definition}

    \subsubsection{Matrices as Linear Maps}
    \begin{definition}
        Matrices are used as linear maps, where:
        \[
        y = A x
        \]
        \[
        \begin{bmatrix} y \end{bmatrix} \in \mathbb{R}^m \quad = \quad \begin{bmatrix} A \end{bmatrix} \in \mathbb{R}^{m \times n} \quad \begin{bmatrix} x \end{bmatrix} \in \mathbb{R}^n
        \]
        This represents a \textbf{linear function}.
        \vspace{1em}

        For an \textbf{affine function}, the equation becomes:
        \[
        y = A x + b
        \]
    \end{definition}

    \begin{intuition}
        Linear functions imply a linear mapping. Let $\mathbf{v} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$.

        \begin{enumerate}
            \item $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$
            \[
            A \mathbf{v} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
            \]
            \begin{itemize}
                \item Geometric Interpretation: $A$ leaves the vector $\mathbf{v}$ unchanged (identity transformation).
            \end{itemize}
        
            \item $A = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$
            \[
            A \mathbf{v} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
            \]
            \begin{itemize}
                \item Geometric Interpretation: $A$ swaps the $x$ and $y$ components of the vector, flip it over the line $y=x$.
            \end{itemize}
        
            \item $A = \begin{bmatrix} \frac{1}{\sqrt{2}} & 0 \\ 0 & 1 \end{bmatrix}$
            \[
            A \mathbf{v} = \begin{bmatrix} \frac{1}{\sqrt{2}} & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{\sqrt{2}} \\ 1 \end{bmatrix}
            \]
            \begin{itemize}
                \item Geometric Interpretation: $A$ scales the $x$ component of $\mathbf{v}$ by a factor of $\frac{1}{\sqrt{2}}$ while leaving the $y$ component unchanged.
            \end{itemize}
        
            \item $A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$
            \[
            A \mathbf{v} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}
            \]
            \begin{itemize}
                \item Geometric Interpretation: $A$ rotates the vector $\mathbf{v}$ by $90^\circ$ counterclockwise.
            \end{itemize}
        
            \item $A = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}$
            \[
            A \mathbf{v} = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} \cos \theta - \sin \theta \\ \sin \theta + \cos \theta \end{bmatrix}
            \]
            \begin{itemize}
                \item Geometric Interpretation: $A$ rotates the vector $\mathbf{v}$ by an angle $\theta$ counterclockwise.
            \end{itemize}
        
            \item $A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}$
            \[
            A \mathbf{v} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}
            \]
            \begin{itemize}
                \item Geometric Interpretation: $A$ shears the vector $\mathbf{v}$, combining both components into the $x$-coordinate while leaving the $y$-coordinate unchanged.
            \end{itemize}
        
        \end{enumerate}        
    \end{intuition}

\subsection{Range Space}
    \begin{definition}
        \begin{align}
            \mathcal{R}(A) &= \{Ax \mid x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m \\
            &= \left\{ \sum_{i=1}^{n} x_i a^{(i)} \mid x \in \mathbb{R}^n \right\} \\ 
            &= \text{span} \{a^{(1)}, a^{(2)}, \dots, a^{(n)}\}
        \end{align}
        \begin{itemize}
            \item $\mathcal{R}(A) \text{ is a subspace (closed under addition and scalar multiplication).}$
            \item \textbf{Note:} $A$ can consist of different column vectors that still span the space. E.g. In 2D, $(1,0)$ and $(0,1)$ can be used, but $(1,1)$ and $(1,0)$ can be used as well, which would have different $A$. 
            \item \textbf{Note:} $R(A) = R(A^T)$ iff $A=A^T$ (i.e. symmetric).
        \end{itemize}

        \customFigure[0.25]{00_Images/RS.png}{Range space.}
    \end{definition}

\subsection{Null Space}
    \begin{definition} 
    Set of all vectors that map to zero.

    \begin{equation}
        \mathcal{N}(A) = \{x \in \mathbb{R}^n \mid Ax = 0\} \subseteq \mathbb{R}^n
    \end{equation}

    \begin{itemize}
        \item $\mathcal{N}(A) \text{ is also a subspace (closed under addition and scalar multiplication).}$
    \end{itemize}
    \end{definition}

\subsection{Fundamental theorem of algebra}
\begin{definition}
    $\text{For any matrix } A \in \mathbb{R}^{m \times n}$, the subspaces are orthogonal to each other (i.e. orthogonal complement)
    \begin{equation}
        \mathcal{R}(A) \perp \mathcal{N}(A^T)
    \end{equation}

    \begin{equation}
    \mathcal{R}(A^T) \perp \mathcal{N}(A)
    \end{equation}

    Therefore, since $S \oplus S^\perp = V$, then the direct sum of the two subspaces (i.e. if you take one vector from one subspace, and another vector from the other subspace, the sum of these vectors would be in $\mathbb{R}^n$ or $\mathbb{R}^m$)

    \begin{equation}
    \mathcal{R}(A) \oplus \mathcal{N}(A^T) = \mathbb{R}^m
    \end{equation}
    \begin{itemize}
        \item $\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A^T)) = m$
    \end{itemize}

    \begin{equation}
    \mathcal{R}(A^T) \oplus \mathcal{N}(A) = \mathbb{R}^n
    \end{equation}
    \begin{itemize}
        \item $\dim(\mathcal{R}(A^T)) + \dim(\mathcal{N}(A)) = n$
    \end{itemize}
\end{definition}

\begin{derivation}
    \textbf{Why Does This Hold?} Recall the definition of orthogonal complement:

    \begin{itemize}
        \item $\forall x \in S, \, y \in S^\perp$, we have that $\langle x, y \rangle = 0$
        \item $x \in V$ can be decomposed as $x = x^* + v$, where $x^* \in S$ and $v \in S^\perp$
    \end{itemize}

    Therefore, 
    \begin{equation*}
        \dim V = \dim S + \dim S^\perp
    \end{equation*}
    \customFigure[0.5]{00_Images/OC.png}{Orthogonal complement.}
    \vspace{1em}

    Now using this definition of orthogonal complement, let's apply it to the range space and null space of A:
    Consider 
    \begin{equation*}
        \mathcal{R}(A) = \left\{ y \mid y = \sum_{i=1}^{n} \alpha_i a^{(i)} \right\}
    \end{equation*}

    \begin{equation*}
        \mathcal{N}(A^\top) = \left\{ x \mid \left(a^{(i)}\right)^\top x = 0, \ i = 1, \ldots, n \right\}
    \end{equation*}

    \[
    A^\top = \begin{bmatrix}
    (a^{(1)})^\top \\
    \vdots \\
    (a^{(n)})^\top
    \end{bmatrix}
    \]

    Consider $y \in \mathcal{R}(A)$ so $y = \sum_{i=1}^{n} \alpha_i a^{(i)}$, and $x \in \mathcal{N}(A^\top)$,

    \[
    \langle y, x \rangle = \left\langle \sum_{i=1}^{n} \alpha_i a^{(i)}, x \right\rangle = \sum_{i=1}^{n} \alpha_i \langle a^{(i)}, x \rangle
    \]

    Since $x \in \mathcal{N}(A^\top)$, this implies that $\langle a^{(i)}, x \rangle = 0$, so:

    \[
    \langle y, x \rangle = 0
    \]

    Thus, any $y \in \mathcal{R}(A)$ is orthogonal to any $x \in \mathcal{N}(A^\top)$.
    \vspace{1em}

    Therefore, $\mathcal{R}(A) \oplus \mathcal{N}(A^\top) = \mathbb{R}^m$, and $\dim(\mathcal{R}(A)) + \dim(\mathcal{N}(A^\top)) = m$.

\end{derivation}

\begin{example}
    \customFigure[0.5]{00_Images/FTA.png}{Example of fundamental theorem of linear algebra.}
    \begin{itemize}
        \item Given the range of A, we can find the null space of A by finding what is orthogonal to it. 
    \end{itemize}
\end{example}

\subsubsection{Rank of A}
\begin{definition}
    \begin{equation*}
        \text{Rank}(A) = \dim(\mathcal{R}(A^T)) = \dim(\mathcal{R}(A)) 
    \end{equation*}
    \begin{itemize}
        \item i.e. number of independent rows or independent columns. 
    \end{itemize}
\end{definition}

\subsection{Determinant}
\begin{definition}
    \begin{equation}
    A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}
    \end{equation}

    \begin{equation}
    \det(A) = a_{11}a_{22} - a_{12}a_{21}
    \end{equation}

    The volume of the region transformed by matrix $A$ is given by:
    \begin{equation}
    \det(A)
    \end{equation}
\end{definition}

\begin{intuition}
    Graphically, the determinant represents the volume of the parallelogram spanned by the column vectors of $A$. 
    \begin{itemize}
        \item Starting from a unit square, the matrix $A$ transforms this square into a parallelogram. 
        \item The area of the transformed parallelogram, denoted as $P$, equals the determinant:
    \end{itemize}

    \[
    \text{vol}(P) = \det(A)
    \]

    \textbf{Why? It is easy to see this for a diagonal matrix, where:}

    \[
    A = \begin{bmatrix} a_{11} & 0 \\ 0 & a_{22} \end{bmatrix}
    \]

    The matrix $A$ scales the unit square by $a_{11}$ along one axis and by $a_{22}$ along the other axis. This results in a rectangle with area:

    \[
    \text{vol}(P) = a_{11}a_{22} = \det(A)
    \]

    For instance, applying $A$ to the basis vectors:

    \[
    A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} a_{11} \\ 0 \end{bmatrix}, \quad A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ a_{22} \end{bmatrix}
    \]

    \customFigure[0.5]{00_Images/US.png}{Diagonal}

    \textbf{What if the matrix is not diagonal? Consider an upper triangular matrix:}

    \[
    A = \begin{bmatrix} a_{11} & a_{12} \\ 0 & a_{22} \end{bmatrix}
    \]

    The matrix $A$ still transforms the unit square, but now the parallelogram $P$ is slanted due to the non-zero $a_{12}$. The area of the parallelogram is still given by the determinant:

    \[
    \text{vol}(P) = a_{11} a_{22} = \det(A)
    \]

    Again, applying $A$ to the basis vectors:

    \[
    A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} a_{11} \\ 0 \end{bmatrix}, \quad A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} a_{12} \\ a_{22} \end{bmatrix}
    \]

    \customFigure[0.5]{00_Images/UT.png}{Upper triangular}
    \textbf{For a general matrix $A$, we can use the $QR$ factorization, where:}

    \[
    A = QR
    \]

    $R$ is an upper triangular matrix and $Q$ is an orthogonal matrix (i.e., $Q^\top Q = I$). 
    \begin{itemize}
        \item The orthogonal matrix $Q$ represents a rotation, which preserves volume (the area remains unchanged). 
        \item Therefore, the volume is determined by the determinant of $R$.
    \end{itemize}

    \[
    \text{vol}(P) = \det(R) = r_{11} r_{22} = \det(A)
    \]

    \customFigure[0.5]{00_Images/QR.png}{General matrix}
\end{intuition}

\subsection{Matrix inner product and norm}
\begin{definition}
    Set of matrices $A \in \mathbb{R}^{m \times n}$ is a vector space. We can define an inner product over this space:
    \begin{equation}
        \langle A, B \rangle = \text{tr}(B^T A) = \text{tr}(A^T B)
    \end{equation}
    \begin{itemize}
        \item $\text{tr}:$ Trace is the sum of diagonal elements of a square matrix.
    \end{itemize}
\end{definition}

\subsubsection{Frobenius Norm}
\begin{definition}
    For $A, B \in \mathbb{R}^{m \times n}$, then 
    \begin{equation}
        \| A \|_F = \sqrt{\langle A, A \rangle} = \sqrt{\text{tr}(A^T A)} = \sqrt{\sum_{i,j} a_{ij}^2}
    \end{equation}
\end{definition}

\begin{intuition}
    Let
    \[
    A = \begin{bmatrix} 
    a^{(1)} & \cdots & a^{(n)} 
    \end{bmatrix}
    \]
    then
    \[
    A^T A = \begin{bmatrix} 
    (a^{(1)})^T \\ \vdots \\ (a^{(n)})^T 
    \end{bmatrix} 
    \begin{bmatrix} 
    a^{(1)} & \cdots & a^{(n)} 
    \end{bmatrix}
    = 
    \begin{bmatrix} 
    (a^{(1)})^T a^{(1)} & \cdots & (a^{(1)})^T a^{(n)} \\
    \vdots & \ddots & \vdots \\
    (a^{(n)})^T a^{(1)} & \cdots & (a^{(n)})^T a^{(n)} 
    \end{bmatrix}
    \]
    \begin{itemize}
        \item \textbf{Note:} The matrix multiplication shows us why the trace can be written as $\sum_{i,j} a_{ij}^2$. 
        \begin{itemize}
            \item e.g. $a^{(1)})^T a^{(1)} = a_{11}^2 + a_{21}^2 + \ldots + a_{n1}^2$
        \end{itemize}
    \end{itemize}
    \vspace{1em}

    If we think of the matrix as a long column vector of the column vectors of $A$, then 
    \[
    \begin{bmatrix}
    a^{(1)} \\
    \vdots \\
    a^{(n)}
    \end{bmatrix}
    \]
    so we can think of the Frobenius norm as the $l_2$ norm exactly since it's $\sqrt{\sum_{i,j} a_{ij}^2}$, which is identical to the expression for l2-norm.
\end{intuition}

\subsubsection{Operator norm (Motivation for eigenvalues and eigenvectors):}
\begin{intuition}
    \textbf{Let} $A \in \mathbb{R}^{m \times n}$ \textbf{be a linear map from} $\mathbb{R}^n$ \textbf{to} $\mathbb{R}^m$.
    \begin{itemize}
        \item Think of the matrix as a mapping from $\mathbb{R}^n$ to $\mathbb{R}^m$.
    \end{itemize}

    \[
    v \quad \longrightarrow \quad Av \quad \text{(ellipse)}
    \]

    \customFigure[0.75]{00_Images/OMN.png}{Matrix norm tht maximizes $Av$ given $v$ with unit norm.}

    \textbf{Question:} Which direction of $v$ on the unit circle results in $Av$ with the largest magnitude? \textbf{Operator norm}, which can defined for all p valiues.

    \[
    \|A\|_2 = \max_{\|v\|_2 = 1} \|Av\|_2 
    \]

    \[
    \|A\|_1 = \max_{\|v\|_1 = 1} \|Av\|_1
    \]

    \[
    \|A\|_\infty = \max_{\|v\|_\infty = 1} \|Av\|_\infty
    \]

    \textbf{"Interesting" directions (i.e. directions of interest):}
    \begin{enumerate}
        \item  \textbf{Max/Min amplification direction:}
        \[
        \max_{\|v\|_2 = 1} \|Av\|_2 = \|A\|_2, \quad \min_{\|v\|_2 = 1} \|Av\|_2
        \]
        \begin{itemize}
            \item Later, we will see that this is related to the \textbf{singular values} of the matrix.
        \end{itemize}
    
        \item \textbf{Invariant direction:} (square matrix $A \in \mathbb{R}^{n \times n}$)
    
        \[
        A v = \lambda v
        \]
        \begin{itemize}
            \item $v$ is the eigenvector and $\lambda$ is the eigenvalue.
            \item \textbf{Note:} In 2D, there are 2 eigenvectors (i.e. parallel with same and opposite direction)
        \end{itemize}
    \end{enumerate}
\end{intuition}

\subsection{Eigenvalues and Eigenvectors}
\begin{definition}
    For $A \in \mathbb{R}^{n \times n}$, a non-zero vector $v$ is an eigenvector of $A$ with associated eigenvalue $\lambda$ if 
    \[
    A v = \lambda v
    \]
    \begin{itemize}
        \item \textbf{Consequence:} $(A - \lambda I) v = 0$, therefore, $v \in \mathcal{N}(A - \lambda I)$, where $\mathcal{N}(A - \lambda I)$ is the null space of $(A - \lambda I)$.    
    \end{itemize}
\end{definition}

\subsubsection{Characteristic polynomial}
\begin{definition} The above leads to a polynomial equation in $\lambda$ of degree $n$. The roots are the eigenvalues of $A$. 
    \[
    \det(A - \lambda I) = 0
    \]
    \begin{itemize}
        \item \textbf{Roots can be real or complex.} If complex, they occur in conjugate pairs.
        \item \textbf{Note:} The determinant is equal to 0 because $Av=\lambda v$ (i.e. linearly dependent), and LD vectors have determinant of $0$ by definition, so we want to find $\lambda$ s.t. the determinant is 0. 
    \end{itemize}
\end{definition}

\begin{warning}
    $n$ roots doesn't mean there are $n$ distinct eigen values. 
    \begin{itemize}
        \item e.g. For $(\lambda - 1)^2$, there are $2$ roots, but there is only one eigenvalue. 
    \end{itemize}
\end{warning}

\subsubsection{Geometric and algebraic multiplicity}
\begin{definition}

    \textbf{Algebraic multiplicity:} $\text{AM}(\lambda)$ is the number of times a given eigenvalue appears as a root of the characteristic equation $\det(A - \lambda I) = 0$.
    \begin{itemize}
        \item For example, $(\lambda - 1)^2$ has root 1 with multiplicity 2.
        \item i.e. If $p(x)=(x-\lambda)^m q(x)$, where $\lambda$ is not a root of $q(x)$, then $AM(\lambda)=m$
    \end{itemize}
    \vspace{1em}

    \textbf{Geometric multiplicity:} $\text{GM}(\lambda) = \dim \mathcal{N}(A - \lambda I)$
    \begin{itemize}
        \item \textbf{Key:} If we know the rank of $A - \lambda I$, then we can use FToLA to find the $\text{GM}(\lambda) = \dim \mathcal{N}(A - \lambda I)$.
    \end{itemize}
\end{definition}

\begin{theorem}
    In general, for any matrix $A \in \mathbb{R}^{n \times n}$ (square matrix),
    \[
    GM(\lambda) \leq AM(\lambda).
    \]
\end{theorem}

\subsubsection{Defective and non-defective}
\begin{definition}

    \textbf{Defective Matrix:} If $GM(\lambda) < AM(\lambda)$ for some $\lambda$
    \begin{itemize}
        \item \textbf{Note:} Defective doesn't mean that it's not invertible.
        \item \textbf{Key:} Cannot construct $n$ linearly independent vectors because $\exists$ a $\lambda$ s.t. $\sum \text{AM} = n$ but $\sum \text{GM} < n$.  
    \end{itemize}
    \vspace{1em}

    \textbf{Non-defective Matrix:} If $GM(\lambda) = AM(\lambda)$, then the matrix is \textbf{diagonalizable} (i.e. non-defective).
    \begin{itemize}
        \item \textbf{"Most" matrices are non-defective.} In other words, the set of non-defective matrices is dense (a random matrix is almost surely non-defective).
    \end{itemize}

\end{definition}

\subsubsection{Eigenvectors corresponding to different eigenvalues are l.i.}
\begin{theorem}
    \textbf{Lemma:} If $u^{(1)}, \ldots, u^{(k)}$ are eigenvectors then,

    \begin{equation}
        u^{(i)} \notin \phi_j = \mathcal{N}(A-\lambda I) \text{ for } i \neq j
    \end{equation} 

    \begin{itemize}
        \item i.e. An eigen vector of $i$ of one null space cannot be in another eigenvector's null space. 
    \end{itemize}
\end{theorem}

\begin{derivation}
    \textit{Why?} Prove by contradiction, assume $u^{(i)}$ is in the null space of $\phi_j$.
    \begin{itemize}
        \item If $A u^{(i)} = \lambda_i u^{(i)}$ and $u^{(i)} \in \mathcal{N}_j$, then $A u^{(i)} = \lambda_j u^{(i)}$. 
        \item Therefore, $(\lambda_i - \lambda_j) u^{(i)} = 0$, which is impossible since $\lambda_i \neq \lambda_j$ and $u^{(i)}$ is a non-zero vector by definition of eigenvectors. 
    \end{itemize}   
\end{derivation}

\begin{theorem}
    Eigenvectors corresponding to different eigenvalues are linearly independent.
\end{theorem}

\begin{warning}
    Holds for any defective or non-defective matrix. 
    \begin{itemize}
        \item \textbf{Same eigenvalue (key subtle detail):} Eigenvectors corresponding to an eigenvalue can be linearly dependent or independent (\textbf{but not guaranteed to be l.i.}). 
            \begin{itemize}
                \item e.g. If $\dim (\mathcal(N)(A-\lambda I)) = 2$, you can find 2 basis vectors (i.e. eigenvectors), but \textbf{not} any two vectors. 
                \item Since we can choose two vectors to be linearly independent or dependent, you have to choose the linearly independent vectors.
                \begin{itemize}
                    \item i.e. Any vector in $\mathcal(N)(A-\lambda I)$ is an eigenvector, but not every 2 eigenvectors in $\mathcal(N)(A-\lambda I)$ is linearly independent. 
                    \item e.g. If $v$ is an eigenvector, then $2v$ is also an eigenvector. 
                \end{itemize}
                \item \textbf{Construction:} So choose two linearly independent vectors to be the eigenvectors of that space. 
            \end{itemize}
        \item \textbf{Different eigenvalue:} For eigenvectors corresponding to different eigenvalues though, we are guaranteed for them to be linearly independent since they live in different null spaces. 
    \end{itemize}
\end{warning}

\begin{derivation}
    Let $\lambda_1, \ldots, \lambda_k$ be distinct eigenvalues, $u^{(1)}, \ldots, u^{(k)}$ be the corresponding eigenvectors, and $\phi_i = \mathcal{N}(A - \lambda_i I)$, the null space corresponding to eigenvalue $\lambda_i$.
    \vspace{1em}

    Proof by contradiction, assume that $u^{(1)}, \ldots, u^{(k)}$ are linearly dependent, i.e.,
    \[
    u^{(1)} = \sum_{i=2}^{k} \alpha_i u^{(i)}.
    \]
    \begin{itemize}
        \item i.e. The first eigenvector can be written as a linear combination of the other eigenvectors. 
    \end{itemize}
    \vspace{1em}

    \textbf{Part 1:}
    \[
    A u^{(1)} = \sum_{i=2}^{k} \alpha_i A u^{(i)} = \sum_{i=2}^{k} \alpha_i \lambda_i u^{(i)}.
    \]
    \begin{itemize}
        \item 2nd part: By definition of $u^{(1)}$
        \item 3rd part: Since $u^{(i)}$ is an eigenvector with corresponding $\lambda_i$
    \end{itemize}

    \textbf{Part 2:}
    \[
    \lambda_1 u^{(1)} = \sum_{i=2}^{k} \alpha_i \lambda_1 u^{(i)}.
    \]
    \begin{itemize}
        \item \textbf{Key:} $\lambda_1$ is inside the summation, while for part 1, it was $\lambda_i$
    \end{itemize}
    \vspace{1em}

    \textbf{Combining both part 1 and part 2:}
    But $A u^{(1)} = \lambda_1 u^{(1)}$, so
    \[
    A u^{(1)} - \lambda_1 u^{(1)} =\sum_{i=2}^{k} \alpha_i (\lambda_1 - \lambda_i) u^{(i)} = 0,
    \]
    which implies that $\{u^{(2)}, \ldots, u^{(k)}\}$ is linearly dependent because $\lambda_1 - \lambda_i \neq 0$ since they are distinct, so there exists constants not all 0 to make the linear combination equal to 0 
    \begin{itemize}
        \item i.e. not linearly independent as the only constants that would satisfy this is all constants being $0$.
    \end{itemize}
    \vspace{1em}

    Continue with the same argument to show that $\{u^{(3)}, \ldots, u^{(k)}\}$ is linearly dependent, and so on. 
    \vspace{1em}

    Eventually, $\{u^{(k-1)}, u^{(k)}\}$ are linearly dependent, so:
    \[
    u^{(k)} = \alpha u^{(k-1)}.
    \]
    But this is impossible by the previous lemma since an eigenvector of one null space cannot be in another eigenvector's null space (i.e. cannot write on as a l.c. of the other). Therefore, $\{u^{(1)}, \ldots, u^{(k)}\}$ is linearly independent.    
\end{derivation}

\subsection{Matrix Diagonalization}
\begin{intuition}
    The previous theorem leads us to the diagonalization of a non-defective square matrix $A \in \mathbb{R}^{n \times n}$. 
    \vspace{1em}

    The idea is to look at the null space $\mathcal{N}(A - \lambda_i I)$. 
    \begin{itemize}
        \item When $\lambda_1, \ldots, \lambda_n$ are distinct, then $u^{(1)}, \ldots, u^{(n)}$ are linearly independent, so the set $\{u^{(1)}, \ldots, u^{(n)}\}$ forms a basis for $\mathbb{R}^n$.
        \item If some $\lambda_i$ is a repeated eigenvalue. For example, if $AM(\lambda_i) = 2$, then $GM(\lambda_i) = 2$ because it's non-defective matrix (i.e. non-defective by definition means that $AM(\lambda)=GM(\lambda)$) so $\dim \mathcal{N}(A - \lambda_i I) = 2$. 
        \begin{itemize}
            \item This means we can find $\{u^{(i,1)}, u^{(i,2)}\}$ that span this null space.
            \item So, if we assemble all these $u^{(i,j)}$'s, then we get a basis for $\mathbb{R}^n$.
        \end{itemize}
    \end{itemize}
\end{intuition}

\begin{theorem}
    Let $\lambda_i$ for $i = 1, \dots, k$, be distinct eigenvalues of a non-defective matrix $A \in \mathbb{R}^{n \times n}$. Let $\mu_i$ be the multiplicity of $\lambda_i$ (i.e., $AM(\lambda_i) = GM(\lambda_i) = \mu_i$).
    \vspace{1em}

    Let
    \[
    U^{(i)} = \begin{bmatrix} 
    u^{(i,1)} & \dots & u^{(i,\mu_i)}
    \end{bmatrix}
    \]
    be a matrix of eigenvectors that form a basis for $\mathcal{N}(A - \lambda_i I)$.
    \vspace{1em}

    Then 
    \[
    U = \begin{bmatrix} 
    U^{(1)} & \dots & U^{(k)}
    \end{bmatrix}
    \]
    is a complete set of basis vectors for $\mathbb{R}^n$.
    \vspace{1em}

    Then the \textbf{Eigendecomposition} is
    \[
    A = U \Lambda U^{-1}
    \]
    where 
    \[
    \Lambda = \begin{bmatrix}
    \lambda_1 & 0 & 0 & 0 & \dots & 0 \\
    0 & \lambda_1 & 0 & 0 & \dots & 0 \\
    0 & 0 & \lambda_2 & 0 & \dots & 0 \\
    0 & 0 & 0 & \lambda_2 & \dots & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & 0 & \dots & \lambda_n
    \end{bmatrix}
    \]
    is a diagonal matrix with the eigenvalues of $A$ on the diagonal.
    \begin{itemize}
        \item Where $\lambda_1$ appears $\mu_1$ times on the diagonal (indicating multiplicity), followed by $\lambda_2$ appears $\mu_2$ and so on. 
    \end{itemize}
\end{theorem}

\begin{warning}
    \begin{itemize}
        \item This is an extension of the previous theorem, since we are only considering non-defective matrices, which means $AM=GM$, so then you can find $n$ linearly independent vectors. 
        \begin{itemize}
            \item \textbf{Defective:} We cannot find $n$ linearly independent vectors because $GM < AM$.
        \end{itemize}
        \item But, the same key subtlety pops up as for the same eigenvalue, we have to choose eigenvectors in that space that are linearly independent by \textbf{construction}.
    \end{itemize}
\end{warning}

\begin{warning}
    You cannot orthnormalize these $n$ linearly indpeendent vectors using Gram because they are not in the same space (i.e. if you Gram-Schmidt it, you will get non-eigenvectors).
    \begin{itemize}
        \item Therefore orthonormalizing vectors in the same space (i.e. same eigenvalue) retains the eigenvector property, but across spaces, will lose the eigenvector properties. 
    \end{itemize}
\end{warning}

\begin{derivation}
    $A u^{(i,j)} = \lambda_i u^{(i,j)}$, where $u^{(i,j)}$ is an eigenvector with eigenvalue $\lambda_i$, then this implies that

    \[
    A \begin{bmatrix}
    u^{(1,1)} & \dots & u^{(1,\mu_1)} & u^{(2,1)} & \dots & u^{(2,\mu_2)} & \dots & u^{(n,\mu_n)}
    \end{bmatrix}
    = U
    \begin{bmatrix}
        \lambda_1 & 0 & 0 & 0 & \dots & 0 \\
        0 & \lambda_1 & 0 & 0 & \dots & 0 \\
        0 & 0 & \lambda_2 & 0 & \dots & 0 \\
        0 & 0 & 0 & \lambda_2 & \dots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & 0 & \dots & \lambda_n
        \end{bmatrix}
    \]
    \begin{itemize}
        \item i.e. Generalizing to all the eigenvectors with their corresponding eigenvalues. 
    \end{itemize}
    \vspace{1em}

    Thus, we have:
    \[
    A U = U \Lambda
    \]
    which implies:
    \[
    A = U \Lambda U^{-1}
    \]
    where \( U \) is invertible because its columns (the eigenvectors) are linearly independent, and \( U \) is a square matrix.
\end{derivation}

\subsubsection{What does this diagonalization mean?}
\begin{intuition}
    This applies to any non-defective (or diagonalizable) matrix \( A \in \mathbb{R}^{n \times n} \). We start with the eigendecomposition of a matrix \( A \):
    \[
    A = U \Lambda U^{-1}
    \]
    \begin{itemize}
        \item $U = \begin{bmatrix} u^{(1)} & \dots & u^{(n)} \end{bmatrix}.$
    \end{itemize}
    \vspace{1em}

    Let \( x \in \mathbb{R}^n \) be any vector. Then:
    \[
    A x = U \Lambda U^{-1} x.
    \]
    Let \( \tilde{x} = U^{-1} x \), so that:
    \[
    A x = U \Lambda \tilde{x} \quad \text{and} \quad x = U \tilde{x} = \begin{bmatrix} u^{(1)} & \dots & u^{(n)} \end{bmatrix} \begin{bmatrix} \tilde{x}_1 \\ \vdots \\ \tilde{x}_n \end{bmatrix} = \sum_{i=1}^{n} \tilde{x}_i u^{(i)}.
    \]
    \begin{itemize}
        \item \( \tilde{x} = U^{-1}x\): Coordinates of $x$ in the new basis \( \{u^{(1)}, \dots, u^{(n)}\} \).
        \begin{itemize}
            \item Since $x$ is a linear combination of eigenvectors, therefore, we are \textbf{transforming} to the basis of eigenvectors, where $x$ can be written as $\sum_{i=1}^{n} \tilde{x}_i u^{(i)}$.
        \end{itemize}
        \item \( \Lambda \tilde{x} \) means that we are scaling \( \tilde{x}_i \) by the corresponding eigenvalue \( \lambda_i \).
        \item \(y = Ax = U \Lambda \tilde{x} \) means that we are changing the coordinates back to the original basis.
        \begin{itemize}
            \item Since the inverse of $U^{-1}$ is $U$, therefore, after scaling by $\Lambda$, we are essentially transforming from the basis of eigenvectors to the original basis by applying the inverse transformation. 
        \end{itemize}
    \end{itemize}
\end{intuition}

\begin{warning}
    \begin{itemize}
        \item \textbf{Change of basis to eigenvector basis:} $U^{-1} x$ transforms $x$ from the original basis to the eigenvector basis. The ith component of $x$ is represented by $\tilde{x}_i$ (i.e. coordinate) in the new basis.
        \item \textbf{Scaling:} $\tilde{x}$ get scaled by their corresponding eigenvalues in $\Lambda$
        \item \textbf{Change of basis to original basis:} $\tilde{x}$ then gets transformed back to original basis by applying the inverse transformation of $U^{-1}$, which is $U$.
    \end{itemize}
\end{warning}

\subsection{Application: Google's PageRank Algorithm}
\subsubsection{How to measure importance of a webpage}
\begin{intuition}
    The following two methods are insufficient as there are loop holes.
    \begin{itemize}
        \item \textbf{Web Search:} Mapping keywords to URLs. 
        \item \textbf{Hyperlinks:} Hyperlinks that link to a webpage give an indication of how "important" the webpage is. 
    \end{itemize}
    \vspace{1em}

    Therefore, the importance of a webpage should depend on the importance of the webpages linking to it.
\end{intuition}

\subsubsection{Example: Random Walk across Webpages}
\begin{example}
    Bot performs a random walk across the webpages. Let $x_i^{(t)}$ be the probability that the bot is on page $i$ at time $t$. 
    \customFigure[0.75]{00_Images/GPR.png}{P are the pages, and the numbers in red are the probabilities of going to that page with the bot}

\[
x_1^{(t+1)} = 1 \cdot x_3^{(t)} + \frac{1}{2} x_4^{(t)}
\]
\[
x_2^{(t+1)} = \frac{1}{3} x_1^{(t)}
\]
\[
x_3^{(t+1)} = \frac{1}{3} x_1^{(t)} + \frac{1}{2} x_2^{(t)} + \frac{1}{2} x_4^{(t)}
\]
\[
x_4^{(t+1)} = \frac{1}{3} x_1^{(t)} + \frac{1}{2} x_2^{(t)}
\]

This system can be written as:

\begin{equation*}
\begin{bmatrix}
x_1^{(t+1)} \\
x_2^{(t+1)} \\
x_3^{(t+1)} \\
x_4^{(t+1)}
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 1 & \frac{1}{2} \\
\frac{1}{3} & 0 & 0 & 0 \\
\frac{1}{3} & \frac{1}{2} & 0 & \frac{1}{2} \\
\frac{1}{3} & \frac{1}{2} & 0 & 0 
\end{bmatrix}
\begin{bmatrix}
x_1^{(t)} \\
x_2^{(t)} \\
x_3^{(t)} \\
x_4^{(t)}
\end{bmatrix}
\end{equation*}
\begin{itemize}
    \item $P$ is the transition probability matrix. 
\end{itemize}
\vspace{1em}
\end{example}

\subsubsection{Convergence to a Steady State Distribution}
\begin{definition}
    The evolution of the system is given by $x^{(t+1)} = P x^{(t)}$. As $t \to \infty$, $x^{(t)} \to x^{(\infty)}$ (under certain conditions) s.t. $Px^{(\infty)}=x^{(\infty)}$.
    \begin{itemize}
        \item i.e. The steady state $x^{(\infty)}$ is the eigenvector of $P$ with eigenvalue 1. That is, $P x^{(\infty)} = x^{(\infty)}$. 
    \end{itemize}
\end{definition}

\subsubsection{Conditions for Convergence}
\begin{definition}
    When the Markov chain is both irreducible and aperiodic, then $x^{(t)} \to x^{(\infty)}$ as $t \to \infty$.
    \begin{itemize}
        \item An \textbf{aperiodic Markov chain} means the Markov chain does not exhibit periodic behavior, meaning it doesn't alternate between certain states but can visit them in an irregular pattern.
        \item An \textbf{irreducible Markov chain} means that there is a path from every state (webpage) to every other state, ensuring eventual convergence to a steady state.
    \end{itemize}
\end{definition}

\begin{example} Examples of divergence
    \customFigure[0.75]{00_Images/C.png}{Non-convergent Markov chains}
    \begin{itemize}
        \item Reducible Markov chains have disconnected chains, meaning that it will never converge since they are separated.
        \item Periodic Markov chains will always oscillate between webpages.
        \item \textbf{Key:} The convergence of the PageRank vector depends on the initial condition and the structure of the Markov chain.
    \end{itemize}
\end{example}

\subsubsection{Practical Modification}
\begin{intuition}
    In practice, the following adjustment ensures irreducibility and aperiodicity in web surfing:
    \begin{itemize}
        \item With probability 85\%, the robot follows one of the hyperlinks from the current webpage.
        \item With probability 15\%, the robot jumps to a totally random webpage on the Internet.
    \end{itemize}

This ensures that the Markov chain is irreducible and aperiodic, i.e., $x^{(t)} \to x^{(\infty)}$.
\end{intuition}

\subsubsection{Why is 1 an eigenvalue of P?}
\begin{derivation}
    \begin{enumerate}
        \item Fact: $\det(A) = \det(A^T)$.
    
        \item Recall that the set of eigenvalues of a matrix is given by:
        
        \[
        \{ \lambda : \det(P - \lambda I) = 0 \}
        \]
        
        \item Using the fact that the determinant of a matrix equals the determinant of its transpose, we have:
        
        \[
        \{ \lambda : \det((P - \lambda I)^T) = 0 \}
        \]
        
        \item This implies:
        
        \[
        \{ \lambda : \det(P^T - \lambda I) = 0 \}
        \]
        
        \item Thus, the set of eigenvalues of $P$ is the same as the set of eigenvalues of $P^T$.
        
        \item Consider the following equation:
        
        \[
        P^T \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix} = 1 \cdot \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix}
        \]
        \begin{itemize}
            \item Since the sum of the columns equal to $1$ for $P$, therefore, the sums of the rows equal to $1$ for $P^T$. 
            \item Therefore, if multiply $P^T$ by the ones vector, then this will yield the sums of each row (i.e. 1)
            \item So, 1 is an eigenvalue of $P$.
        \end{itemize}
    \end{enumerate}

\end{derivation}

\subsubsection{What about other eigenvalues?}
\begin{definition}
    All other eigenvalues $\lambda_i$ satisfy:    
        \[
        |\lambda_i| \leq 1
        \]
\end{definition}

\begin{derivation}
    \begin{enumerate}
        \item We know that $P x = \lambda x$. By applying $P$ $t$ times, we get:

        \[
        P(P(\dots(Px))) = P^t x = \lambda^t x
        \]
        
        \item Thus, $P^t x = \lambda^t x$, and since every entry of $P^t x \leq 1$ because it's a probability distribution. Therefore, we should have that every entry of $P^t x$ is bounded.
        
        \item If $|\lambda_i| > 1$, then $\lambda_i^t$ diverges to infinity as $t \to \infty$. But, for stability:
        \begin{itemize}
            \item Since $P^T x = \lambda^T x$ and $P^T x \leq 1 \implies |\lambda_i| \leq 1 \quad \forall i$
        \end{itemize}
    \end{enumerate}
\end{derivation}

\subsubsection{Convergence via Power Iteration}
\begin{definition}
    We can express the eigenvalue decomposition of the matrix $P$ as follows:

    \begin{enumerate}
        \item The eigenvalue decomposition of $P$ is given by:
        \[
        P = U \Lambda U^{-1}
        \]
        where $U$ is the matrix of eigenvectors, and $\Lambda$ is the diagonal matrix of eigenvalues.

        \item After $t$ iterations of applying $P$, we have:
        \[
        P^t x = (U \Lambda U^{-1})(U \Lambda U^{-1}) \dots (U \Lambda U^{-1}) x = U \Lambda^t U^{-1} x
        \]

        \item Breaking this down, we get:
        \[
        P^t x = U \begin{bmatrix}
        1 & 0 & \dots & 0 \\
        0 & \lambda_2^t & \dots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \dots & \lambda_n^t
        \end{bmatrix} U^{-1} x
        \]

        \item As $t \to \infty$, if $|\lambda_i| < 1$ for $i = 2, 3, \dots, n$, then:
        \[
        \lambda_i^t \to 0
        \]
        Hence, the system converges to:
        \[
        a_1 u^{(1)} 
        \]
        \begin{itemize}
            \item $u^{(1)}$ is the eigenvector corresponding to the largest eigenvalue $\lambda_1 = 1$.
            \item $U^{-1}x = a$ 
        \end{itemize}

        \item By normalizing the vector $x^{(t)}$ at each iteration, we obtain $u^{(1)}$, which corresponds to the eigenvalue $\lambda_1 = 1$. This process is known as the \textbf{power iteration method}.
    \end{enumerate}
\end{definition}



